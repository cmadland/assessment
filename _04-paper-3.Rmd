# Assessment in Digital Learning {-}

## Alt-Title - Thoerizing at the Intersection of Assessment and Educational Technology {-}

- Emergent themes based on first two papers, which ***may*** include:
    - technology for adaptive testing, automated item generation, large-scale assessments (NCLEX, PISA, FSA, etc)
    - technology for surveillance or academic integrity (Turnitin, Proctorio, etc)
    - technology to enable new forms of learning- and learner-centred assessments (portfolios, collaboration, version control, learner contributors)
    - ethical applications of technology in assessment

## Introduction  {-}

Assessment in higher education 
There are two branches of literature related to assessment in technology-mediated HE. The first is related to the application of technology to increasing the efficiency and predictive power of algorithmic assessment tools such as automated essay scoring [@kumarExplainableAutomatedEssay2020], automated item generation [@shinMultipleChoiceItemDistractor2019], adaptive testing [@bornEvaluatingDifferentEquating2019], predictive statistical analyses [@gorgunPolytomousScoringApproach2021] and gamified immersive environments [@clarke-miduraAssessmentTechnologyChange2010]. The other branch is more focussed on the power of technology to mediate formative assessment practices through learner- and community-centred environments [@conradAssessmentStrategiesOnline2018; @gikandiTheoryFormativeAssessment2015; @gikandiOnlineFormativeAssessment2011]. These branches do have some common characteristics in that researchers in both advocate for greater varieties of assessments compared to pencil-and-paper tests, greater efficiency and accuracy in grading, increased ability to capture and assess learner processes, and greater flexibility in the timing of assessments as technology allows for asynchronous interactions [@conradAssessmentStrategiesOnline2018].


## Literature    {-}

### Theory and Educational Technology

[@bowerTechnologyMediatedLearning2019]
- 7 premises
- agentic intentions


 [@oliverLearningTechnologyTheorising2013] 
- affordances
- textual analysis of technology
- technology under-theorized


[@veletsianosRiseEducationalTechnology2017] [@selwynUseComputerTechnology2007]
### Assessment and Technology {-}

Educational technologies are often viewed and reported on with a distinct positivity bias [@irvineLandscapeMergingModalities2020] wherein 'new' technologies are presumed to represent progress and will inevitably have a positive effect on learning [@bowerTechnologyMediatedLearning2019].  This can be seen in the titles given to some initiatives, such as "Technology-Enhanced Assessment" [@oldfieldAssessmentDigitalAge2012; @timmisRethinkingAssessmentDigital2016], "IT-enabled assessment" [@webbAssessmentTwentyFirstCentury2018], or "technology-rich" [@linAssessingLearningTechnologyrich2020]. As such, I will use the more neutral term "technology-mediated" to indicate that adding digital technology to an assessment environment does not necessarily improve that environment. Similar to assessment practices being grounded in (both philosophically, as in 'based upon', and figuratively, as in 'stuck in') behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment, so too, many educational technologies are grounded in (based upon and stuck in) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment. 

This can be seen in the progressively more advanced technologies beginning with Pressey's teaching machines [@benjaminHistoryTeachingMachines1988; @presseyMachineAutomaticTeaching1927; @wattersTeachingMachinesHistory2021] which was built to automate the process of "drilling" learners in an effort to teach them some concept. The machine needed to be pre-programmed with a series of selected-response questions along with distractors and correct answers. As a learner answered each question, the machine was programmed to match the response to the programmed correct response, and if it matched, the learner was determined to have "mastered" that question and it was dropped from the bank of questions the learner had not yet mastered. If it did not match, the question was cycled back into the bank to be repeated. Clearly, this technology was promoted as a tool to be used to modernize and increase the efficiency of tasks that aligned with the dominant pedagigical paradigm at the time. A second example, although not one marketed directly to schools, but to parents, was the *Speak & Spell*, released in 1978 by Texas Instruments [@braguinskiArchiveCommunicationInteractive2018; @frantzSpeakSpell2014], which represented an advance in technology and an increase in efficiency, as the *Speak & Spell* could be programmed to store and reproduce voice recordings of words as well as multiple recordings of feedback messages [@frantzSpeakSpell2014]. While the *Speak & Spell* was a leap forward in processing power, memory storage, and therefore complexity, the underlying pedagogy remained identical to that of Pressey's teaching machine [@wattersSpeakSpellHistory2015]. Moving forward again, and modern technologies are vastly more powerful than teaching machines or the *Speak & Spell* and power very complex adaptive tests, such as the NCLEX-RN, the national licensing exam for Registered Nurses in Canada and the USA [@smithglasgowStandardizedTestingNursing2019]. These advances in both hardware and software allow for still greater efficiencies in testing, yet the NCLEX-RN must still be programmed with selected-response questions, their distractors, and correct responses, all still in alignment with behavioural models of pedagogy. The NCLEX-RN is an example of a large-scale, standardized assessment (LSA), so is not parallel to the classroom assessment practices which are the subject of this paper, but I mention it here to draw the distinction between professionally-created LSAs and most instructor-created classroom assessments. The NCLEX-RN is continually revised and updated and reflects very robust psychometric properties (validity and reliability) [@smithglasgowStandardizedTestingNursing2019]. More importantly, however, the NCLEX-RN has been updated to "shift away from a primary focus on content and the indirect testing of clinical judgment to a major focus on clinical judgment" [@caputiReflectionsNextGeneration2019, p. 2]. Caputi, in her reflections on the next-generation NCLEX-RN (NGN) asks 2 questions (the second of which is most relevant here): "1) Are students ready for this type of NCLEX? 2) If our students already pass the NCLEX, can we keep doing the same type of preparation for the NGN?" (p. 2). Her answer to both questions is "No." She goes on to argue,

> So, what can faculty do? <i>I propose that nurse faculty, at all levels of nursing education, revise their curricula to teach a detailed thinking process that students must employ over and over throughout the nursing curriculum. Just as students practice psychomotor skills until they are perfected, they must do so with thinking skills and strategies A new model for teaching clinical judgment is needed",</i> (p. 2, emphasis in original).

In this case, the technology-mediated assessment instrument has been designed to measure 21st century skills, and Caputi recognizes that if pre-service nurses are going to have to pass the NGN, nursing instructors will need to realign their pedagogy. This example illustrates how LSAs exert pressure on instructors and schools to adjust their pedagogy, in the case of nursing, to encourage 21st century pedagogy that teaches thinking skills. The opposite is also true, however, in that LSAs which emphasize the lower-level cognitive skills prized by the behaviourists cause instructors to match their pedagogy to that model [@caputiReflectionsNextGeneration2019; @clarke-miduraAssessmentTechnologyChange2010; @delucaExploringAssessmentCultures2021; @pellegrinoPerspectivesIntegrationTechnology2010]. Further, the American Educational Research Association (AERA), the National Council on Measurement in Education (NCME), and the American Psychological Association (APA) argue that LSAs tend to have other negative effects on education systems, namely the narrowing of curricula (teaching to the test), reduced instructional strategies (previously mentioned), higher dropout rates, and the enactment of policies and practices that increase test scores without increasing learning [-@aeraStandardsEducationalPsychological2014]. The NCLEX-RN notwithstanding, many implementations of technology in assessment remain focused on increasing the efficiencies of summative test administration [@broadfootAssessmentTwentyFirstCenturyLearning2016; @pellegrinoPerspectivesIntegrationTechnology2010; @webbAssessmentTwentyFirstCentury2018].


### Theoretical / Conceptual Framework    {-}

### Research Method   {-}

### Analysis   {-}

### Findings and Limitations   {-}

### Discussion  {-}

### Impact on Theory and/or Practice   {-}

### Recommendations for Future Research   {-}

### Conclusion   {-}

### Data Availability   {-}

### References   {-}


## Conclusions and Recommendations {-}

## References {-}

## Appendices {-}



