<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Review of the Literature | Assessment in Higher Education</title>
  <meta name="description" content="This site hosts the open development of my PhD dissertation at University of Victoria." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Review of the Literature | Assessment in Higher Education" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This site hosts the open development of my PhD dissertation at University of Victoria." />
  <meta name="github-repo" content="cmadland/assessment" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Review of the Literature | Assessment in Higher Education" />
  
  <meta name="twitter:description" content="This site hosts the open development of my PhD dissertation at University of Victoria." />
  

<meta name="author" content="Colin Madland" />


<meta name="date" content="2022-01-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="method.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="https://hypothes.is/embed.js" async></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Assessment in Higher Education</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#topic-of-the-research"><i class="fa fa-check"></i>Topic of the Research</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#problem-to-be-researched"><i class="fa fa-check"></i>Problem to be Researched</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#purpose-of-the-research"><i class="fa fa-check"></i>Purpose of the Research</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#research-questions"><i class="fa fa-check"></i>Research Questions</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#significance-of-the-research"><i class="fa fa-check"></i>Significance of the Research</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html"><i class="fa fa-check"></i>Review of the Literature</a>
<ul>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#structure-of-the-paper"><i class="fa fa-check"></i>Structure of the Paper</a></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#defining-assessment"><i class="fa fa-check"></i>Defining Assessment</a></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#conceptions-of-assessment"><i class="fa fa-check"></i>Conceptions of Assessment</a></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#assessment-literacy"><i class="fa fa-check"></i>Assessment Literacy</a></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#assessment-and-technology"><i class="fa fa-check"></i>Assessment and Technology</a></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#impact-on-learners"><i class="fa fa-check"></i>Impact on Learners</a></li>
<li class="chapter" data-level="" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#theoretical-framework-overview"><i class="fa fa-check"></i>Theoretical Framework Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html"><i class="fa fa-check"></i>Method</a>
<ul>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#overview-of-the-problem-purpose-and-questions"><i class="fa fa-check"></i>Overview of the Problem, Purpose, and Questions</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#methodological-approaches"><i class="fa fa-check"></i>Methodological Approaches</a>
<ul>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#quantitative-approaches"><i class="fa fa-check"></i>Quantitative approaches</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#qualitative-approaches"><i class="fa fa-check"></i>Qualitative approaches</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#mixed-approaches"><i class="fa fa-check"></i>Mixed approaches</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#epistemological-alignment-with-purpose-of-the-research"><i class="fa fa-check"></i>Epistemological Alignment with Purpose of the Research</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#visualization-of-a-mixed-research-project"><i class="fa fa-check"></i>Visualization of a Mixed Research Project</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#participants-and-sampling-strategy"><i class="fa fa-check"></i>Participants and Sampling Strategy</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#data-collection-strategy"><i class="fa fa-check"></i>Data Collection Strategy</a>
<ul>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#phase-1"><i class="fa fa-check"></i>Phase 1</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#phase-2"><i class="fa fa-check"></i>Phase 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#types-of-data-and-analysis"><i class="fa fa-check"></i>Types of Data and Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#quantitative-data"><i class="fa fa-check"></i>Quantitative data</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#qualitative-data"><i class="fa fa-check"></i>Qualitative data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#research-procedures"><i class="fa fa-check"></i>Research Procedures</a></li>
<li class="chapter" data-level="" data-path="method.html"><a href="method.html#chapter-summary"><i class="fa fa-check"></i>Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i>Results</a></li>
<li class="chapter" data-level="" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i>Discussion</a></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/cmadland/assessment" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Assessment in Higher Education</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="review-of-the-literature" class="section level1 unnumbered">
<h1>Review of the Literature</h1>
<div id="structure-of-the-paper" class="section level2 unnumbered">
<h2>Structure of the Paper</h2>
<p>This paper will begin with establishing a clear definition of assessment followed by an analysis of the literature related to the concept of <em>assessment literacy</em> (AL) <span class="citation">(<a href="#ref-delucaDifferentialSituatedView2019" role="doc-biblioref">DeLuca, Coombs, et al. 2019</a>)</span> as it relates to the approaches to assessment <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">DeLuca, LaPointe-McEwan, and Luhanga 2016a</a>)</span> taken by HE instructors and the perceptions and experiences of HE learners. The last section will introduce a framework for understanding assessment in HE, situated within Biggs’ <span class="citation">(<a href="#ref-biggsWhatStudentDoes1999" role="doc-biblioref">1999</a>; <a href="#ref-biggsTheoryPracticeCognitive1993" role="doc-biblioref">1993</a>)</span> 3P model of teaching and learning, and research questions which emerge from the literature.</p>
</div>
<div id="defining-assessment" class="section level2 unnumbered">
<h2>Defining Assessment</h2>
<p>There are deep and rich bodies of literature addressing educational assessment <em>writ large</em>, both from a summative, psychometric perspective, and from a formative perspective. There is notably more research in the K-12 context, especially in relation to teacher preparation, compared to HE.</p>
<p>Among the more influential publications related to modern views of assessment (then usually called “evaluation”) was Scriven’s <span class="citation">(<a href="#ref-scrivenMethodologyEvaluation1967" role="doc-biblioref">1967</a>)</span> article in which he drew distinctions between “formative” and “summative” evaluation. Formative evaluation was described as evaluation for the purposes of improvement, and summative evaluation was seen as a validation of the quality of work at the end of a process. This distinction was quickly incorporated into Bloom’s <span class="citation">(<a href="#ref-bloomLearningMasteryInstruction1968" role="doc-biblioref">1968</a>)</span> ideas related to mastery learning and began to be promoted as a model for educational reform. However, by the late 1990s, when Black and Wiliam <span class="citation">(<a href="#ref-blackAssessmentClassroomLearning1998" role="doc-biblioref">1998</a>)</span> published their thorough review of the literature, the idea of formative assessment was still not well-defined or implemented. Black and Wiliam framed formative assessment as “encompassing all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the teaching and learning activities in which they are engaged” <span class="citation">(<a href="#ref-blackAssessmentClassroomLearning1998" role="doc-biblioref">1998, 7–8</a>)</span>. Although Black and Wiliam came to very strongly-stated conclusions about the value of formative assessments (e.g. “The research reported here shows conclusively that formative assessment does improve learning. The gains in achievement appear to be quite considerable, and as noted earlier, amongst the largest ever reported for educational interventions.” <span class="citation">(<a href="#ref-blackAssessmentClassroomLearning1998" role="doc-biblioref">1998, 61</a>)</span>), reliance on summative assessments in HE has remained high <span class="citation"><a href="#ref-lipnevichWhatGradesMean2020" role="doc-biblioref">Lipnevich et al.</a> (<a href="#ref-lipnevichWhatGradesMean2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>The National Research Council’s (NRC) 2001 report <em>Knowing what students know,</em> advanced understanding of assessment with their definition of assessment as “a process of drawing reasonable inferences about what students know on the basis of evidence derived from observations of what they say, do, or make in selected situations” <span class="citation">(<a href="#ref-pellegrinoKnowingWhatStudents2001" role="doc-biblioref">Pellegrino, Chudowsky, and Glaser 2001, 112</a>)</span> or, more simply, “reasoning from evidence” <span class="citation">(<a href="#ref-pellegrinoKnowingWhatStudents2001" role="doc-biblioref">Pellegrino, Chudowsky, and Glaser 2001, 43</a>)</span>, based on Mislevy’s assertion that “test theory is machinery for reasoning from students’ behavior to conjectures about their competence, as framed in a particular conception of competence.” <span class="citation">(<a href="#ref-mislevyTestTheoryReconcieved1994" role="doc-biblioref">1994, 4</a>)</span>. Such a parsimonious description, however, may hide some of the complexities of fairly and equitably coming to know what learners know and can do in relation to particular outcomes. Since knowledge of a particular domain cannot be directly observed in a learner, and therefore cannot be quantified, instructors must rely on data gathered during the teaching process to support a particular inference about what a learner <em>probably</em> knows. The data gathered from performance tasks such as exams, essays, portfolios, labs, etc, become <em>evidence</em> when they support an inference about what a learner knows and can do. Hence, <em>all</em> summative assessments are probablistic, not deterministic.</p>
<!--

The NRC models assessment as a triangle composed of three interdependent components of any assessment (Figure 1):  *cognition*, or a model of the domain to be learned; *observation*, or the performance task learners will complete to demonstrate their competence; and an *inference* or *interpretation* of the data produced by the observation. The interdependent nature of the three components requires that both the observation and interpretation components be grounded in the nature of the cognitive model of the domain. For example, if the domain of knowledge is, broadly speaking, statistics, then the observation, or performance task, must elicit responses which require the examinee to demonstrate competence in statistics [@gerritsen-vanleeuwenkampAssessmentQualityTertiary2017]. If the performance task requires the ability to speak Icelandic, a different cognitive domain, then a Swahili-speaking examinee's responses will not be representative of their ability in statistics, but rather their lack of ability to speak Icelandic. Consequently, the examiner will have no basis for making an inference about the examinee's statistical ability; in other terms, the inference would be invalid because the performance task is not aligned with the cognitive model (see also [@biggsWhatStudentDoes1999]).

![alt-text](../assets/images/assessment-triangle.png "Assessment triangle")
Figure 1. Assessment Triangle [@pellegrinoKnowingWhatStudents2001]

-->
<p>More recently, Earl <span class="citation">(<a href="#ref-earlAssessmentLearningUsing2013" role="doc-biblioref">2013</a>)</span> further clarified the role assessment can play in learning by highlighting a distinction between assessment <em>of</em> learning (summative assessment) and assessment <em>for</em> learning (formative assessment by way of feedback) and also distinguishing both of those from assessment <em>as</em> learning (a subset of assessment <em>for</em> learning in which learners employ metacognitive skills to regulate their own learning tasks). Earl’s delineation between types of assessment reflects the modern view that assessment and learning are, or ought to be, tightly integrated. This relationship will be explored in more detail in relation to Biggs’ 3P Model of Teaching and Learning <span class="citation">(<a href="#ref-biggsEnhancingTeachingConstructive1996" role="doc-biblioref">1996</a>, <a href="#ref-biggsWhatStudentDoes1999" role="doc-biblioref">1999</a>)</span>.</p>
<p>The definitions of assessment above are typically understood as being <em>classroom assessment</em>, language which is more readily applied to face-to-face K-12 learning environments as opposed to HE environments mediated by technology. For the purposes of this paper, I will consider assessment <em>of</em> learning and summative assessment to be essentially synonymous, and I will differentiate between assessment <em>for</em> and <em>as</em> learning. I will use the term <em>classroom assessment</em> to differentiate from <em>large-scale assessment</em>, understood to be assessments deployed at levels above individual classrooms, such as school-, system-, or provincial/federal-levels, and I will use <em>online assessment</em> or <em>technology-mediated assessment</em> to refer specifically to classroom assessment in learning environments mediated by technology whether the learners are remote or not.</p>
</div>
<div id="conceptions-of-assessment" class="section level2 unnumbered">
<h2>Conceptions of Assessment</h2>
<p>Instructors in HE typically receive little formal preparation in either teaching practices or assessment during their graduate studies <span class="citation">(<a href="#ref-lipnevichWhatGradesMean2020" role="doc-biblioref">Lipnevich et al. 2020</a>; <a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>)</span>. Consequently, their own practice tends to follow from what they experienced as learners, which likely emphasized high-stakes summative tests which were either in alignment with outdated pedagogical practices or out of alignment with modern pedagogical practices. These prior conceptions of assessment carry significant weight in how HE instructors approach the assessment of learners in their own courses.</p>
<p>Instructors bring to an educational environment a host of influences related to their institutional context, their past experiences with assessment, their own course policies, and affective beliefs or conceptions about assessment and its purposes <span class="citation">(<a href="#ref-biggsTheoryPracticeCognitive1993" role="doc-biblioref">J. B. Biggs 1993</a>; <a href="#ref-brownQueenslandTeachersConceptions2011" role="doc-biblioref">G. T. L. Brown, Lake, and Matters 2011</a>)</span>. These influences play a significant role in determining the approaches taken by both instructors with respect to assessment and learners with respect to how they approach learning tasks <span class="citation">(<a href="#ref-brownQueenslandTeachersConceptions2011" role="doc-biblioref">G. T. L. Brown, Lake, and Matters 2011</a>)</span>. DeLuca et al. <span class="citation">(<a href="#ref-delucaEstablishingFoundationValid2013" role="doc-biblioref">2013</a>)</span> argue that there are categories of conceptions exhibited by K-12 preservice teachers: assessment as testing, assessment as format, assessment purpose, and assessment as process. These conceptions are seen as increasingly complex, with those who see assessment as testing believing that assessment is primarily concerned with summative assessment of learning, usually using teacher-created selected-response tests. Those who see assessment as format tend to focus on whether the assessment is a “performance, product, or objectively-scored assessment” (p. 110). Assessment as purpose is delineated according to the summative/formative binary or Earl’s <span class="citation">(<a href="#ref-earlAssessmentLearningUsing2013" role="doc-biblioref">2013</a>)</span> assessment <em>of/for/as</em> learning model. DeLuca et al. also identify other purposes of assessment such as accountability, gatekeeping, and teacher evaluation. Lastly, assessment as process, which is based on the National Research Council’s description of assessment being a process of reasoning from evidence <span class="citation">(<a href="#ref-pellegrinoKnowingWhatStudents2001" role="doc-biblioref">2001</a>)</span>.</p>
<!--
Offerdahl and Tomanek [-@offerdahlChangesInstructorsAssessment2011] investigated instructors' assessment thinking in relation to being presented with new assessment strategies. They found that changes in assessment *practice* didn't necessarily follow from changes in assessment *thinking* and that there are many factors which influence both assessment thinking and practice.
-->
<p>Fletcher et al. <span class="citation">(<a href="#ref-fletcherFacultyStudentsConceptions2012" role="doc-biblioref">2012</a>)</span> used Brown’s <span class="citation">(<a href="#ref-brownTeachersConceptionsAssessment2017" role="doc-biblioref">2017</a>)</span> abridged <em>Conceptions of Assessment</em> (CoA) questionnaire to measure learners’ and instructors’ conceptions as follows: “assessment makes institutions accountable, assessment makes students accountable, assessment describes improvements in student abilities, assessment improves student learning, assessment improves teaching, assessment is valid, assessment is irrelevant and bad, assessment is irrelevant and ignored, and assessment is irrelevant and inaccurate” (p. 122). They report that instructors were more likely than learners to view assessment as consistent and trustworthy methods to understand and improve learning and that learners were more likely to have negative views of assessment and viewed it as a measure of student and institutional accountability.</p>
<p>Massey et al. <span class="citation">(<a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>)</span> used DeLuca et al.’s <span class="citation">(<a href="#ref-delucaEstablishingFoundationValid2013" role="doc-biblioref">2013</a>)</span> framework of conceptions in their study of HE instructors’ conceptions of assessment before and after an instructional development course focussed on assessment. They also considered the idea that there are two general orientations towards assessment in HE, an “assessment culture” and a “testing culture” <span class="citation"><a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan</a> (<a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">2020</a>)</span>. They report that they saw significant shifts in participants’ conceptions of assessment from more simplistic views of assessment as testing pre-treatment, to more complex and nuanced views or assessment as process post-treatment.</p>
<p>From the literature, it is clear that instructors’ conceptions of assessment are deeply influenced by many internal and external factors and, especially in HE, where there are few constraints on assessment practice <span class="citation">(<a href="#ref-lipnevichWhatGradesMean2020" role="doc-biblioref">Lipnevich et al. 2020</a>)</span>, there are many ways to describe or delineate different conceptions of assessment. Accordingly, there are multiple ways to conceptualize the skills and dispositions that comprise the idea of <em>assessment literacy</em>.</p>
</div>
<div id="assessment-literacy" class="section level2 unnumbered">
<h2>Assessment Literacy</h2>
<!--
Literacy, according to the Oxford English Dictionary, is "competence or knowledge in a particular area" [@oxfordenglishdictionaryLiteracy]. AL is a relatively nascent concept in the educational assessment literature, although some work has been done to conceptualize it, particularly in the context of K-12 teacher preparation [@medlandExaminingAssessmentLiteracy2015], but less so in HE.
-->
<p>The idea of AL is relatively recent in the K-12 literature and is nascent and under-theorized with respect to HE contexts <span class="citation">(<a href="#ref-medlandExaminingAssessmentLiteracy2015" role="doc-biblioref">Medland 2015</a>)</span>. AL has been defined variously as “the skills and knowledge teachers require to measure and support student learning through assessment” <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">DeLuca, LaPointe-McEwan, and Luhanga 2016a</a>)</span>, “a basic understanding of educational assessment and related skills to apply such knowledge to various measures of student achievement” <span class="citation">(<a href="#ref-xuTeacherAssessmentLiteracy2016" role="doc-biblioref">Xu and Brown 2016</a>)</span>, “an individual’s understandings of the fundamental assessment concepts and procedures deemed likely to influence educational decisions” <span class="citation">(<a href="#ref-pophamAssessmentLiteracyOverlooked2011" role="doc-biblioref">Popham 2011</a>)</span> and “a dynamic context-dependent social practice that involves teachers articulating and negotiating classroom and cultural knowledges with one another and with learners, in the initiation, development and practice of assessment to achieve the learning goals of students” <span class="citation">(<a href="#ref-willisConceptualisingTeachersAssessment2013" role="doc-biblioref">Willis, Adie, and Klenowski 2013</a>)</span>. Key to these definitions are the ideas that AL is a complex, multi-faceted construct, that AL requires <em>adequate</em> (not high) levels of psychometric or statistical analyses, and that it is intended to enable learner success.</p>
<p>The recognition of AL as a critical competency for educators was influenced by the growing demands for teacher and school accountability in the post-WWII era in the USA and Canada, particularly the <em>Elementary and Secondary Education Act</em> (ESEA), passed in 1965, and the <em>No Child Left Behind</em> (NCLB) act, passed in 2002 <span class="citation">(see <a href="#ref-delucaPreparingTeachersAge2012" role="doc-biblioref">DeLuca 2012</a> for a detailed discussion)</span>. As such, conceptualizations of AL have tended to be based on sets of standards to which K-12 teachers are obligated. The first set of standards was the <em>Standards for Teacher Competence in Educational Assessment of Students</em> (the <em>Standards</em>), published by a committee of representatives from the American Federation of Teachers, the National Council on Measurement in Education, and the National Education Association <span class="citation">(<a href="#ref-StandardsTeacherCompetence1990" role="doc-biblioref">AFT, NCME, and NEA 1990</a>)</span>. The <em>Standards</em> are a list of seven skills expected of teachers:</p>
<ol style="list-style-type: decimal">
<li>Teachers should be skilled in <em>choosing</em> assessment methods appropriate for instructional decisions.</li>
<li>Teachers should be skilled in <em>developing</em> assessment methods appropriate for instructional decisions.</li>
<li>The teacher should be skilled in administering, scoring, and interpreting the results of both externally-produced and teacher-produced assessment methods.</li>
<li>Teachers should be skilled in using assessment results when making decisions about individual students, planning teaching, developing curriculum, and school improvement.</li>
<li>Teachers should be skilled in developing valid pupil grading procedures which use pupil assessments.</li>
<li>Teachers should be skilled in communicating assessment results to students, parents, other lay audiences, and other educators.</li>
<li>Teachers should be skilled in recognizing unethical, illegal, and otherwise inappropriate assessment methods and uses of assessment information.</li>
</ol>
<p>Shortly after the publication of the <em>Standards</em>, the term <em>assessment literacy</em> appeared in the literature with Stiggins’ <span class="citation">(<a href="#ref-stigginsAssessmentLiteracy1991" role="doc-biblioref">1991</a>)</span> article called <em>Assessment Literacy</em>. Stiggins initial article was an account of his observation that teacher education programs at the time spent very little time training teachers in the methods and dispositions of educational measurement. Stiggins followed this with another article <span class="citation">(<a href="#ref-stigginsAssessmentLiteracy21st1995" role="doc-biblioref">Stiggins 1995</a>)</span> where he outlined five characteristics of sound assessments, which:</p>
<ol style="list-style-type: decimal">
<li>arise from and serve clear purposes;</li>
<li>arise from and reflect clear and appropriate achievement targets;</li>
<li>rely on a proper assessment method, given the purpose and the target;</li>
<li>sample student achievement appropriately; and</li>
<li>control for all relevant sources of bias and distortion. <span class="citation">(<a href="#ref-stigginsAssessmentLiteracy21st1995" role="doc-biblioref">1995, 240</a>)</span></li>
</ol>
<p>At around the same time, a group of Canadian educators published the <em>Principles for Fair Student Assessment Practices for Education in Canada</em>, Part A of which was a list of 37 guidelines related to five principles of fair student classroom assessment and was based on the 1990 <em>Standards</em> (see Appendix A). Part B was focused on externally-developed standardized tests.</p>
<p>While these three sets of recommendations varied widely in their granularity, all tended to reflect an emphasis on the 20th century conceptions of curriculum which prioritized linear and sequential teaching of knowledge followed sometime later by selected-response tests of knowledge. This required teachers to be literate in the psychometric skills required to administer and interpret these tests <span class="citation">(<a href="#ref-delucaTeacherAssessmentLiteracy2016" role="doc-biblioref">DeLuca, LaPointe-McEwan, and Luhanga 2016b</a>; <a href="#ref-shepardRoleAssessmentLearning2000" role="doc-biblioref">Shepard 2000</a>; <a href="#ref-xuTeacherAssessmentLiteracy2016" role="doc-biblioref">Xu and Brown 2016</a>)</span> or assessment <em>of</em> learning. One notable distinction between the sets of standards is that the Canadian committee specifically noted their principles could be applied to K-12 as well as higher education, although the latter context would require changes in how assessment data are reported.</p>
<p>Twenty years following the publication of the <em>Standards</em>, Brookhart <span class="citation">(<a href="#ref-brookhartEducationalAssessmentKnowledge2011" role="doc-biblioref">2011</a>)</span> argued that the <em>Standards</em> had become outdated because they did not address either the growing practices and ideas of formative assessment (assessment <em>for</em> and <em>as</em> learning) or standards-based assessment and that they needed to be revised. Brookhart suggested a list of 11 skills (see Appendix A) to adjust the focus of the 1990 <em>Standards</em> to be in greater alignment with more modern conceptions of assessment.</p>
<p>Finally, in 2015, the Joint Committee on Standards for Educational Evaluation (JCSEE), with key representatives from both Canada and the USA, published the most recent set of standards, called the <em>Classroom Assessment Standards for PreK-12 Teachers</em> (see Appendix A). The JCSEE standards are grouped into three broad domains (foundations, use, and quality), each with five or six related standards. Despite the similarities to the <em>Principles for Fair Student Assessment Practices for Education in Canada</em>, including at least one common committee member, the JCSEE Standards are specifically not intended for use in HE.</p>
<p>As traditional conceptions of assessment and the standards expected of teachers, grounded in behaviourism and the need for objectivity tended to focus on assessment as a set of skill-based competencies to be employed by instructors, so AL could be defined as a set of sequential tasks in which instructors should engage to ensure objectivity and fairness (e.g. <span class="citation">(<a href="#ref-natrielloImpactEvaluationProcesses1987" role="doc-biblioref">Natriello 1987</a>)</span>). Recently, as curriculum and pedagogy have changed, several researchers have proposed models related to AL grounded in socio-constructivist views of learning <span class="citation">(<a href="#ref-delucaPreparingTeachersAge2012" role="doc-biblioref">DeLuca 2012</a>; <a href="#ref-pastoreTeacherAssessmentLiteracy2019" role="doc-biblioref">Pastore and Andrade 2019</a>; <a href="#ref-xuTeacherAssessmentLiteracy2016" role="doc-biblioref">Xu and Brown 2016</a>)</span>. DeLuca’s <span class="citation">(<a href="#ref-delucaPreparingTeachersAge2012" role="doc-biblioref">2012</a>)</span> model, developed in the context of the <em>No Child Left Behind</em> accountability mandate in K-12 schools in the USA, is a coherent lens through which to understand how a pre-service teacher could develop assessment expertise throughout their teacher education program. DeLuca frames AL within Fostaty Young and Wilson’s <span class="citation">(<a href="#ref-fostatyyoungAssessmentLearningICE2000" role="doc-biblioref"><strong>fostatyyoungAssessmentLearningICE2000?</strong></a>)</span> ICE model which presents an integrated progression of learning through three levels of complexity, ideas, connections, and extensions. Underlying the three levels is a foundational level, where teacher candidates come to know and understand assessment as being situated within a model of teaching and learning, experiential and inclusive pedagogies, and an orientation towards continual professional learning. At the <em>ideas</em> level, teacher candidates gain expertise in the big ideas related to assessment (theories of learning, planning educational experiences, classroom assessment, and issues of diversity and inclusion). At the <em>connections</em> level, teacher candidates begin to construct their own cognitive conceptions of assessment as they integrate their own past experiences with their new knowledge and add their experience in practica. At the <em>extensions</em> level, teacher candidates begin to hone their practice as they deepen their understanding of assessment.</p>
<p>Willis et al. <span class="citation">(<a href="#ref-willisConceptualisingTeachersAssessment2013" role="doc-biblioref">2013</a>)</span> describe assessment literacy in alignment with Bernstien’s <span class="citation">(<a href="#ref-bernsteinVerticalHorizontalDiscourse1999" role="doc-biblioref">1999</a>)</span> idea that there are “horizontal” and “vertical” discourses (p. 159) with respect to assessment. A horizontal discourse is the local, contextualized discourse around assessment which influences local practice, while a vertical discourse is the formalized, structured discourse on assessment in the literature and other more authoritative venues. They argue that any given instructor’s conceptions and practice of assessment will be informed by and negotiated within multiple horizontal and vertical discourses. Similarly, Xu and Brown’s <span class="citation">(<a href="#ref-xuTeacherAssessmentLiteracy2016" role="doc-biblioref">2016</a>)</span> model, teacher AL in practise (TALiP) presents a pathway for pre-service teachers to gain expertise in assessment. Based on a synthesis of 100 peer-reviewed publications between 1985 and 2015, they present a six-component model:</p>
<ol style="list-style-type: decimal">
<li>the knowledge base</li>
<li>teacher conceptions of assessment,</li>
<li>institutional and socio-cultural contexts,</li>
<li>teacher assessment literacy in practice,</li>
<li>teacher learning, and</li>
<li>teacher identity as assessor.
Finally, Pastore and Andrade <span class="citation">(<a href="#ref-pastoreTeacherAssessmentLiteracy2019" role="doc-biblioref">2019</a>)</span> developed their model through a Delphi inquiry of 35 international experts in educational assessment and teacher education. They propose a model with three dimensions, conceptual, praxeological, and socio-emotional.</li>
</ol>
<p>The authors of each of these three models recognize that AL is conceptualized as a multi-dimensional construct encompassing psychometric skills, affective beliefs and values, external and regulatory environments, and socially negotiated practices. However, these models are largely specific to the K-12 environment in general, and more specifically related to the preparation of K-12 teachers. Given that there are very few parallels between the preparation of K-12 teachers for their role and the preparation of HE instructors for theirs, it seems that a framework for understanding AL among HE instructors should consider their general lack of formal preparation for teaching or assessing learning <span class="citation">(<a href="#ref-lipnevichWhatGradesMean2020" role="doc-biblioref">Lipnevich et al. 2020</a>; <a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>)</span>.</p>
<p>Bearman et al. <span class="citation">(<a href="#ref-bearmanSupportAssessmentPractice2016" role="doc-biblioref">2016</a>)</span> proposed a model for assessment decision-making in Australian HE, the Assessment Design Decisions Framwork (ADDF) in which the authors acknowledge the difficulty in translating idealized beliefs about assessment into actual practice as well as the lack of literature regarding <em>how</em> HE instructors go about designing assessments. While not specifically an AL framework, there are overlaps in terms of the dimensions they identified. They argue, like those previously mentioned, that assessment is a complex and messy process. Unique to their model is the idea that there is often a disconnect between what instructors know or believe to be true about assessment, and how instructors’ practice is impacted (or not) by their beliefs. Additionally, they argue, in alignment with Price et al. <span class="citation">(<a href="#ref-priceIfWasGoing2011" role="doc-biblioref">2011</a>)</span>, that AL ought to be considered from both instructor and learner perspectives. Their framework is comprised of six dimensions:</p>
<ol style="list-style-type: decimal">
<li>purposes of assessments</li>
<li>contexts of assessments</li>
<li>learner outcomes</li>
<li>tasks</li>
<li>feedback processes</li>
<li>interactions</li>
</ol>
<p>Herppich et al. <span class="citation">(<a href="#ref-herppichTeachersAssessmentCompetence2018" role="doc-biblioref">2018</a>)</span> frame a model of assessment competence beginning with the idea that the purpose of educational assessment is to inform both formative (ongoing learning) and summative (credentialling or certification) decisions locating the educational decision subsequent and subordinate to the judgement. They use an example where a teacher observes a learner struggling with a test and might come to an appropriate judgement of a learner’s knowledge, but make an inappropriate instructional decision based on that information. This distinction is useful when, like Herppich and colleagues, the construct under investigation is <em>assessment competence</em>, which is observed <em>after</em> instructional activities. However, in the present paper, <em>conceptions of assessment</em> and <em>AL</em> are the relevant constructs and they are embedded in a model of teaching and learning where conceptions of assessment <em>precede</em> instructional activities.</p>
<p>DeLuca et al.’s model <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">2016a</a>)</span>, <em>approaches to classroom assessment</em> defines and conceptualizes the <em>assessment</em> component of Biggs’ 3P model <span class="citation">(<a href="#ref-biggsWhatStudentDoes1999" role="doc-biblioref">1999</a>; <a href="#ref-biggsTheoryPracticeCognitive1993" role="doc-biblioref">1993</a>)</span> and so aligns well with a concise model of teaching and learning in higher education. The <em>Approaches to Classroom Assessment</em> model is based on the JCSEE standards <span class="citation">(<a href="#ref-klingerClassroomAssessmentStandards2015" role="doc-biblioref">Klinger et al. 2015</a>)</span> and describes four themes of AL, each with three dimensions. The model represents somewhat of a break from previous models in that it references <em>approaches to assessment</em> rather than <em>assessment literacies</em>. This is a reflection of the authors’ view that language around <em>literacies</em> and <em>competencies</em> may indicate a reliance on “correct” views or methods rather than the complex array of influences that lead to multiple legitimate approaches as identified in the literature <span class="citation">(<a href="#ref-delucaDifferentialSituatedView2019" role="doc-biblioref">DeLuca, Coombs, et al. 2019</a>; <a href="#ref-willisConceptualisingTeachersAssessment2013" role="doc-biblioref">Willis, Adie, and Klenowski 2013</a>)</span>.</p>
<p>The themes DeLuca et al <span class="citation">(<a href="#ref-delucaExploringAssessmentCultures2021" role="doc-biblioref">2021, 10</a>)</span> describe along with their associated dimensions are listed below and illustrated in figure 2:</p>
<ol style="list-style-type: decimal">
<li>Assessment purposes.<br />
</li>
</ol>
<ul>
<li>Assessment <em>of</em> learning
<ul>
<li>Teachers’ use of evidence to summate student learning and assign a grade in relation to students’ achievement of learning objectives<br />
</li>
</ul></li>
<li>Assessment <em>for</em> learning
<ul>
<li>Teachers’ and students’ use of evidence to provide feedback on progress towards learning objectives (i.e., inform next steps for learning and instructions).<br />
</li>
<li>Involves both teacher-directed and student-centred approaches to formative assessment.<br />
</li>
</ul></li>
<li>Assessment <em>as</em> learning
<ul>
<li>Focuses on how the student is learning by providing feedback or experiences that foster students’ metacognitive abilities and learning skills (e.g., self-assessment, goal-setting, learning plans).<br />
</li>
<li>Involves teachers but is primarily student-centred.<br />
</li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Assessment process<br />
</li>
</ol>
<ul>
<li>Design
<ul>
<li>Focuses on the development of reliable assessments and items that measure student learning in relation to learning objectives.<br />
</li>
</ul></li>
<li>Use/scoring
<ul>
<li>Focuses of the adjustment and use of scoring protocols and grading schemes to respond to assessment scenarios.<br />
</li>
</ul></li>
<li>Communication
<ul>
<li>focuses on the interpretation of assessment results and feedback through communication to students and parents.<br />
</li>
</ul></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Assessment fairness<br />
</li>
</ol>
<ul>
<li>Standard
<ul>
<li>Maintains the equal assessment protocols for all students.<br />
</li>
</ul></li>
<li>Equitable
<ul>
<li>Differentiates assessment protocols for formally identified students (i.e., special education or English language learners)<br />
</li>
</ul></li>
<li>Differentiated
<ul>
<li>Individualizes learning opportunities and assessments that address each student’s unique learning needs and goals.<br />
</li>
</ul></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Assessment theory<br />
</li>
</ol>
<ul>
<li>Consistent
<ul>
<li>Works to ensure consistency in results within assessments, across time periods, and between teachers.<br />
</li>
</ul></li>
<li>Contextual
<ul>
<li>Works to ensure assessment or evaluation measures what it claims to measure (i.e., learning objectives) and promote valid interpretations of results.<br />
</li>
</ul></li>
<li>Balanced
<ul>
<li>Works to ensure consistency in measuring what an assessment or evaluation intends to measure, and degree to which an assessment or evaluation measures what it claims to measure.</li>
</ul></li>
</ul>
<p><img src="images/approaches-to-assessment.png" title="Approaches to Classroom Assessment" alt="alt-text" />
Figure 2. Approaches to Classroom Assessment. <span class="citation">(<a href="#ref-delucaExploringAssessmentCultures2021" role="doc-biblioref">DeLuca, Rickey, and Coombs 2021, 10</a>)</span></p>
</div>
<div id="assessment-and-technology" class="section level2 unnumbered">
<h2>Assessment and Technology</h2>
<p>Educational technologies are often viewed and reported on with a distinct positivity bias <span class="citation">(<a href="#ref-irvineLandscapeMergingModalities2020" role="doc-biblioref">Irvine 2020</a>)</span> wherein ‘new’ technologies are presumed to represent progress and will inevitably have a positive effect on learning. This can be seen in the titles given to some initiatives, such as “Technology-Enhanced Assessment” <span class="citation">(<a href="#ref-oldfieldAssessmentDigitalAge2012" role="doc-biblioref">Oldfield et al. 2012</a>; <a href="#ref-timmisRethinkingAssessmentDigital2016" role="doc-biblioref">Timmis et al. 2016</a>)</span>, “IT-enabled assessment” <span class="citation">(<a href="#ref-webbAssessmentTwentyFirstCentury2018" role="doc-biblioref">Webb and Ifenthaler 2018</a>)</span>, or “technology-rich” <span class="citation">(<a href="#ref-linAssessingLearningTechnologyrich2020" role="doc-biblioref">Lin et al. 2020</a>)</span>. As such, I will use the more neutral term “technology-mediated” to indicate that adding digital technology to an assessment environment does not necessarily improve that environment. Similar to assessment practices being grounded in (both philosophically, as in ‘based upon,’ and figuratively, as in ‘stuck in’) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment, so too, many educational technologies are grounded in (based upon and stuck in) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment. This can be seen in the progressively more advanced technologies beginning with Pressey’s teaching machines <span class="citation">(<a href="#ref-benjaminHistoryTeachingMachines1988" role="doc-biblioref">Benjamin 1988</a>; <a href="#ref-presseyMachineAutomaticTeaching1927" role="doc-biblioref">Pressey 1927</a>; <a href="#ref-wattersTeachingMachinesHistory2021" role="doc-biblioref">A. Watters 2021</a>)</span> which was built to automate the process of “drilling” learners in an effort to teach them some concept. The machine needed to be pre-programmed with a series of selected-response questions along with distractors and correct answers. As a learner answered each question, the machine was programmed to match the response to the programmed correct response, and if it matched, the learner was determined to have “mastered” that question and it was dropped from the bank of questions the learner had not yet mastered. If it did not match, the question was cycled back into the bank to be repeated. Clearly, this technology was promoted as a tool to be used to modernize and increase the efficiency of tasks that aligned with the dominant pedagigical paradigm at the time. A second example, although not one marketed directly to schools, but to parents, was the <em>Speak &amp; Spell</em>, released in 1978 by Texas Instruments <span class="citation">(<a href="#ref-braguinskiArchiveCommunicationInteractive2018" role="doc-biblioref">Braguinski 2018</a>; <a href="#ref-frantzSpeakSpell2014" role="doc-biblioref">Frantz 2014</a>)</span>, which represented an advance in technology and an increase in efficiency, as the <em>Speak &amp; Spell</em> could be programmed to store and reproduce voice recordings of words as well as multiple recordings of feedback messages <span class="citation">(<a href="#ref-frantzSpeakSpell2014" role="doc-biblioref">Frantz 2014</a>)</span>. While the <em>Speak &amp; Spell</em> was a leap forward in processing power, memory storage, and therefore complexity, the underlying pedagogy remained identical to that of Pressey’s teaching machine <span class="citation">(<a href="#ref-wattersSpeakSpellHistory2015" role="doc-biblioref">Audrey Watters 2015</a>)</span>. Moving forward again, and modern technologies are vastly more powerful than teaching machines or the <em>Speak &amp; Spell</em> and power very complex adaptive tests, such as the NCLEX-RN, the national licensing exam for Registered Nurses in Canada and the USA <span class="citation">(<a href="#ref-smithglasgowStandardizedTestingNursing2019" role="doc-biblioref">Smith Glasgow, Dreher, and Schreiber 2019</a>)</span>. These advances in both hardware and software allow for still greater efficiencies in testing, yet the NCLEX-RN must still be programmed with selected-response questions, their distractors, and correct responses, all still in alignment with behavioural models of pedagogy. The NCLEX-RN is an example of a large-scale, standardized assessment (LSA), so is not parallel to the classroom assessment practices which are the subject of this paper, but I mention it here to draw the distinction between professionally-created LSAs and most instructor-created classroom assessments. The NCLEX-RN is continually revised and updated and reflects very robust psychometric properties (validity and reliability) <span class="citation">(<a href="#ref-smithglasgowStandardizedTestingNursing2019" role="doc-biblioref">Smith Glasgow, Dreher, and Schreiber 2019</a>)</span>. More importantly, however, the NCLEX-RN has been updated to “shift away from a primary focus on content and the indirect testing of clinical judgment to a major focus on clinical judgment” <span class="citation">(<a href="#ref-caputiReflectionsNextGeneration2019" role="doc-biblioref">Caputi 2019, 2</a>)</span>. Caputi, in her reflections on the next-generation NCLEX-RN (NGN) asks 2 questions (the second of which is most relevant here): “1) Are students ready for this type of NCLEX? 2) If our students already pass the NCLEX, can we keep doing the same type of preparation for the NGN?” (p. 2). Her answer to both questions is “No.” She goes on to argue,</p>
<blockquote>
<p>So, what can faculty do? <i>I propose that nurse faculty, at all levels of nursing education, revise their curricula to teach a detailed thinking process that students must employ over and over throughout the nursing curriculum. Just as students practice psychomotor skills until they are perfected, they must do so with thinking skills and strategies A new model for teaching clinical judgment is needed”,</i> (p. 2, emphasis in original).</p>
</blockquote>
<p>In this case, the technology-mediated assessment instrument has been designed to measure 21st century skills, and Caputi recognizes that if pre-service nurses are going to have to pass the NGN, nursing instructors will need to realign their pedagogy. This example illustrates how LSAs exert pressure on instructors and schools to adjust their pedagogy, in the case of nursing, to encourage 21st century pedagogy that teaches thinking skills. The opposite is also true, however, in that LSAs which emphasize the lower-level cognitive skills prized by the behaviourists cause instructors to match their pedagogy to that model <span class="citation">(<a href="#ref-caputiReflectionsNextGeneration2019" role="doc-biblioref">Caputi 2019</a>; <a href="#ref-clarke-miduraAssessmentTechnologyChange2010" role="doc-biblioref">Clarke-Midura and Dede 2010</a>; <a href="#ref-delucaExploringAssessmentCultures2021" role="doc-biblioref">DeLuca, Rickey, and Coombs 2021</a>; <a href="#ref-pellegrinoPerspectivesIntegrationTechnology2010" role="doc-biblioref">Pellegrino and Quellmalz 2010</a>)</span>. Further, the American Educational Research Association (AERA), the National Council on Measurement in Education (NCME), and the American Psychological Association (APA) argue that LSAs tend to have other negative effects on education systems, namely the narrowing of curricula (teaching to the test), reduced instructional strategies (previously mentioned), higher dropout rates, and the enactment of policies and practices that increase test scores without increasing learning <span class="citation">(<a href="#ref-aeraStandardsEducationalPsychological2014" role="doc-biblioref">2014</a>)</span>. The NCLEX-RN notwithstanding, many implementations of technology in assessment remain focused on increasing the efficiencies of summative test administration <span class="citation">(<a href="#ref-broadfootAssessmentTwentyFirstCenturyLearning2016" role="doc-biblioref">Broadfoot 2016</a>; <a href="#ref-pellegrinoPerspectivesIntegrationTechnology2010" role="doc-biblioref">Pellegrino and Quellmalz 2010</a>; <a href="#ref-webbAssessmentTwentyFirstCentury2018" role="doc-biblioref">Webb and Ifenthaler 2018</a>)</span>.</p>
<!--
There are two branches of literature related to assessment in technology-mediated HE. The first is related to the application of technology to increasing the efficiency and predictive power of algorithmic assessment tools such as automated essay scoring [@kumarExplainableAutomatedEssay2020], automated item generation [@shinMultipleChoiceItemDistractor2019], adaptive testing [@bornEvaluatingDifferentEquating2019], predictive statistical analyses [@gorgunPolytomousScoringApproach2021] and gamified immersive environments [@clarke-miduraAssessmentTechnologyChange2010]. The other branch is more focussed on the power of technology to mediate formative assessment practices through learner- and community-centred environments [@conradAssessmentStrategiesOnline2018; @gikandiTheoryFormativeAssessment2015; @gikandiOnlineFormativeAssessment2011]. These branches do have some common characteristics in that researchers in both advocate for greater varieties of assessments compared to pencil-and-paper tests, greater efficiency and accuracy in grading, increased ability to capture and assess learner processes, and greater flexibility in the timing of assessments as technology allows for asynchronous interactions [@conradAssessmentStrategiesOnline2018].
-->
</div>
<div id="impact-on-learners" class="section level2 unnumbered">
<h2>Impact on Learners</h2>
<p>Aside from the effects on learners’ approaches to their learning, assessment, particularly summative assessment, has profound effects in other ways, such as determining status or progression in a course or program, determining eligibility for scholarships and awards, determining career paths and on other affective constructs such as motivation <span class="citation">(<a href="#ref-crooksImpactClassroomEvaluation1988" role="doc-biblioref">Crooks 1988</a>)</span> and anxiety <span class="citation">(<a href="#ref-birenbaumAssessmentInstructionPreferences2007" role="doc-biblioref">Menucha Birenbaum 2007</a>; <a href="#ref-harlenSystematicReviewImpact2002" role="doc-biblioref">Harlen and Deakin Crick 2002</a>)</span>. Jones et al. <span class="citation">(<a href="#ref-jonesStudentWellbeingAssessment2021" role="doc-biblioref">2021</a>)</span> report that assessment practices impact learners’ well-being, which they define as including physical and mental health as well as the ability for learners to “fully exercise their cognitive, emotional, physical and social powers, leading to flourishing” (p. 439). They note that summative assessment practices are associated in the literature with “anxiety, depression, disordered eating, self-harm, panic attacks, burnout, … thoughts of suicide … disordered sleep, loss of appetite, physical inactivity, poor physical health, … substance misuse … poorer productivity, motivation and test scores” and that changing from norm-referenced scoring (where learners are ranked relative to each other) to criterion-referenced or pass-fail scoring is associated with lower levels of stress and anxiety among medical students in the USA.</p>
</div>
<div id="theoretical-framework-overview" class="section level2 unnumbered">
<h2>Theoretical Framework Overview</h2>
<p>Given the wide variety of models, mostly in K-12 contexts but increasingly in HE, and the degree to which there appears to be overlap between those models, I take the position that introducing a new model into an already crowded landscape would serve little purpose. Furthermore, models do exist which have been validated empirically <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">DeLuca, LaPointe-McEwan, and Luhanga 2016a</a>)</span> and the authors of which have indicated that, despite the model being developed for K-12 contexts, adjusting the model to fit the HE context would be a worthwhile contribution to the literature (personal communication).</p>
<p>Here, I present an argument that DeLuca et al.’s idea of <em>approaches to classroom assessment</em> <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">2016a</a>)</span> aligns well with the landscape of teaching, learning, and assessment in HE and may provide a blueprint for assessment reform in HE. In order to situate <em>Approaches to Classroom Assessment</em> within the context of teaching and learning in HE, I begin with Biggs’ <span class="citation">(<a href="#ref-biggsWhatStudentDoes1999" role="doc-biblioref">J. Biggs 1999</a>; <a href="#ref-biggsTheoryPracticeCognitive1993" role="doc-biblioref">J. B. Biggs 1993</a>)</span> 3P model of teaching and learning (see Figure 3) which presents three stages of the teaching and learning process. In the first stage, <em>presage</em>, Biggs models both learners and the teaching context. The learners’ contexts include their previous learning, cognitive abilities, and their beliefs about the learning context, and all of these influence their preferred approaches to learning. The teaching context includes the instructors beliefs and conceptions of the curriculum and assessment, and also institutional and sometimes regulatory requirements, and each of these influence the instructor’s approach to teaching, and more to the point for this paper, their approach to assessment. The <em>process</em> stage represents the activities in which learners engage in order to meet the learning outcomes. Biggs notes that learners tend to either take <em>surface</em> approaches, where they use low-level cognitive skills (memorization of facts) in activities that require high-level cognitive skills (analysis or critique), or they take deep approaches, where they use high-level cognitive skills for activities which require them. Learner activity in this stage is influenced by their own backgrounds, abilities, and affective views of the purpose of the task and it is influenced by the choices the instructor makes based on their presage influences. In this way, Earl’s <span class="citation">(<a href="#ref-earlAssessmentLearningUsing2013" role="doc-biblioref">2013</a>)</span> delineation between assessment <em>of</em>, <em>for</em>, and <em>as</em> learning can be seen to influence student learning approaches. For example, researchers note that when instructors’ approach prioritizes assessment <em>of</em> learning, learners tend to take a surface approach to their learning <span class="citation">(<a href="#ref-birenbaumAssessmentInstructionPreferences2007" role="doc-biblioref">Menucha Birenbaum 2007</a>)</span>. Finally, the <em>product</em> stage represents the learners’ achievement in relation to the outcomes. Moving left to right, each of the first two stages represents an input into the system, and the third, an output. However, as the arrows between the components are two-way, the product stage also becomes an input, the results of which are fed back into the system (right to left) informing learners of their status in relation to the outcomes as well as the utility of their learning activities. It also informs the instructor about the nature of the learners’ achievement and the utility of the learning tasks. The <em>achievement</em> component provides formative feedback directly to both instructors and learners, informing both about next steps in the learning process. It also provides insight into the learning activities and, when that is fed back to learners, provides metacognitive cues as to the strategies learners might use in future tasks. It also provides information to instructors about the nature of the learning activities and how their approaches to assessment are impacting learning.</p>
<p><img src="images/3p-model.png" title="3P Model of Teaching and Learning" alt="alt-text" />
Figure 3. 3P Model of Teaching and Learning. <span class="citation">(<a href="#ref-biggsWhatStudentDoes1999" role="doc-biblioref">J. Biggs 1999</a>; <a href="#ref-biggsTheoryPracticeCognitive1993" role="doc-biblioref">J. B. Biggs 1993</a>)</span></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-StandardsTeacherCompetence1990" class="csl-entry">
AFT, NCME, and NEA. 1990. <span>“Standards for <span>Teacher Competence</span> in <span>Educational Assessment</span> of <span>Students</span>.”</span> <a href="https://eric.ed.gov/?id=ED323186">https://eric.ed.gov/?id=ED323186</a>.
</div>
<div id="ref-bearmanSupportAssessmentPractice2016" class="csl-entry">
Bearman, Margaret, Phillip Dawson, David Boud, Sue Bennett, Matt Hall, and Elizabeth Molloy. 2016. <span>“Support for Assessment Practice: Developing the <span>Assessment Design Decisions Framework</span>.”</span> <em>Teaching in Higher Education</em> 21 (5): 545–56. <a href="https://doi.org/gjsn3g">https://doi.org/gjsn3g</a>.
</div>
<div id="ref-benjaminHistoryTeachingMachines1988" class="csl-entry">
Benjamin, Ludy T. 1988. <span>“A History of Teaching Machines.”</span> <em>American Psychologist</em> 43 (9): 703–12. <a href="https://doi.org/djgzjr">https://doi.org/djgzjr</a>.
</div>
<div id="ref-bernsteinVerticalHorizontalDiscourse1999" class="csl-entry">
Bernstein, Basil. 1999. <span>“Vertical and <span>Horizontal Discourse</span>: <span>An Essay</span>.”</span> <em>British Journal of Sociology of Education</em> 20 (2): 157–73. <a href="https://doi.org/ftmsvc">https://doi.org/ftmsvc</a>.
</div>
<div id="ref-biggsTheoryPracticeCognitive1993" class="csl-entry">
Biggs, J. B. 1993. <span>“From <span>Theory</span> to <span>Practice</span>: <span>A Cognitive Systems Approach</span>.”</span> <em>Higher Education Research &amp; Development</em> 12 (1): 73–85. <a href="https://doi.org/ccdmd9">https://doi.org/ccdmd9</a>.
</div>
<div id="ref-biggsEnhancingTeachingConstructive1996" class="csl-entry">
Biggs, John. 1996. <span>“Enhancing Teaching Through Constructive Alignment.”</span> <em>Higher Education</em> 32 (3): 347–64. <a href="https://doi.org/chx3gp">https://doi.org/chx3gp</a>.
</div>
<div id="ref-biggsWhatStudentDoes1999" class="csl-entry">
———. 1999. <span>“What the <span>Student Does</span>: Teaching for Enhanced Learning.”</span> <em>Higher Education Research &amp; Development</em> 18 (1): 57–75. <a href="https://doi.org/drgphk">https://doi.org/drgphk</a>.
</div>
<div id="ref-birenbaumAssessmentInstructionPreferences2007" class="csl-entry">
Birenbaum, Menucha. 2007. <span>“Assessment and <span>Instruction Preferences</span> and <span>Their Relationship</span> with <span>Test Anxiety</span> and <span>Learning Strategies</span>.”</span> <em>Higher Education</em> 53 (6): 749–68. <a href="https://doi.org/cqgvs8">https://doi.org/cqgvs8</a>.
</div>
<div id="ref-blackAssessmentClassroomLearning1998" class="csl-entry">
Black, Paul, and Dylan Wiliam. 1998. <span>“Assessment and <span>Classroom Learning</span>.”</span> <em>Assessment in Education: Principles, Policy &amp; Practice</em> 5 (1): 7–74. <a href="https://doi.org/fpnss4">https://doi.org/fpnss4</a>.
</div>
<div id="ref-bloomLearningMasteryInstruction1968" class="csl-entry">
Bloom, Benjamin. 1968. <span>“Learning for <span>Mastery</span>. <span>Instruction</span> and <span>Curriculum</span>. <span>Regional Education Laboratory</span> for the <span>Carolinas</span> and <span>Virginia</span>, <span>Topical Papers</span> and <span>Reprints</span>, <span>Number</span> 1.”</span> <em>Evaluation Comment</em> 1 (2): 12. <a href="https://eric.ed.gov/?id=ED053419">https://eric.ed.gov/?id=ED053419</a>.
</div>
<div id="ref-braguinskiArchiveCommunicationInteractive2018" class="csl-entry">
Braguinski, Nikita. 2018. <span>“An (<span>An</span>)<span>Archive</span> of <span>Communication</span>: <span>Interactive Toys</span> as <span>Interlocutor</span>.”</span> <em>Communication +1</em> 7 (1): 19. <a href="https://pure.mpg.de/rest/items/item_3326892/component/file_3327086/content">https://pure.mpg.de/rest/items/item_3326892/component/file_3327086/content</a>.
</div>
<div id="ref-broadfootAssessmentTwentyFirstCenturyLearning2016" class="csl-entry">
Broadfoot, Patricia. 2016. <span>“Assessment for <span>Twenty</span>-<span>First</span>-<span>Century Learning</span>: <span>The Challenges Ahead</span>.”</span> In <em>Learning, <span>Design</span>, and <span>Technology</span></em>, edited by Michael J Spector, Barbara B Lockee, and Marcus D. Childress, 1–23. <span>Cham</span>: <span>Springer International Publishing</span>. <a href="https://doi.org/10.1007/978-3-319-17727-4_64-1">https://doi.org/10.1007/978-3-319-17727-4_64-1</a>.
</div>
<div id="ref-brookhartEducationalAssessmentKnowledge2011" class="csl-entry">
Brookhart, Susan M. 2011. <span>“Educational <span>Assessment Knowledge</span> and <span>Skills</span> for <span>Teachers</span>.”</span> <em>Educational Measurement: Issues and Practice</em> 30: 3–12. <a href="https://doi.org/cwcqj4">https://doi.org/cwcqj4</a>.
</div>
<div id="ref-brownTeachersConceptionsAssessment2017" class="csl-entry">
Brown, Gavin. 2017. <span>“Teachers <span>Conceptions</span> of <span>Assessment</span> - <span>Secondary Schools Long</span> and <span>Abridged</span>.”</span> <a href="https://doi.org/gj4tz6">https://doi.org/gj4tz6</a>.
</div>
<div id="ref-brownQueenslandTeachersConceptions2011" class="csl-entry">
Brown, Gavin T. L., Robert Lake, and Gabrielle Matters. 2011. <span>“Queensland Teachers’ Conceptions of Assessment: <span>The</span> Impact of Policy Priorities on Teacher Attitudes.”</span> <em>Teaching and Teacher Education</em> 27 (1): 210–20. <a href="https://doi.org/c3k8f5">https://doi.org/c3k8f5</a>.
</div>
<div id="ref-caputiReflectionsNextGeneration2019" class="csl-entry">
Caputi, Linda J. 2019. <span>“Reflections on the <span>Next Generation NCLEX</span> with <span>Implications</span> for <span>Nursing Programs</span>.”</span> <em>Nursing Education Perspectives</em> 40 (1): 2–3. <a href="https://doi.org/d8wq">https://doi.org/d8wq</a>.
</div>
<div id="ref-clarke-miduraAssessmentTechnologyChange2010" class="csl-entry">
Clarke-Midura, Jody, and Chris Dede. 2010. <span>“Assessment, <span>Technology</span>, and <span>Change</span>.”</span> <em>Journal of Research on Technology in Education</em> 42 (3): 309–28. <a href="https://doi.org/ghdnhh">https://doi.org/ghdnhh</a>.
</div>
<div id="ref-crooksImpactClassroomEvaluation1988" class="csl-entry">
Crooks, Terence J. 1988. <span>“The <span>Impact</span> of <span>Classroom Evaluation Practices</span> on <span>Students</span>.”</span> <em>Review of Educational Research</em> 58 (4): 438–81. <a href="https://doi.org/dvd8nf">https://doi.org/dvd8nf</a>.
</div>
<div id="ref-delucaPreparingTeachersAge2012" class="csl-entry">
———. 2012. <span>“Preparing Teachers for the Age of Accountability: <span>Toward</span> a Framework for Assessment Education.”</span> <em>Action in Teacher Education</em> 34: 576–91. <a href="https://doi.org/10.1080/01626620.2012.730347">https://doi.org/10.1080/01626620.2012.730347</a>.
</div>
<div id="ref-delucaEstablishingFoundationValid2013" class="csl-entry">
DeLuca, Christopher, Teresa Chavez, and Chunhua Cao. 2013. <span>“Establishing a Foundation for Valid Teacher Judgement on Student Learning: The Role of Pre-Service Assessment Education.”</span> <em>Assessment in Education: Principles, Policy &amp; Practice</em> 20 (1): 107–26. <a href="https://doi.org/gj5v98">https://doi.org/gj5v98</a>.
</div>
<div id="ref-delucaDifferentialSituatedView2019" class="csl-entry">
DeLuca, Christopher, A. Coombs, S. Macgregor, and A. Rasooli. 2019. <span>“Toward a Differential and Situated View of Assessment Literacy: <span>Studying</span> Teachers’ Responses to Classroom Assessment Scenarios.”</span> <em>Frontiers in Education</em> 4. <a href="https://doi.org/gh5k63">https://doi.org/gh5k63</a>.
</div>
<div id="ref-delucaApproachesClassroomAssessment2016" class="csl-entry">
DeLuca, Christopher, Danielle LaPointe-McEwan, and Ulemu Luhanga. 2016a. <span>“Approaches to Classroom Assessment Inventory: <span>A</span> New Instrument to Support Teacher Assessment Literacy.”</span> <em>Educational Assessment</em> 21: 248–66. <a href="https://doi.org/gfgtsg">https://doi.org/gfgtsg</a>.
</div>
<div id="ref-delucaTeacherAssessmentLiteracy2016" class="csl-entry">
———. 2016b. <span>“Teacher Assessment Literacy: A Review of International Standards and Measures.”</span> <em>Educational Assessment, Evaluation and Accountability</em> 28: 251–72. <a href="https://doi.org/f828mh">https://doi.org/f828mh</a>.
</div>
<div id="ref-delucaExploringAssessmentCultures2021" class="csl-entry">
DeLuca, Christopher, Nathan Rickey, and Andrew Coombs. 2021. <span>“Exploring Assessment Across Cultures: <span>Teachers</span>’ Approaches to Assessment in the <span>U</span>.<span>S</span>., <span>China</span>, and <span>Canada</span>.”</span> Edited by Sammy King Fai Hui. <em>Cogent Education</em> 8 (1): 1921903. <a href="https://doi.org/gjxvc7">https://doi.org/gjxvc7</a>.
</div>
<div id="ref-earlAssessmentLearningUsing2013" class="csl-entry">
Earl, Lorna M. 2013. <em>Assessment as Learning: Using Classroom Assessment to Maximize Student Learning</em>. Second edition. <span>Thousand Oaks, Calif</span>: <span>Corwin Press</span>.
</div>
<div id="ref-fletcherFacultyStudentsConceptions2012" class="csl-entry">
Fletcher, Richard B., Luanna H. Meyer, Helen Anderson, Patricia Johnston, and Malcolm Rees. 2012. <span>“Faculty and <span>Students Conceptions</span> of <span>Assessment</span> in <span>Higher Education</span>.”</span> <em>Higher Education</em> 64 (1): 119–33. <a href="https://doi.org/ctccpq">https://doi.org/ctccpq</a>.
</div>
<div id="ref-frantzSpeakSpell2014" class="csl-entry">
Frantz, Gene. 2014. <em>The <span>Speak N Spell</span></em>. <span>OpenStax CNX</span>. <a href="http://cnx.org/contents/b3014cd9-6e3a-474f-bbaa-c948cbc8e205@5.12.">http://cnx.org/contents/b3014cd9-6e3a-474f-bbaa-c948cbc8e205@5.12.</a>
</div>
<div id="ref-harlenSystematicReviewImpact2002" class="csl-entry">
Harlen, Wynne, and Ruth Deakin Crick. 2002. <span>“A Systematic Review of the Impact of Summative Assessment and Tests on Students’ Motivation for Learning.”</span> <em>Research Evidence in Education Library</em>, no. 1: 151. <a href="https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=108">https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=108</a>.
</div>
<div id="ref-herppichTeachersAssessmentCompetence2018" class="csl-entry">
Herppich, Stephanie, Anna-Katharina Praetorius, Natalie Förster, Inga Glogger-Frey, Karina Karst, Detlev Leutner, Lars Behrmann, et al. 2018. <span>“Teachers’ Assessment Competence: <span>Integrating</span> Knowledge-, Process-, and Product-Oriented Approaches into a Competence-Oriented Conceptual Model.”</span> <em>Teaching and Teacher Education</em> 76 (November): 181–93. <a href="https://doi.org/gfjsvn">https://doi.org/gfjsvn</a>.
</div>
<div id="ref-irvineLandscapeMergingModalities2020" class="csl-entry">
Irvine, Valerie. 2020. <span>“The <span>Landscape</span> of Merging Modalities.”</span> <em>EDUCAUSE Review</em> 2020 (4): 40–58. <a href="https://er.educause.edu/articles/2020/10/the-landscape-of-merging-modalities">https://er.educause.edu/articles/2020/10/the-landscape-of-merging-modalities</a>.
</div>
<div id="ref-jonesStudentWellbeingAssessment2021" class="csl-entry">
Jones, Emma, Michael Priestley, Liz Brewster, Susan J. Wilbraham, Gareth Hughes, and Leigh Spanner. 2021. <span>“Student Wellbeing and Assessment in Higher Education: The Balancing Act.”</span> <em>Assessment &amp; Evaluation in Higher Education</em> 46 (3): 438–50. <a href="https://doi.org/gk36pd">https://doi.org/gk36pd</a>.
</div>
<div id="ref-klingerClassroomAssessmentStandards2015" class="csl-entry">
Klinger, Don, Patricia McDivitt, Barbara Howard, Todd Rogers, Marco Munoz, and Caroline Wylie. 2015. <em>Classroom <span>Assessment Standards</span> for <span>PreK</span>-12 <span>Teachers</span></em>. <span>Joint Committee on Standards for Educational Evaluation</span>. <a href="https://www.amazon.ca/Classroom-Assessment-Standards-PreK-12-Teachers-ebook/dp/B00V6C9RVO?asin=B00V6C9RVO&amp;revisionId=d45424dd&amp;format=1&amp;depth=1">https://www.amazon.ca/Classroom-Assessment-Standards-PreK-12-Teachers-ebook/dp/B00V6C9RVO?asin=B00V6C9RVO&amp;revisionId=d45424dd&amp;format=1&amp;depth=1</a>.
</div>
<div id="ref-linAssessingLearningTechnologyrich2020" class="csl-entry">
Lin, Qiao, Yue Yin, Xiaodan Tang, Roxana Hadad, and Xiaoming Zhai. 2020. <span>“Assessing Learning in Technology-Rich Maker Activities: <span>A</span> Systematic Review of Empirical Research.”</span> <em>Computers &amp; Education</em> 157 (November): 103944. <a href="https://doi.org/ghgzp9">https://doi.org/ghgzp9</a>.
</div>
<div id="ref-lipnevichWhatGradesMean2020" class="csl-entry">
Lipnevich, Anastasiya A., Thomas R. Guskey, Dana M. Murano, and Jeffrey K. Smith. 2020. <span>“What Do Grades Mean? <span>Variation</span> in Grading Criteria in <span>American</span> College and University Courses.”</span> <em>Assessment in Education: Principles, Policy &amp; Practice</em> 27 (5): 480–500. <a href="https://doi.org/ghjw3k">https://doi.org/ghjw3k</a>.
</div>
<div id="ref-masseyAssessmentLiteracyCollege2020" class="csl-entry">
Massey, Kyle D., Christopher DeLuca, and Danielle LaPointe-McEwan. 2020. <span>“Assessment <span>Literacy</span> in <span>College Teaching</span>: <span>Empirical Evidence</span> on the <span>Role</span> and <span>Effectiveness</span> of a <span>Faculty Training Course</span>.”</span> <em>To Improve the Academy</em> 39 (1). <a href="https://doi.org/gj5ngz">https://doi.org/gj5ngz</a>.
</div>
<div id="ref-medlandExaminingAssessmentLiteracy2015" class="csl-entry">
Medland, Emma. 2015. <span>“Examining the Assessment Literacy of External Examiners.”</span> <em>London Review of Education</em>, December. <a href="https://doi.org/gk5sph">https://doi.org/gk5sph</a>.
</div>
<div id="ref-mislevyTestTheoryReconcieved1994" class="csl-entry">
Mislevy, Robert J. 1994. <span>“Test Theory Reconcieved.”</span> <em>ETS Research Report Series</em> 1994 (1): i–38. <a href="https://doi.org/gjm236">https://doi.org/gjm236</a>.
</div>
<div id="ref-natrielloImpactEvaluationProcesses1987" class="csl-entry">
Natriello, Gary. 1987. <span>“The <span>Impact</span> of <span>Evaluation Processes</span> on <span>Students</span>.”</span> <em>Educational Psychologist</em> 22 (2): 155–75. <a href="https://doi.org/cgqtqx">https://doi.org/cgqtqx</a>.
</div>
<div id="ref-oldfieldAssessmentDigitalAge2012" class="csl-entry">
Oldfield, Alison, Patricia Broadfoot, Rosamund Sutherland, and Sue Timmis. 2012. <span>“Assessment in a <span>Digital Age</span>: <span>A</span> Research Review.”</span> <span>Graduate School of Education, University of Bristol</span>. <a href="https://www.bristol.ac.uk/media-library/sites/education/documents/researchreview.pdf">https://www.bristol.ac.uk/media-library/sites/education/documents/researchreview.pdf</a>.
</div>
<div id="ref-pastoreTeacherAssessmentLiteracy2019" class="csl-entry">
Pastore, Serafina, and Heidi L. Andrade. 2019. <span>“Teacher Assessment Literacy: <span>A</span> Three-Dimensional Model.”</span> <em>Teaching and Teacher Education</em> 84 (August): 128–38. <a href="https://doi.org/gh5k7b">https://doi.org/gh5k7b</a>.
</div>
<div id="ref-pellegrinoKnowingWhatStudents2001" class="csl-entry">
Pellegrino, James W., Naomi Chudowsky, and Robert Glaser. 2001. <em>Knowing <span>What Students Know</span>: <span>The Science</span> and <span>Design</span> of <span>Educational Assessment</span></em>. <span>Washington, D.C.</span>: <span>National Academies Press</span>. <a href="https://doi.org/10.17226/10019">https://doi.org/10.17226/10019</a>.
</div>
<div id="ref-pellegrinoPerspectivesIntegrationTechnology2010" class="csl-entry">
Pellegrino, James W., and Edys S. Quellmalz. 2010. <span>“Perspectives on the <span>Integration</span> of <span>Technology</span> and <span>Assessment</span>.”</span> <em>Journal of Research on Technology in Education</em> 43 (2): 119–34. <a href="https://doi.org/ggfh8z">https://doi.org/ggfh8z</a>.
</div>
<div id="ref-pophamAssessmentLiteracyOverlooked2011" class="csl-entry">
Popham, W. James. 2011. <span>“Assessment <span>Literacy Overlooked</span>: <span>A Teacher Educator</span>’s <span>Confession</span>.”</span> <em>The Teacher Educator</em> 46 (4): 265–73. <a href="https://doi.org/dctz5h">https://doi.org/dctz5h</a>.
</div>
<div id="ref-presseyMachineAutomaticTeaching1927" class="csl-entry">
Pressey, S. L. 1927. <span>“A Machine for Automatic Teaching of Drill Material.”</span> <em>School &amp; Society</em> 25: 549–52.
</div>
<div id="ref-priceIfWasGoing2011" class="csl-entry">
Price, Margaret, Jude Carroll, Berry O’Donovan, and Chris Rust. 2011. <span>“If <span>I</span> Was Going There <span>I</span> Wouldn’t Start from Here: A Critical Commentary on Current Assessment Practice.”</span> <em>Assessment &amp; Evaluation in Higher Education</em> 36 (4): 479–92. <a href="https://doi.org/d4sz5m">https://doi.org/d4sz5m</a>.
</div>
<div id="ref-scrivenMethodologyEvaluation1967" class="csl-entry">
Scriven, Michael. 1967. <span>“The Methodology of Evaluation.”</span> In <em>Perspectives of Curriculum Evaluation</em>. Rand <span>McNally Education Series</span>. <span>Rand McNally</span>.
</div>
<div id="ref-shepardRoleAssessmentLearning2000" class="csl-entry">
Shepard, Lorrie A. 2000. <span>“The <span>Role</span> of <span>Assessment</span> in a <span>Learning Culture</span>.”</span> <em>Educational Researcher</em> 29 (7): 4–14. <a href="https://doi.org/cw9jwc">https://doi.org/cw9jwc</a>.
</div>
<div id="ref-smithglasgowStandardizedTestingNursing2019" class="csl-entry">
Smith Glasgow, Mary Ellen, H. Michael Dreher, and James Schreiber. 2019. <span>“Standardized Testing in Nursing Education: <span>Preparing</span> Students for <span>NCLEX</span>-<span>RN</span>® and Practice.”</span> <em>Journal of Professional Nursing</em> 35 (6): 440–46. <a href="https://doi.org/ggqh9p">https://doi.org/ggqh9p</a>.
</div>
<div id="ref-aeraStandardsEducationalPsychological2014" class="csl-entry">
<em>Standards for <span>Educational</span> and <span>Psychological Testing</span></em>. 2014. <span>Washington, D.C</span>: <span>American Educational Research Association</span>. <a href="https://www.testingstandards.net/uploads/7/6/6/4/76643089/9780935302356.pdf">https://www.testingstandards.net/uploads/7/6/6/4/76643089/9780935302356.pdf</a>.
</div>
<div id="ref-stigginsAssessmentLiteracy1991" class="csl-entry">
Stiggins, Richard J. 1991. <span>“Assessment <span>Literacy</span>.”</span> <em>The Phi Delta Kappan</em> 72 (7): 534–39. <a href="http://www.jstor.org.ezproxy.library.uvic.ca/stable/20404455">http://www.jstor.org.ezproxy.library.uvic.ca/stable/20404455</a>.
</div>
<div id="ref-stigginsAssessmentLiteracy21st1995" class="csl-entry">
———. 1995. <span>“Assessment <span>Literacy</span> for the 21st <span>Century</span>.”</span> <em>The Phi Delta Kappan</em> 77 (3): 238–45. <a href="http://www.jstor.org.ezproxy.library.uvic.ca/stable/20405538">http://www.jstor.org.ezproxy.library.uvic.ca/stable/20405538</a>.
</div>
<div id="ref-timmisRethinkingAssessmentDigital2016" class="csl-entry">
Timmis, Sue, Patricia Broadfoot, Rosamund Sutherland, and Alison Oldfield. 2016. <span>“Rethinking Assessment in a Digital Age: Opportunities, Challenges and Risks.”</span> <em>British Educational Research Journal</em> 42 (3): 454–76. <a href="https://doi.org/gftz95">https://doi.org/gftz95</a>.
</div>
<div id="ref-wattersTeachingMachinesHistory2021" class="csl-entry">
Watters, A. 2021. <em>Teaching Machines: <span>The</span> History of Personalized Learning</em>. <span>MIT Press</span>. <a href="https://books.google.ca/books?id=zukGEAAAQBAJ">https://books.google.ca/books?id=zukGEAAAQBAJ</a>.
</div>
<div id="ref-wattersSpeakSpellHistory2015" class="csl-entry">
Watters, Audrey. 2015. <span>“Speak &amp; <span>Spell</span>: <span>A History</span>.”</span> <span>Hack Education</span>. January 13, 2015. <a href="http://hackeducation.com/2015/01/13/speak-and-spell">http://hackeducation.com/2015/01/13/speak-and-spell</a>.
</div>
<div id="ref-webbAssessmentTwentyFirstCentury2018" class="csl-entry">
Webb, Mary, and Dirk Ifenthaler. 2018. <span>“Assessment as, for, and of <span>Twenty</span>-<span>First Century Learning Using Information Technology</span>: <span>An Overview</span>.”</span> In <em>Second <span>Handbook</span> of <span>Information Technology</span> in <span>Primary</span> and <span>Secondary Education</span></em>, edited by Joke Voogt, Gerald Knezek, Rhonda Christensen, and Kwok-Wing Lai, 581–600. <span>Cham</span>: <span>Springer International Publishing</span>. <a href="https://doi.org/10.1007/978-3-319-71054-9_37">https://doi.org/10.1007/978-3-319-71054-9_37</a>.
</div>
<div id="ref-willisConceptualisingTeachersAssessment2013" class="csl-entry">
Willis, Jill, Lenore Adie, and Val Klenowski. 2013. <span>“Conceptualising Teachers’ Assessment Literacies in an Era of Curriculum and Assessment Reform.”</span> <em>The Australian Educational Researcher</em> 40 (2): 241–56. <a href="https://doi.org/gh5k7d">https://doi.org/gh5k7d</a>.
</div>
<div id="ref-xuTeacherAssessmentLiteracy2016" class="csl-entry">
Xu, Yueting, and Gavin T. L. Brown. 2016. <span>“Teacher Assessment Literacy in Practice: <span>A</span> Reconceptualization.”</span> <em>Teaching and Teacher Education</em> 58: 149–62. <a href="https://doi.org/f8txgm">https://doi.org/f8txgm</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="method.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cmadland/assessment/edit/master/02-literature.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
