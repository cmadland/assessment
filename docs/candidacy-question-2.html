<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 3 Candidacy Question 2 | Assessment in Online Higher Education</title>
  <meta name="description" content="This site hosts the open development of my PhD dissertation at University of Victoria. Now you can stop asking me if I am done." />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content=" 3 Candidacy Question 2 | Assessment in Online Higher Education" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This site hosts the open development of my PhD dissertation at University of Victoria. Now you can stop asking me if I am done." />
  <meta name="github-repo" content="cmadland/assessment" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 3 Candidacy Question 2 | Assessment in Online Higher Education" />
  
  <meta name="twitter:description" content="This site hosts the open development of my PhD dissertation at University of Victoria. Now you can stop asking me if I am done." />
  

<meta name="author" content="Colin Madland" />


<meta name="date" content="2021-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="candidacy-question-1.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Assessment in Online Higher Education</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html"><i class="fa fa-check"></i><b>2</b> Candidacy Question 1</a>
<ul>
<li class="chapter" data-level="2.1" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#topic-of-the-research"><i class="fa fa-check"></i><b>2.1</b> Topic of the Research</a></li>
<li class="chapter" data-level="2.2" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#problem-to-be-researched"><i class="fa fa-check"></i><b>2.2</b> Problem to be Researched</a></li>
<li class="chapter" data-level="2.3" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#purpose-of-the-research"><i class="fa fa-check"></i><b>2.3</b> Purpose of the Research</a></li>
<li class="chapter" data-level="2.4" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#structure-of-the-paper"><i class="fa fa-check"></i><b>2.4</b> Structure of the Paper</a></li>
<li class="chapter" data-level="2.5" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#defining-assessment"><i class="fa fa-check"></i><b>2.5</b> Defining Assessment</a></li>
<li class="chapter" data-level="2.6" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#conceptions-of-assessment"><i class="fa fa-check"></i><b>2.6</b> Conceptions of Assessment</a></li>
<li class="chapter" data-level="2.7" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#assessment-literacy"><i class="fa fa-check"></i><b>2.7</b> Assessment Literacy</a></li>
<li class="chapter" data-level="2.8" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#assessment-and-technology"><i class="fa fa-check"></i><b>2.8</b> Assessment and Technology</a></li>
<li class="chapter" data-level="2.9" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#impact-on-learners"><i class="fa fa-check"></i><b>2.9</b> Impact on Learners</a></li>
<li class="chapter" data-level="2.10" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#theoretical-framework-overview"><i class="fa fa-check"></i><b>2.10</b> Theoretical Framework Overview</a></li>
<li class="chapter" data-level="2.11" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#summary-and-research-questions"><i class="fa fa-check"></i><b>2.11</b> Summary and Research Questions</a></li>
<li class="chapter" data-level="2.12" data-path="candidacy-question-1.html"><a href="candidacy-question-1.html#significance-of-the-research"><i class="fa fa-check"></i><b>2.12</b> Significance of the Research</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html"><i class="fa fa-check"></i><b>3</b> Candidacy Question 2</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#a-methodology-for-investigating-assessment-approaches-and-their-impact-on-learners"><i class="fa fa-check"></i><b>3.0.1</b> A Methodology for Investigating Assessment Approaches and their Impact on Learners</a></li>
<li class="chapter" data-level="3.1" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#overview-of-the-problem-purpose-and-questions"><i class="fa fa-check"></i><b>3.1</b> Overview of the Problem, Purpose, and Questions</a></li>
<li class="chapter" data-level="3.2" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#methodological-approaches"><i class="fa fa-check"></i><b>3.2</b> Methodological Approaches</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#quantitative-approaches"><i class="fa fa-check"></i><b>3.2.1</b> Quantitative approaches</a></li>
<li class="chapter" data-level="3.2.2" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#qualitative-approaches"><i class="fa fa-check"></i><b>3.2.2</b> Qualitative approaches</a></li>
<li class="chapter" data-level="3.2.3" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#mixed-approaches"><i class="fa fa-check"></i><b>3.2.3</b> Mixed approaches</a></li>
<li class="chapter" data-level="3.2.4" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#epistemological-alignment-with-purpose-of-the-research"><i class="fa fa-check"></i><b>3.2.4</b> Epistemological Alignment with Purpose of the Research</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#visualization-of-a-mixed-research-project"><i class="fa fa-check"></i><b>3.3</b> Visualization of a Mixed Research Project</a></li>
<li class="chapter" data-level="3.4" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#participants-and-sampling-strategy"><i class="fa fa-check"></i><b>3.4</b> Participants and Sampling Strategy</a></li>
<li class="chapter" data-level="3.5" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#data-collection-strategy"><i class="fa fa-check"></i><b>3.5</b> Data Collection Strategy</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#phase-1"><i class="fa fa-check"></i><b>3.5.1</b> Phase 1</a></li>
<li class="chapter" data-level="3.5.2" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#phase-2"><i class="fa fa-check"></i><b>3.5.2</b> Phase 2</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#types-of-data-and-analysis"><i class="fa fa-check"></i><b>3.6</b> Types of Data and Analysis</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#quantitative-data"><i class="fa fa-check"></i><b>3.6.1</b> Quantitative data</a></li>
<li class="chapter" data-level="3.6.2" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#qualitative-data"><i class="fa fa-check"></i><b>3.6.2</b> Qualitative data</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#research-procedures"><i class="fa fa-check"></i><b>3.7</b> Research Procedures</a></li>
<li class="chapter" data-level="3.8" data-path="candidacy-question-2.html"><a href="candidacy-question-2.html#chapter-summary"><i class="fa fa-check"></i><b>3.8</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>4</b> References</a></li>
<li class="chapter" data-level="5" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>5</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction.html"><a href="introduction.html#topic-of-the-research-1"><i class="fa fa-check"></i><b>5.1</b> Topic of the Research</a></li>
<li class="chapter" data-level="5.2" data-path="introduction.html"><a href="introduction.html#problem-to-be-researched-1"><i class="fa fa-check"></i><b>5.2</b> Problem to be Researched</a></li>
<li class="chapter" data-level="5.3" data-path="introduction.html"><a href="introduction.html#purpose-of-the-research-1"><i class="fa fa-check"></i><b>5.3</b> Purpose of the Research</a></li>
<li class="chapter" data-level="5.4" data-path="introduction.html"><a href="introduction.html#research-questions"><i class="fa fa-check"></i><b>5.4</b> Research Questions</a></li>
<li class="chapter" data-level="5.5" data-path="introduction.html"><a href="introduction.html#significance-of-the-research-1"><i class="fa fa-check"></i><b>5.5</b> Significance of the Research</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html"><i class="fa fa-check"></i><b>6</b> Review of the Literature</a>
<ul>
<li class="chapter" data-level="6.1" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#structure-of-the-paper-1"><i class="fa fa-check"></i><b>6.1</b> Structure of the Paper</a></li>
<li class="chapter" data-level="6.2" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#defining-assessment-1"><i class="fa fa-check"></i><b>6.2</b> Defining Assessment</a></li>
<li class="chapter" data-level="6.3" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#conceptions-of-assessment-1"><i class="fa fa-check"></i><b>6.3</b> Conceptions of Assessment</a></li>
<li class="chapter" data-level="6.4" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#assessment-literacy-1"><i class="fa fa-check"></i><b>6.4</b> Assessment Literacy</a></li>
<li class="chapter" data-level="6.5" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#assessment-and-technology-1"><i class="fa fa-check"></i><b>6.5</b> Assessment and Technology</a></li>
<li class="chapter" data-level="6.6" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#impact-on-learners-1"><i class="fa fa-check"></i><b>6.6</b> Impact on Learners</a></li>
<li class="chapter" data-level="6.7" data-path="review-of-the-literature.html"><a href="review-of-the-literature.html#theoretical-framework-overview-1"><i class="fa fa-check"></i><b>6.7</b> Theoretical Framework Overview</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="method.html"><a href="method.html"><i class="fa fa-check"></i><b>7</b> Method</a>
<ul>
<li class="chapter" data-level="7.1" data-path="method.html"><a href="method.html#overview-of-the-problem-purpose-and-questions-1"><i class="fa fa-check"></i><b>7.1</b> Overview of the Problem, Purpose, and Questions</a></li>
<li class="chapter" data-level="7.2" data-path="method.html"><a href="method.html#methodological-approaches-1"><i class="fa fa-check"></i><b>7.2</b> Methodological Approaches</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="method.html"><a href="method.html#quantitative-approaches-1"><i class="fa fa-check"></i><b>7.2.1</b> Quantitative approaches</a></li>
<li class="chapter" data-level="7.2.2" data-path="method.html"><a href="method.html#qualitative-approaches-1"><i class="fa fa-check"></i><b>7.2.2</b> Qualitative approaches</a></li>
<li class="chapter" data-level="7.2.3" data-path="method.html"><a href="method.html#mixed-approaches-1"><i class="fa fa-check"></i><b>7.2.3</b> Mixed approaches</a></li>
<li class="chapter" data-level="7.2.4" data-path="method.html"><a href="method.html#epistemological-alignment-with-purpose-of-the-research-1"><i class="fa fa-check"></i><b>7.2.4</b> Epistemological Alignment with Purpose of the Research</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="method.html"><a href="method.html#visualization-of-a-mixed-research-project-1"><i class="fa fa-check"></i><b>7.3</b> Visualization of a Mixed Research Project</a></li>
<li class="chapter" data-level="7.4" data-path="method.html"><a href="method.html#participants-and-sampling-strategy-1"><i class="fa fa-check"></i><b>7.4</b> Participants and Sampling Strategy</a></li>
<li class="chapter" data-level="7.5" data-path="method.html"><a href="method.html#data-collection-strategy-1"><i class="fa fa-check"></i><b>7.5</b> Data Collection Strategy</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="method.html"><a href="method.html#phase-1-1"><i class="fa fa-check"></i><b>7.5.1</b> Phase 1</a></li>
<li class="chapter" data-level="7.5.2" data-path="method.html"><a href="method.html#phase-2-1"><i class="fa fa-check"></i><b>7.5.2</b> Phase 2</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="method.html"><a href="method.html#types-of-data-and-analysis-1"><i class="fa fa-check"></i><b>7.6</b> Types of Data and Analysis</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="method.html"><a href="method.html#quantitative-data-1"><i class="fa fa-check"></i><b>7.6.1</b> Quantitative data</a></li>
<li class="chapter" data-level="7.6.2" data-path="method.html"><a href="method.html#qualitative-data-1"><i class="fa fa-check"></i><b>7.6.2</b> Qualitative data</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="method.html"><a href="method.html#research-procedures-1"><i class="fa fa-check"></i><b>7.7</b> Research Procedures</a></li>
<li class="chapter" data-level="7.8" data-path="method.html"><a href="method.html#chapter-summary-1"><i class="fa fa-check"></i><b>7.8</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>8</b> Results</a></li>
<li class="chapter" data-level="9" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>9</b> Discussion</a></li>
<li class="chapter" data-level="10" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>10</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/cmadland/assessment" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Assessment in Online Higher Education</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="candidacy-question-2" class="section level1" number="3">
<h1><span class="header-section-number"> 3</span> Candidacy Question 2</h1>
<div id="a-methodology-for-investigating-assessment-approaches-and-their-impact-on-learners" class="section level3" number="3.0.1">
<h3><span class="header-section-number">3.0.1</span> A Methodology for Investigating Assessment Approaches and their Impact on Learners</h3>
<p>Colin Madland</p>
<p>Department of Curriculum and Instruction, Faculty of Education, University of Victoria</p>
<p>EDCI 693: Candidacy Examination - Curriculum and Instruction</p>
<p>Dr. Valerie Irvine (Supervisor),</p>
<p>Dr. Christopher DeLuca, and</p>
<p>Dr. Okan Bulut</p>
<p>Monday, August 9, 2021</p>
<p><em>Describe a methodological approach to investigate higher education instructors’ assessment literacies, practices, and the impacts of those on learners. Consider and justify your methodological decisions based on extant literature. Explore how the underpinning epistemology aligns with the purpose of the investigation. Consider the practical methods as based on previous research (where relevant) that could be used to pursue such an investigation, explaining the type of data you would collect, potential tools to collect data, and how you would use the data to address your research question(s).</em></p>
</div>
<div id="overview-of-the-problem-purpose-and-questions" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Overview of the Problem, Purpose, and Questions</h2>
<p>Research suggests that higher education faculty receive little formal preparation for assessing learning <span class="citation">(<a href="#ref-lipnevichWhatGradesMean2020" role="doc-biblioref">Lipnevich et al. 2020</a>)</span> and that there may be misalignment between modern pedagogical practices and the approaches to assessment taken by higher education instructors <span class="citation">(<a href="#ref-knightSummativeAssessmentHigher2002" role="doc-biblioref">Knight 2002</a>; <a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>; <a href="#ref-shepardRoleAssessmentLearning2000" role="doc-biblioref">Shepard 2000</a>)</span>. The purpose of this paper will be to outline a methodological approach to investigating higher education instructors’ assessment literacies and practices as well as the impact of those practices on learners. Assessment literacy is the result of a complex interplay of skills, beliefs, and values about four domains of assessment practice: (1) the purposes of assessment; (2) the processes associated with designing, administering, scoring, and communicating the results of assessment tasks; (3) fairness in assessment practices; and (4) assessment theory, each with three priority areas. <span class="citation">(<a href="#ref-delucaDifferentialSituatedView2019" role="doc-biblioref">DeLuca, Coombs, et al. 2019</a>)</span>.</p>
<p>Specific research questions to be addressed are:</p>
<ol style="list-style-type: decimal">
<li>Are there distinct patterns in higher education instructors’ approaches to assessment in Canada?</li>
<li>Does the prevalence of these patterns differ by:
<ul>
<li>instructors’ levels of experience in teaching face-to-face versus online?</li>
<li>instructors’ levels of experience using technology?</li>
</ul></li>
<li>What factors influence instructors’ approaches to assessment?</li>
<li>How do instructors’ assessment approaches affect learners’ experiences?</li>
</ol>
<p>In reviewing the extant literature concerning these topics and questions, and how researchers approach them, I searched the Educational Resource Information Center (ERIC), Google Scholar, and the University of Victoria Library Summon 2.0 databases. I also searched specific journals, such as Frontiers in Education, International Review of Research in Open and Distributed Learning, the Journal of Mixed Methods Research, Educational Measurement: Issues and Practice, and Assessment and Evaluation in Higher Education. Search terms included “assessment literacy,” “assessment,” “online,” “higher education,” “learning,” and “learner impact” in various combinations. I also used <a href="https://researchrabbit.ai">researchrabbit.ai</a> to assist with tracking citations of seminal and other important articles and for visualizing the relationships between found articles.</p>
</div>
<div id="methodological-approaches" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Methodological Approaches</h2>
<p>All research is grounded in a particular tradition or worldview, often called a <em>paradigm</em>. A research paradigm is a set of philosophical assumptions and values about the universe and the ways in which we can learn and know about it <span class="citation">(<a href="#ref-johnsonEducationalResearchQuantitative2017" role="doc-biblioref">Johnson and Christensen 2017</a>)</span>. According to Johnson and Christenson <span class="citation">(<a href="#ref-johnsonEducationalResearchQuantitative2017" role="doc-biblioref">2017</a>)</span>, research paradigms are generally concerned with the answers to questions related to:
- how we learn about something (methodology),
- how we know something is true (epistemology)
- how we know reality, or if something exists (ontology),
- how we know if something is valuable or ethical (axiology), and;
- how we communicate and present arguments (rhetoric).</p>
<p>The three primary research paradigms are <em>quantitative</em>, <em>qualitative</em>, and <em>mixed</em> research (often called <em>mixed methods</em> research). Quantitative researchers traditionally followed the epistemological position of <em>positivism</em>, characterized by the idea that knowledge is singular, objective, and can be discovered through the empirical analysis of numerical data <span class="citation">(<a href="#ref-heldDecolonizingResearchParadigms2019" role="doc-biblioref">Held 2019</a>)</span>, although most modern quantitative researchers take a <em>post-positivist</em> view which recognizes that the values and beliefs of the researcher play a role in research and challenge the idea of pure objectivity <span class="citation">(<a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">Tashakkori, Johnson, and Teddlie 2020</a>)</span>. Quantitative research is often confirmatory in nature, with the researcher gathering data to either confirm or disconfirm a specific hypothesis <span class="citation">(<a href="#ref-johnsonEducationalResearchQuantitative2017" role="doc-biblioref">Johnson and Christensen 2017</a>)</span>. Qualitative researchers, on the other hand, tend to take the epistemological view of <em>constructivism</em> which posits that meaning is constructed by the researcher as they observe individuals and groups in relation to each other and particular phenomena. Qualitative research is exploratory in nature and uses non-numerical data such as texts, transcripts, images, and categories <span class="citation">(<a href="#ref-johnsonEducationalResearchQuantitative2017" role="doc-biblioref">Johnson and Christensen 2017</a>)</span> in response to open-ended questions <span class="citation">(<a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">Tashakkori, Johnson, and Teddlie 2020</a>)</span>. During the latter half of the 20th century, when qualitative approaches were becoming more popular the research community tended towards a dualistic, ‘either/or’ view of these two paradigms <span class="citation">(<a href="#ref-niglasMultidimensionalModelResearch2010" role="doc-biblioref">Niglas 2010</a>; <a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">Tashakkori, Johnson, and Teddlie 2020</a>)</span> to the extent that the two paradigms were seen to be fundamentally incompatible. For example, Guba <span class="citation">(<a href="#ref-gubaParadigmDialog1990" role="doc-biblioref">1990, 81</a> as quoted in; <a href="#ref-johnsonMixedMethodsResearch2004" role="doc-biblioref">Johnson and Onwuegbuzie 2004, 14</a>)</span> emphasized “accommodation between paradigms is impossible … we are led to vastly diverse, disparate, and totally antithetical ends.” Early in the 21st century, however, Johnson and Onwuegbuzie countered that argument and proposed that paradigms are better visualized on a continuum, that both paradigms are useful, and that mixing methods can “draw from the strengths and minimize the weaknesses of both in single research studies and across studies” <span class="citation">(<a href="#ref-johnsonMixedMethodsResearch2004" role="doc-biblioref">2004, 13–14</a>)</span>. So mixed research is characterized by an epistemologically <em>pragmatic</em> view that both quantitative and qualitative approaches are valuable and that combining numerical and non-numerical data, analyses, and results can lead to deeper understanding <span class="citation">(<a href="#ref-bazeleyIntegratingAnalysesMixed2018" role="doc-biblioref">Bazeley 2018</a>; <a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">Tashakkori, Johnson, and Teddlie 2020</a>)</span>. More specifically, Johnson, et al. <span class="citation">(<a href="#ref-johnsonDefinitionMixedMethods2007" role="doc-biblioref">2007</a>)</span> provide the following definition of mixed research:</p>
<blockquote>
<p>Mixed methods research is the type of research in which a researcher or team of researchers combines elements of qualitative and quantitative research approaches (e.g., use of qualitative and quantitative viewpoints, data collection, analysis, inference techniques) for the broad purposes of breadth and depth of understanding and corroboration. (p. 123)</p>
</blockquote>
<p>I will use the term <em>mixed research</em> in alignment with Johnson and Christensen <span class="citation">(<a href="#ref-johnsonEducationalResearchQuantitative2017" role="doc-biblioref">2017</a>)</span> who advocate for that term rather than <em>mixed methods research</em> because it is more than the method of research that is mixed. Indeed, mixing occurs at the level of paradigms, questions, methods, analyses, and reporting of results <span class="citation">(<a href="#ref-bazeleyIntegratingAnalysesMixed2018" role="doc-biblioref">Bazeley 2018</a>)</span>.</p>
<p>Niglas <span class="citation">(<a href="#ref-niglasMultidimensionalModelResearch2010" role="doc-biblioref">2010</a>)</span> contends that while the epistemological and philosophical stances of the researcher are important in informing the method of research, the more important considerations are the problem, purpose and questions the researcher intends to investigate. Similarly, Bazeley <span class="citation">(<a href="#ref-bazeleyIntegratingAnalysesMixed2018" role="doc-biblioref">2018</a>)</span> argues that the most important factor that researchers should consider when choosing a methodology is “whether the methods chosen and the strategies used … serve the <em>purpose</em> of the research” (p. 8, emphasis in original). Following Bazeley’s argument, and considering the purpose of this research project, a mixed approach is justified. Further justification is indicated in the diversity of approaches taken by researchers investigating assessment literacy or the impact of assessment on learners using quantitative <span class="citation">(<a href="#ref-delucaTeachersApproachesClassroom2016" role="doc-biblioref">DeLuca et al. 2016</a>; <a href="#ref-delucaExploringAssessmentCultures2021" role="doc-biblioref">DeLuca, Rickey, and Coombs 2021</a>; <a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>; <a href="#ref-nayaginPreserviceTeachersApproaches2020" role="doc-biblioref">Nayagi N and Rajendran 2020</a>; <a href="#ref-pereiraHowUndergraduatesPerceive2021" role="doc-biblioref">Pereira et al. 2021</a>)</span>, qualitative <span class="citation">(<a href="#ref-boudWhatFeedbackLiterate2021" role="doc-biblioref">Boud and Dawson 2021</a>; <a href="#ref-coombsSeaSeaCanadian2020" role="doc-biblioref">Coombs, Ge, and DeLuca 2020</a>; <a href="#ref-delucaPedagogySlowSignificant2021" role="doc-biblioref">DeLuca et al. 2021</a>; <a href="#ref-earleBalancingDemandsValidity2020" role="doc-biblioref">Earle 2020</a>; <a href="#ref-fivesNavigatingComplexCognitive2020" role="doc-biblioref">Fives and Barnes 2020</a>; <a href="#ref-medlandAssessmentIlliterateShared2019" role="doc-biblioref">Medland 2019</a>; <a href="#ref-watsonSmallDataOnline2017" role="doc-biblioref">Watson et al. 2017</a>)</span>, and mixed <span class="citation">(<a href="#ref-delucaStudentPerspectivesAssessment2018" role="doc-biblioref">DeLuca et al. 2018</a>; <a href="#ref-nicholsonEnhancingStudentEngagement2018" role="doc-biblioref">Nicholson 2018</a>; <a href="#ref-iannoneImpactHighStakes2020" role="doc-biblioref">Iannone, Czichowsky, and Ruf 2020</a>; <a href="#ref-tekirAlignmentIntendedEnacted2021" role="doc-biblioref">Tekir 2021</a>)</span> approaches.</p>
<div id="quantitative-approaches" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Quantitative approaches</h3>
<p>The construct of assessment literacy has a history of being grounded in sets of standards published by researchers, governments, or regulatory agencies and of being assessed through the collection and analysis of numeric and categorical data <span class="citation">(<a href="#ref-delucaTeachersApproachesClassroom2016" role="doc-biblioref">DeLuca et al. 2016</a>; <a href="#ref-gotchSystematicReviewAssessment2014" role="doc-biblioref">Gotch and French 2014</a>)</span>. Gotch and French <span class="citation">(<a href="#ref-gotchSystematicReviewAssessment2014" role="doc-biblioref">2014</a>)</span>, in their systematic review of the literature on assessment literacy inventories, found 36 instruments, all but one of which <span class="citation">(<a href="#ref-jarrEducationPractitionersInterpretation2012" role="doc-biblioref">Jarr 2012</a>)</span> gathered only quantitative (numeric and categorical) data. Studies based on these instruments (for example, Alkharusi, et al. <span class="citation">(<a href="#ref-alkharusiEducationalAssessmentAttitudes2012" role="doc-biblioref">2012</a>)</span> based on Plake, et al. <span class="citation">(<a href="#ref-plakeAssessmentCompetenciesTeachers2005" role="doc-biblioref">2005</a>)</span>) generally included quantitative analyses because the data gathered are quantitative. Early instruments <span class="citation">(<a href="#ref-mertlerMeasuringTeachersKnowledge2005" role="doc-biblioref">Mertler and Campbell 2005</a>; <a href="#ref-plakeTeacherAssessmentLiteracy1993" role="doc-biblioref">Barbara S. Plake 1993</a>)</span> were based on the 1990 <em>Standards for Teacher Competency in Educational Assessment of Students</em> <span class="citation">(<a href="#ref-StandardsTeacherCompetence1990" role="doc-biblioref">AFT, NCME, and NEA 1990</a>)</span>. Since then, Brookhart <span class="citation">(<a href="#ref-brookhartEducationalAssessmentKnowledge2011" role="doc-biblioref">2011</a>)</span> published a critique of the 1990 standards arguing that they were out of date and not in alignment with modern pedagogy. This was followed a few years later by Gotch and French <span class="citation">(<a href="#ref-gotchSystematicReviewAssessment2014" role="doc-biblioref">2014</a>)</span> publishing the aforementioned systematic review, during which they found that the psychometric properties of the 36 instruments published between 1991 and 2012 were low. Finally, Klinger et al. <span class="citation">(<a href="#ref-klingerClassroomAssessmentStandards2015" role="doc-biblioref">2015</a>)</span> published the <em>Classroom Assessment Standards for PreK-12 Teachers</em> <span class="citation">(<a href="#ref-klingerClassroomAssessmentStandards2015" role="doc-biblioref">Klinger et al. 2015</a>)</span>. These cumulative advances led to DeLuca et al. <span class="citation">(<a href="#ref-delucaTeacherAssessmentLiteracy2016" role="doc-biblioref">2016b</a>)</span> to develop the <em>Approaches to Classroom Assessment Inventory (ACAI)</em> and enabled researchers to base their quantitative investigations on an instrument shown to produce valid results in relation to a modern model of assessment literacy.</p>
<p>There are numerous examples of research teams using quantitative approaches to investigate assessment literacy, some of which are mentioned here. DeLuca et al. <span class="citation">(<a href="#ref-delucaTeachersApproachesClassroom2016" role="doc-biblioref">2016</a>)</span> surveyed 404 K-12 teachers in Canada and the USA using DeLuca et al.’s <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">2016a</a>)</span> <em>Approaches to Classroom Assessment Inventory (ACAI)</em>, an instrument based on the 2015 Standards. DeLuca et al. <span class="citation">(<a href="#ref-delucaExploringAssessmentCultures2021" role="doc-biblioref">2021</a>)</span>, recently used the same inventory in an international study comparing assessment practices in the USA, Canada, and China. Nayagi and Rajendran <span class="citation">(<a href="#ref-nayaginPreserviceTeachersApproaches2020" role="doc-biblioref">2020</a>)</span> also used the ACAI to investigate approaches to assessment, although they modified it for their context of pre-service teachers in India. Massey et al. <span class="citation">(<a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">2020</a>)</span> surveyed higher education instructors to investigate their approaches to and confidence with assessment. Lastly, with respect to the impact of instructors’ assessment practices on learners, Pereira et al. <span class="citation">(<a href="#ref-pereiraHowUndergraduatesPerceive2021" role="doc-biblioref">2021</a>)</span>, Pereira et al. <span class="citation">(<a href="#ref-pereiraPerceptionsPortugueseUndergraduate2017" role="doc-biblioref">2017</a>)</span>, and Flores et al. <span class="citation">(<a href="#ref-floresPerceptionsEffectivenessFairness2015" role="doc-biblioref">2015</a>)</span> used surveys to collect quantitative data about the perceptions of Portuguese undergraduate learners regarding the assessment practices of their instructors. While there are more studies of assessment literacy in the K-12 sector <span class="citation">(<a href="#ref-medlandAssessmentIlliterateShared2019" role="doc-biblioref">Medland 2019</a>)</span>, there is a growing number in the higher education sector (e.g. <span class="citation">(<a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>; <a href="#ref-nayaginPreserviceTeachersApproaches2020" role="doc-biblioref">Nayagi N and Rajendran 2020</a>; <a href="#ref-pereiraHowUndergraduatesPerceive2021" role="doc-biblioref">Pereira et al. 2021</a>)</span>).</p>
<p>Research questions 1 and 2 in the present study are intended to answer questions about the assessment literacies of higher education instructors in Canada, a construct which has been shown to be amenable to quantitative survey approaches in diverse settings, including K-12 teachers, pre-service teachers, and higher education instructors both in Canada and internationally.</p>
</div>
<div id="qualitative-approaches" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Qualitative approaches</h3>
<p>Assessment literacy has been the focus of significant attention in quantitative research, but it is also becoming clear that the construct is much deeper and more complex than models based on sets of standards and competencies <span class="citation">(<a href="#ref-delucaPoliciesProgramsPractices2019" role="doc-biblioref">DeLuca, Willis, et al. 2019</a>, <a href="#ref-delucaPoliciesProgramsPractices2019" role="doc-biblioref">2019</a>; <a href="#ref-willisConceptualisingTeachersAssessment2013" role="doc-biblioref">Willis, Adie, and Klenowski 2013</a>; <a href="#ref-xuTeacherAssessmentLiteracy2016" role="doc-biblioref">Xu and Brown 2016</a>)</span>. Willis <span class="citation">(<a href="#ref-willisConceptualisingTeachersAssessment2013" role="doc-biblioref">2013</a>)</span> framed assessment literacy within multiple “horizontal and vertical structures of educational discourse” following Bernstein <span class="citation">(<a href="#ref-bernsteinVerticalHorizontalDiscourse1999" role="doc-biblioref">1999</a>)</span>, who described vertical discourses as structured disciplinary knowledge passed down from specialists, and horizontal discourses as more contextualized knowledge practiced and passed along at a local level. Following on this idea, DeLuca et al. <span class="citation">(<a href="#ref-delucaDifferentialSituatedView2019" role="doc-biblioref">2019</a>)</span> describe assessment literacy as being a “situated and differential practice predicated on negotiated knowledges” (p. 7) and elsewhere, DeLuca et al. <span class="citation">(<a href="#ref-delucaPoliciesProgramsPractices2019" role="doc-biblioref">2019</a>)</span> argue that “Assessment capability involves situated professional judgement, that is the ability to draw on learning and assessment theories and experiences to purposefully design, interpret, and use a range of assessment evidence in the service of student learning” (p. 2). Similarly, Xu and Brown’s <span class="citation">(<a href="#ref-xuTeacherAssessmentLiteracy2016" role="doc-biblioref">2016</a>)</span> model, <em>Teacher Assessment Literacy in Practice (TALiP)</em>, explicitly includes socio-cultural factors, teacher judgement, and teacher identity.</p>
<p>The methodological implication of this shift to a more complex understanding of assessment literacy is that there seems to have been an attendant shift in researchers engaging in qualitative investigations of assessment literacy <span class="citation">(<a href="#ref-bearmanHowUniversityTeachers2017" role="doc-biblioref">Bearman et al. 2017</a>; <a href="#ref-delucaPedagogySlowSignificant2021" role="doc-biblioref">DeLuca et al. 2021</a>; <a href="#ref-fivesNavigatingComplexCognitive2020" role="doc-biblioref">Fives and Barnes 2020</a>; <a href="#ref-medlandAssessmentIlliterateShared2019" role="doc-biblioref">Medland 2019</a>; <a href="#ref-watsonSmallDataOnline2017" role="doc-biblioref">Watson et al. 2017</a>)</span>. Qualitative methodologies exist because of the view that, according to Guba and Lincoln <span class="citation">(<a href="#ref-gubaFourthGenerationEvaluation1989" role="doc-biblioref">1989</a>)</span>, reality is constructed through social processes, and is not simply observed in the world. In my view, this epistemological stance within the community of qualitative researchers aligns well with the nature of assessment literacy as a phenomenon impacted by horizontal discourses. In my own review of the literature on assessment literacy, there seems to be a marked increase in the number of qualitative studies published since 2016, which may reflect a deepening understanding of the characteristics of assessment literacy.</p>
<p>While the trend to use qualitative approaches to investigate assessment literacy and the impact of assessment on learners is more recent compared to quantitative approaches, there are good examples of qualitative studies, some of which I briefly describe here. Watson et al. <span class="citation">(<a href="#ref-watsonSmallDataOnline2017" role="doc-biblioref">2017</a>)</span> published a case study of how instructor assessment practices impacted a single learner who struggled through a graduate-level learning module. Benediktsson and Ragnarsdóttir <span class="citation">(<a href="#ref-benediktssonImmigrantStudentsExperiences2020" role="doc-biblioref">2020</a>)</span> used focus group interviews followed by semi-structured interviews with individual learners to investigate the assessment experiences of immigrant learners in Icelandic universities. Bearman et al. <span class="citation">(<a href="#ref-bearmanHowUniversityTeachers2017" role="doc-biblioref">2017</a>)</span> interviewed thirty-three higher education instructors from a variety of disciplines and institutions to investigate how they designed assessments and found that there is a need for professional development activities about assessment to have a relational focus. In the UK context of having external peers provide regulatory feedback, Medland <span class="citation">(<a href="#ref-medlandAssessmentIlliterateShared2019" role="doc-biblioref">2019</a>)</span> used an exploratory case study approach to investigate the assessment literacy of external examiners in UK higher education. In another case study with a single subject, Fives and Barnes <span class="citation">(<a href="#ref-fivesNavigatingComplexCognitive2020" role="doc-biblioref">2020</a>)</span>, recognizing the complexity of classroom assessment, investigated the assessment-related routines of an experienced teacher. Their goal was to engage in a close examination of how an experienced teacher enacted assessment practices in their classroom in order to “make explicit [the] cognitive tasks involved in their work” (p. 3). More recently, DeLuca et al. (<span class="citation">(<a href="#ref-delucaPedagogySlowSignificant2021" role="doc-biblioref">2021</a>)</span>) used a case study methodology to analyze data gathered from interviews, reflections, and course work to investigate the learning trajectories of 35 pre-service teachers during their program. Each of these studies was exploratory in nature, and/or designed specifically to uncover deeper meanings than could be discerned with the use of a survey or other quantitative instrument, which would require the researcher to anticipate and direct respondents toward a limited set of possible answers.</p>
<p>Research questions 3 and 4 of the present study are intended to explore the qualitative and subjective experiences of instructors, what factors influence their approaches to assessment and the experiences of learners as they engage in assessment activities. Both of these questions will require responses to open-ended questions and will be exploratory in nature, aligning with a qualitative paradigm and epistemology.</p>
</div>
<div id="mixed-approaches" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Mixed approaches</h3>
<p>Researchers have argued that using mixed approaches to research is a way to strengthen and increase confidence in the validity of results by maximizing the strengths of each paradigm while minimizing the weaknesses <span class="citation">(<a href="#ref-creswellResearchDesignQualitative2009" role="doc-biblioref">Creswell 2009</a>; <a href="#ref-mckimValueMixedMethods2017" role="doc-biblioref">McKim 2017</a>)</span>. Creamer <span class="citation">(<a href="#ref-creamerIntroductionFullyIntegrated2018" role="doc-biblioref">2018</a>)</span> argues that mixing of qualitative and quantitative methods is the defining characteristic of mixed research. She defines mixing as “<em>the linking, merging, or embedding of qualitative and quantitative strands of a mixed methods study</em>” (p. 6, emphasis in original). In a departure from the historical paradigm wars between quantitative and qualitative approaches, Niglas <span class="citation">(<a href="#ref-niglasMultidimensionalModelResearch2010" role="doc-biblioref">2010</a>)</span> contends for the idea that mixed research studies fall on a continuum between qualitative and quantitative approaches with varying degrees of epistemological stances and methods. Given the abundance of literature exploring instructor assessment literacy and practices and the impact of those practices on learners from both quantitative and qualitative paradigms, it is not surprising that there is also a body of literature investigating these constructs using mixed research approaches <span class="citation">(<a href="#ref-esfandiariMixedmethodsCrosssectionalStudy2016" role="doc-biblioref">Esfandiari et al. 2016</a>; <a href="#ref-ogan-bekirogluPreServiceTeachers2014" role="doc-biblioref">Ogan‐Bekiroglu and Suzuk 2014</a>; <a href="#ref-solomonidouStudentsConceptionsAssessment2017" role="doc-biblioref">Solomonidou and Michaelides 2017</a>)</span>. Creamer <span class="citation">(<a href="#ref-creamerIntroductionFullyIntegrated2018" role="doc-biblioref">2018</a>)</span> suggests evaluating mixed research projects using a table to make the structure of the study more plainly visible. The following tables are overviews of selected mixed research projects investigating assessment literacy in K-12 and higher education.</p>
<p><strong>Table 1</strong></p>
<p><em>Overview of </em>“Students’ conceptions of assessment purposes in a low stakes secondary-school context: A mixed methodology approach”* <span class="citation">(<a href="#ref-solomonidouStudentsConceptionsAssessment2017" role="doc-biblioref">Solomonidou and Michaelides 2017</a>)</span>*</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Study Component</th>
<th>Details</th>
<th>Yes/No</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rationale/Purpose for Mixing</td>
<td>Enhancement</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Priority</td>
<td>QUAN → qual</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Timing of Data Collection</td>
<td>Sequential</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Timing of Data Analysis</td>
<td>Sequential</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Mixing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Design</td>
<td>Yes</td>
<td>Qualitative data were used to confirm tentative inferences from quantitative analysis.</td>
</tr>
<tr class="odd">
<td></td>
<td>Data Collection</td>
<td>Yes</td>
<td>Semi-structured interview questions were revised in light of quantitative inferences, and participants could view their survey answers during their interviews.</td>
</tr>
<tr class="even">
<td></td>
<td>Data Analysis</td>
<td>Yes</td>
<td>Blended in the report.</td>
</tr>
<tr class="odd">
<td></td>
<td>Inferences</td>
<td>Yes</td>
<td>Blended in the report.</td>
</tr>
<tr class="even">
<td></td>
<td>Fully Integrated</td>
<td>Yes</td>
<td></td>
</tr>
<tr class="odd">
<td>Qualitative Inference</td>
<td>Learners value assessment information but it takes work to use the information formatively.</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>Quantitative Inference</td>
<td>Learners agreed more with the idea that assessment is a tool for improvement and less with the idea that assessment is negative.</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Meta-inference</td>
<td>Teacher education and development should systematically include alternative assessment practices.</td>
<td></td>
<td>-</td>
</tr>
<tr class="even">
<td>Value Added</td>
<td>Linking data from both surveys and interviews provided opportunity for learners to elaborate on their views and provide a richer picture of their conceptions of assessment.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Table 2</strong></p>
<p><em>Overview of </em>“A Mixed-methods, Cross-sectional Study of Assessment Literacy of Iranian University Instructors: Implications for Teachers’ Professional Development”* <span class="citation">(<a href="#ref-esfandiariMixedmethodsCrosssectionalStudy2016" role="doc-biblioref">Esfandiari et al. 2016</a>)</span>*</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Study Component</th>
<th>Details</th>
<th>Yes/No</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rationale/Purpose for Mixing</td>
<td>Complementarity</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Priority</td>
<td>QUAN → qual</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Timing of Data Collection</td>
<td>Sequential</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Timing of Data Analysis</td>
<td>Sequential</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Mixing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Design</td>
<td>Yes</td>
<td>The qualitative phase was used to elaborate on the quantiative phase.</td>
</tr>
<tr class="odd">
<td></td>
<td>Data Collection</td>
<td>No</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Data Analysis</td>
<td>Yes</td>
<td>Qualitative analysis filled gaps in quantitative inferences.</td>
</tr>
<tr class="odd">
<td></td>
<td>Inferences</td>
<td>Yes</td>
<td>Blended in the report.</td>
</tr>
<tr class="even">
<td></td>
<td>Fully Integrated</td>
<td>No</td>
<td></td>
</tr>
<tr class="odd">
<td>Qualitative Inference</td>
<td>Subgroups of participants assessed learners differently, used assessment for different purposes, had different levels of training in assessment, and different views on the psychometric properties of tests.</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Quantitative Inference</td>
<td>Results showed support for a three-component model of assessment literacy.</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Meta-inference</td>
<td>Assessment literacy is a multi-dimensional concept affected my many factors, including teaching experience, professional development and training, and the local teaching context.</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Value Added</td>
<td>Qualitative analysis was critical to helping the researchers understand the differences seen between subgroups in the quantitative analysis.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Table 3</strong></p>
<p><em>Overview of “Pre-service teachers’ assessment literacy and its implementation into practice” <span class="citation">(<a href="#ref-ogan-bekirogluPreServiceTeachers2014" role="doc-biblioref">Ogan‐Bekiroglu and Suzuk 2014</a>)</span></em></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Study Component</th>
<th>Details</th>
<th>Yes/No</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rationale/Purpose for Mixing</td>
<td>Triangulation and Enhancement</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Priority</td>
<td>QUAN → qual+qual</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Timing of Data Collection</td>
<td>Sequential</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Timing of Data Analysis</td>
<td>Side-by-side comparison (concurrent)</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Mixing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Design</td>
<td>Yes</td>
<td>The qualitative phase was used to validate and elaborate on the quantitative phase.</td>
</tr>
<tr class="odd">
<td></td>
<td>Data Collection</td>
<td>No</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Data Analysis</td>
<td>Yes</td>
<td>Qualitative and quantitative data were analyzed concurrently by presenting them side-by-side for comparison.</td>
</tr>
<tr class="odd">
<td></td>
<td>Inferences</td>
<td>Yes</td>
<td>Blended in the report.</td>
</tr>
<tr class="even">
<td></td>
<td>Fully Integrated</td>
<td>No</td>
<td></td>
</tr>
<tr class="odd">
<td>Qualitative Inference</td>
<td>Qualitative analysis showed alignment with quantitative analysis</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Quantitative Inference</td>
<td>Pre-service teachers’ assessment literacy was determined to be “close to constructivist” (p. 352)</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Meta-inference</td>
<td>Although participants were able to demonstrate knowledge of assessment literacy, they had difficulty translating that knowledge into practice.</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Value Added</td>
<td>Qualitative analysis was important for determining the difficulty in translating knowledge of assessment literacy into practice of good assessment.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Table 4</strong></p>
<p><em>Overview of “A mixed method exploration of student perceptions of assessment in nursing and biomedicine” <span class="citation">(<a href="#ref-garveyMixedMethodExploration2021" role="doc-biblioref">Garvey, Hodgson, and Tighe 2021</a>)</span></em></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Study Component</th>
<th>Details</th>
<th>Yes/No</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rationale/Purpose for Mixing</td>
<td>Triangulation</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Priority</td>
<td>QUAN+QUAL</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Timing of Data Collection</td>
<td>Concurrent</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Timing of Data Analysis</td>
<td>Consecutive</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Mixing</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Design</td>
<td>Yes</td>
<td>The qualitative phase was used to validate and elaborate on the quantitative phase.</td>
</tr>
<tr class="odd">
<td></td>
<td>Data Collection</td>
<td>Yes</td>
<td>Open-ended (qualitative) responses were gathered along with quantitative responses and were designed to elicit clarifying views.</td>
</tr>
<tr class="even">
<td></td>
<td>Data Analysis</td>
<td>No</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Inferences</td>
<td>Yes</td>
<td>Blended in the report.</td>
</tr>
<tr class="even">
<td></td>
<td>Fully Integrated</td>
<td>No</td>
<td></td>
</tr>
<tr class="odd">
<td>Qualitative Inference</td>
<td>Qualitative analysis deeper levels of complexity compared to quantitative analysis.</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Quantitative Inference</td>
<td>First-year university learners indicated positive views of assessment, regardless of their program.</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Meta-inference</td>
<td>Universities should recognize that first-year learners tend to have positive views of assessment and should communicate often with them about the purposes of assessment.</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Value Added</td>
<td>Qualitative analysis showed areas of misalignment with the quantitative analysis.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Each of the preceding four studies shows different ways in which mixed research can enhance validity or provide avenues for deeper and more complex understanding of instructors’ assessment literacy and practices and the impact of those on learners. The tables illustrate the complexity of mixed studies and highlight the challenges associated with integrating quantitative and qualitative approaches at each stage of the research. However, even those studies which do not demonstrate all of the qualities of fully integrated mixed research contain valuable insights based on the integration of the two analyses.</p>
</div>
<div id="epistemological-alignment-with-purpose-of-the-research" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Epistemological Alignment with Purpose of the Research</h3>
<p>The purpose of this project is to investigate a multi-dimensional construct, assessment literacy, that is influenced by factors both internal (for example: beliefs, values, and past experiences) and external (for example: assessment culture <span class="citation">(<a href="#ref-delucaExploringAssessmentCultures2021" role="doc-biblioref">DeLuca, Rickey, and Coombs 2021</a>; <a href="#ref-masseyAssessmentLiteracyCollege2020" role="doc-biblioref">Massey, DeLuca, and LaPointe-McEwan 2020</a>)</span> or government regulations) to instructors <span class="citation">(<a href="#ref-delucaDifferentialSituatedView2019" role="doc-biblioref">DeLuca, Coombs, et al. 2019</a>)</span>. In addition, the project will investigate the factors that influence instructors in higher education to take particular approaches to their assessment practice and how those practices impact learners. In envisioning this multi-dimensional research project and formulating the research questions, I recognize value in both the discovery and the creation of meaning in the research process, a pragmatic epistemology that aligns with the ethos of many mixed researchers. In alignment with the examples from the literature described in the preceding sections, this project will draw from both quantitative and qualitative paradigms, will use both quantitative and qualitative data connected in multiple ways, engage in analyses in alignment with both traditions, and will present results as inferences from both quantitative and qualitative analyses in addition to meta-inferences <span class="citation">(<a href="#ref-bazeleyIntegratingAnalysesMixed2018" role="doc-biblioref">Bazeley 2018, 62</a>)</span> from the integration of analyses. As such, I propose a mixed research investigation, which will be described in greater detail in the following sections.</p>
</div>
</div>
<div id="visualization-of-a-mixed-research-project" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Visualization of a Mixed Research Project</h2>
<p>Researchers who use mixed approaches often include visual overviews of their research project as mixed research projects can be very complex <span class="citation">(<a href="#ref-creamerIntroductionFullyIntegrated2018" role="doc-biblioref">Creamer 2018</a>)</span>. Figure 1 below is a visual representation of the present project. It is helpful to understand some of the conventions of visualizing mixed research projects as seen in figure 1. According to Tashakkori et al. <span class="citation">(<a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">2020</a>)</span>, quantitative components are depicted as ovals and qualitative components are depicted as rectangles. Additionally, in figure 1, barred rectangles indicate mixed inferences. Solid arrows indicate the general direction of the workflow throughout the project and dashed arrows represent potential feedback loops where the results and inferences from one phase inform one or more components of a subsequent phase. Mixed researchers use upper- and lower-case letters to indicate the priority in any given phase. QUAN (all-caps) indicates that quantitative data and analyses are prioritized, while qual (lower-case) indicates that qualitative data and analyses are secondary. Further, a ‘+’ indicates that two or more types of data are collected and/or analyzed concurrently, while a ’ → ’ indicates that two or more types of data are collected or analyzed consecutively or sequentially. So, the notation ‘QUAN+qual’ indicates that both quantitative and qualitative data are collected and analyzed concurrently with quantitative data and analysis being prioritized. The notation ‘QUAN → QUAL’ indicates that quantitative data are being collected and/or analyzed before qualitative data, and both types of data and analysis have equal weight.</p>
<p><strong>Figure 1</strong></p>
<p><em>A Visualization of a QUAN+qual → QUAL Sequential Exploratory Mixed Research Study.</em></p>
<p><img src="images/methods-viz.png" /></p>
<p>Following a pilot phase, I intend to engage in a QUAN+qual → QUAL sequential exploratory research project comprising two separate data collection and analysis phases. In phase 1a, I will use a survey of higher education instructors to collect and analyze primarily quantitative data to investigate research questions 1 and 2.</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Are there distinct patterns in higher education instructors’ approaches to assessment in Canada?</li>
<li>Does the prevalence of these patterns differ by:</li>
</ol>
<ul>
<li>instructors’ levels of experience in teaching face-to-face versus online?</li>
<li>instructors’ levels of experience using technology?</li>
</ul>
</blockquote>
<p>This will be supplemented in phase 1b by the <em>concurrent collection</em> and <em>subsequent analysis</em> of qualitative data to answer research question 3.</p>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>What factors influence instructors’ approaches to assessment?</li>
</ol>
</blockquote>
<p>The second phase will begin following the analysis and integrationof phase 1a+b data and will collect and analyze qualitative data in semi-structured interviews with learners nominated by instructors in phase 1. Phase 2 analysis is intended to answer research question 4.</p>
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>How do instructors’ assessment approaches affect learners’ experiences?</li>
</ol>
</blockquote>
<p>Since research question 4 is an integrative question <span class="citation">(<a href="#ref-creamerIntroductionFullyIntegrated2018" role="doc-biblioref">Creamer 2018</a>)</span>, it cannot be answered fully until the inferences from phase 1 and phase 2 are integrated into one or more meta-inferences.</p>
</div>
<div id="participants-and-sampling-strategy" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Participants and Sampling Strategy</h2>
<p>The target population for the research will be instructors who have taught at least one semester-length, accredited course in the previous 12 months at an English-speaking [^1] public higher education institution in Canada. Ideally, inferences from phase 1 of the study would be generalizable to the total population of English-speaking higher education instructors in Canada, which would require a random sample in which every member of the target population would have equal opportunity to be selected and no selection would have been influenced by a previous selection <span class="citation">(<a href="#ref-hibbertsCommonSurveySampling2012" role="doc-biblioref">Hibberts, Burke Johnson, and Hudson 2012</a>; <a href="#ref-rencklyAirUniversitySampling2002" role="doc-biblioref">Renckly 2002</a>)</span>. Care will be taken to ensure the sample is representative of the whole population by engaging in a stratified random sampling technique at the institutional level <span class="citation">(<a href="#ref-hibbertsCommonSurveySampling2012" role="doc-biblioref">Hibberts, Burke Johnson, and Hudson 2012</a>)</span>. This would involve using regions within Canada as strata (for example, British Columbia, the prairie provinces, the north, Ontario, Quebec, eastern Canada), randomly choosing 2-3 higher education institutions from within those strata, and sending the survey to all instructors at those institutions. Recruitment of participants for phase 2 will involve phase 1 participants (instructors) nominating 8-10 learners from their courses, I will randomly select 2-3 respondents from phase 1 and then randomly select 4-5 of their nominated learners to receive invitations to the semi-structured interviews. This procedure will ensure that instructors will not know which of their nominated learners participated in the research and it will create “linked data” <span class="citation">(<a href="#ref-bazeleyIntegratingAnalysesMixed2018" role="doc-biblioref">2018, 126</a>)</span>, allowing for greater explanatory power due to being able to match specific instructor approaches with learner impacts.</p>
</div>
<div id="data-collection-strategy" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Data Collection Strategy</h2>
<div id="phase-1" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Phase 1</h3>
<p>I propose that the phase 1 data collection instrument be DeLuca et al.’s <span class="citation">(<a href="#ref-delucaApproachesClassroomAssessment2016" role="doc-biblioref">2016a</a>)</span> ACAI (see Appendix 1 for the full ACAI v3.0 and its specifications). The ACAI was designed by a team of researchers, led by Dr. Christopher DeLuca at Queen’s University in Canada to investigate the approaches that K-12 educators take to assessment in their classrooms. It is based on an analysis of assessment standards published in five majority English-speaking countries since 1990. The instrument is based on a four-dimensional framework of assessment literacy (assessment purposes, assessment processes, assessment fairness, and measurement theory), and includes one open-ended question and several closed-ended questions about how teachers form their approaches to assessment. It also includes a section regarding educator beliefs about assessment based on the <em>Beliefs about Assessment</em> scale <span class="citation">(<a href="#ref-smithPreparingTeachersUse2014" role="doc-biblioref">Smith et al. 2014</a>)</span>. The ACAI has been used in a wide variety of studies within the instrument’s author’s research group <span class="citation">(<a href="#ref-coombsChangingApproachesClassroom2018" role="doc-biblioref">Coombs et al. 2018</a>; <a href="#ref-coombsPersoncenteredAnalysisTeacher2020" role="doc-biblioref">Coombs, DeLuca, and MacGregor 2020</a>; <a href="#ref-delucaDifferentialSituatedView2019" role="doc-biblioref">DeLuca, Coombs, et al. 2019</a>; <a href="#ref-delucaTeachersApproachesClassroom2016" role="doc-biblioref">DeLuca et al. 2016</a>; <a href="#ref-delucaPoliciesProgramsPractices2019" role="doc-biblioref">DeLuca, Willis, et al. 2019</a>; <a href="#ref-schneiderLinkingPersonalityTeachers2020" role="doc-biblioref">Schneider et al. 2020</a>)</span>, and also by other researchers <span class="citation">(<a href="#ref-nayaginPreserviceTeachersApproaches2020" role="doc-biblioref">Nayagi N and Rajendran 2020</a>)</span>. In several of these studies, the ACAI has been modified somewhat to fit the particular context or research design.</p>
<p>As the ACAI was designed for use in K-12 contexts and has been used less often to investigate higher education instructors’ approaches to assessment, I will modify the language in the instrument to be more suitable for the higher education context and to include more open-ended questions. Following these modifications, I will undertake a pilot phase to determine whether the modified ACAI is suitable for the higher education context. The pilot phase will inform the version of the ACAI used in Phase 1 of the study and will include both expert review and the recruitment of a small sample from the target population to preview the survey in its intended mode and offer specific and guided feedback <span class="citation">(<a href="#ref-hibbertsCommonSurveySampling2012" role="doc-biblioref">Hibberts, Burke Johnson, and Hudson 2012</a>; <a href="#ref-rencklyAirUniversitySampling2002" role="doc-biblioref">Renckly 2002</a>)</span>.</p>
</div>
<div id="phase-2" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Phase 2</h3>
<p>The semi-structured interviews in phase 2 will include questions similar to the following:</p>
<ul>
<li>Please describe what you understand assessment to be.</li>
<li>Did the assessment strategies that your instructor used help you to learn the course material?</li>
<li>Did [instructor] use technology to administer assessments?
<ul>
<li>How did their use/not use of technology impact your performance on the assessments?</li>
</ul></li>
<li>Please describe your <em>favourite</em> assessment from [instructor]’s course [course name]?
<ul>
<li>What made that assessment your favourite?</li>
</ul></li>
<li>Please describe your <em>least favourite</em> assessment from [instructor]’s course [course name]?
<ul>
<li>What made that assessment your least favourite?</li>
</ul></li>
<li>What factors made it <em>easier</em> to do well on the assessments in [course]?</li>
<li>What factors made it <em>more difficult</em> to do well on the assessments in [course]?</li>
<li>How did the assessment strategies your instructor used make you feel?</li>
<li>Do you receive any support or accommodations to allow you to complete assessments?</li>
</ul>
</div>
</div>
<div id="types-of-data-and-analysis" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Types of Data and Analysis</h2>
<p>As shown in the visualization of the research project (Figure 1), both quantitative and qualitative data will be gathered for analysis. The following sections will describe the data I intend to gather and how it may address the research questions.</p>
<div id="quantitative-data" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Quantitative data</h3>
<p>During phase 1a, quantitative data will be gathered using the ACAI and will primarily be in the form of responses to 6-point Likert scale items. Similar to Flores et al. <span class="citation">(<a href="#ref-floresPortugueseUniversityStudents2020" role="doc-biblioref">2020</a>)</span>, the first step will be to engage in a confirmatory factor analysis to determine the fit of the data with the <em>Approaches to Classroom Assessment</em> model. This step will address research question 1. Then, as the data will be <em>ordinal</em> data (i.e. a score of five is greater than a score of three), but not <em>interval</em> data (i.e. it is not possible to determine <em>how much greater</em> a score of five is compared to a score of three) <span class="citation">(<a href="#ref-bulutLikertScalesFriend2021" role="doc-biblioref">Bulut 2021</a>; <a href="#ref-jamiesonLikertScalesHow2004" role="doc-biblioref">Jamieson 2004</a>)</span>, it will not be appropriate to use the data to calculate common statistics such as mean or standard deviation (i.e. there is no way to calculate the mean of “highly likely” and “highly unlikely”). Consequently, more advanced statistical techniques which rely on the mean and standard deviation (i.e. <em>t</em> test, analysis of variance), otherwise known as <em>parametric</em> tests, are not appropriate for ordinal data <span class="citation">(<a href="#ref-fieldDiscoveringStatisticsUsing2018" role="doc-biblioref">Field 2018</a>)</span>. Fortunately, there are non-parametric tests that enable analysis of data that do not meet the requirements for parametric tests. One example, which will be useful in comparing two different demographic groups within my data (e.g. instructors with more than two years experience teaching with technology compared to instructors with less than two years of experience teaching with technology), is the <em>Mann-Whitney-Wilcoxon</em> test <span class="citation">(<a href="#ref-gaoNonparametricStatistics2010" role="doc-biblioref">Gao 2010</a>)</span> (MWW). Instead of calculating based on the mean scores of different samples, the MWW test requires the researcher to first rank the scores and then calculate the differences between the samples based on the ranked scores. If I am to compare 3 or more groups, the <em>Kruskal-Wallis</em> test <span class="citation">(<a href="#ref-gaoNonparametricStatistics2010" role="doc-biblioref">Gao 2010</a>)</span>, based on the same ranking procedure, would be more appropriate. These two tests will be used to address research question 2.</p>
</div>
<div id="qualitative-data" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Qualitative data</h3>
<p>During phase 1b, I will gather and analyze qualitative data through the use of open-ended questions embedded in the ACAI. Part B of the ACAI currently contains open-ended questions asking respondents to describe how they would respond to the scenario if their response isn’t listed as an option in the instrument <span class="citation">(<a href="#ref-delucaACAIInstrumentSpecificationsND" role="doc-biblioref">DeLuca ND</a>)</span>. Below are some open-ended questions which might be added to Part A (demographics) of the ACAI:</p>
<ul>
<li>In a few sentences or point form notes, please describe the most important factors you consider when planning how you will assess learners in an upcoming course.</li>
<li>In a few sentences or point form notes, please describe what you understand assessment to be.</li>
<li>In what ways do you use digital technology in your assessment practice?</li>
</ul>
<p>The current and proposed open-ended questions in the ACAI are intended to provide qualitative data which will be analyzed using a theory-driven thematic coding process <span class="citation">(<a href="#ref-delucaStudentPerspectivesAssessment2018" role="doc-biblioref">DeLuca et al. 2018</a>; <a href="#ref-nameyDataReductionTechniques2008" role="doc-biblioref">Namey et al. 2008</a>)</span> to explore in greater depth the factors that influence instructors to approach assessment as they do in their courses (research question 3).</p>
<p>Namey et al. <span class="citation">(<a href="#ref-nameyDataReductionTechniques2008" role="doc-biblioref">2008</a>)</span> describe two general approaches to the analysis of qualitative data: content analysis and thematic analysis. In content analysis, the researcher “evaluates the frequency and saliency of particular words or phrases in a body of original text data in order to identify keywords or repeated ideas” (p. 138), but with the drawback that there is limited interpretation or consideration of context. In thematic analysis, researchers consider not only specific words and phrases, but also “implicit and explicit ideas” (p. 138). They further describe “data-driven” (themes emerge from the data) and “theory-driven” (the researcher identifies themes <em>a priori</em>, without consulting the data)(p. 138) approaches <span class="citation">(see also: <a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">Tashakkori, Johnson, and Teddlie 2020</a>)</span>. In phase 1b of the proposed project, I will take a theory-driven approach to a thematic analysis where data will be coded in alignment with the four dimensions of the <em>Approaches to Classroom Assessment</em> framework <span class="citation">(<a href="#ref-delucaTeachersApproachesClassroom2016" role="doc-biblioref">DeLuca et al. 2016</a>)</span> (the dotted line connecting “Phase 1a Inferences” to “Phase 1b Analysis” in figure 1). In addition, I will be particularly mindful that emergent themes may become clear in the data and I will be especially interested in those not represented in, or which are in contradiction to, the quantitative analysis <span class="citation">(<a href="#ref-tashakkoriFoundationsMixedMethods2020" role="doc-biblioref">Tashakkori, Johnson, and Teddlie 2020</a>)</span>.</p>
<p>After completing both phase 1a and 1b, I will be able to synthesize inferences from the two phase 1 analyses, and also one or more meta-inferences from the integrated analysis of the two sub-phases. These inferences will inform the phase 2 semi-structured interview questions, providing an integration point (the dotted lines from “Phase 1a Analysis,” “Phase 1b Analysis,” and “Phase 1 Meta-inferences” to “Semi-structured Interviews”).</p>
<p>Raw qualitative data from phase 2 of the project will be in the form of audio recordings of semi-structured interviews, which will be transcribed verbatim and stored as plain text files. As this data is less connected to phase 1 data because it is a different sample population, I will use a data-driven thematic approach to uncover emergent themes in the data while not precluding the possibility that theory-driven themes may emerge.</p>
<p>Following the phase 1 and phase 2 analyses, I will synthesize the inferences from phase 1a and 1b, the meta-inferences from phase 1, and the inferences from phase 2 to draw project-level meta-inferences from the whole of the data, thereby addressing research question 4.</p>
</div>
</div>
<div id="research-procedures" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Research Procedures</h2>
<p>Following approval from the University of Victoria Human Research Ethics Board (HREB) and a brief pilot phase, I will use a stratified random sampling technique to randomly select 2-3 higher education institutions from each of six regions in Canada (British Columbia, the prairie provinces, Ontario, Quebec, eastern Canada, and northern Canada), for a total of 12-18 institutions. I will contact the faculty associations at each of these institutions to request that they forward my letter of invitation and embedded link to a web-based survey. Each participant will be asked to self-identify and to nominate 8-10 learners in one of their courses and include the nominees’ contact information. To begin phase 2, I will randomly select 2-3 instructors from phase 1, then randomly select 4-5 of their nominated learners to invite to semi-structured interviews. The interviews will be conducted via web-conferencing software that allows recording and the automatic generation of transcripts. Table 5 shows a projected timeline for the research.</p>
<p><strong>Table 5</strong></p>
<p><strong><em>Projected Timeline</em></strong></p>
<table>
<thead>
<tr class="header">
<th align="left">Projected Date</th>
<th>Project Stage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">December 2021</td>
<td>UVic HREB Approval</td>
</tr>
<tr class="even">
<td align="left">January 2022</td>
<td>Pilot Phase</td>
</tr>
<tr class="odd">
<td align="left">February 2022</td>
<td>Phase 1 Data Collection</td>
</tr>
<tr class="even">
<td align="left">July 2022</td>
<td>Phase 1a and 1b Data Analysis</td>
</tr>
<tr class="odd">
<td align="left">September 2022</td>
<td>Phase 2 Data Collection</td>
</tr>
<tr class="even">
<td align="left">November 2022</td>
<td>Phase 2 Data Analysis</td>
</tr>
<tr class="odd">
<td align="left">January 2023</td>
<td>Integration and Write-Up</td>
</tr>
</tbody>
</table>
</div>
<div id="chapter-summary" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Chapter Summary</h2>
<p>In this chapter, I have discussed the three major approaches to research, <em>quantitative</em>, <em>qualitative</em>, and <em>mixed</em>, along with their underlying epistemologies. Through a review of the literature on assessment literacy and its impact on learners in higher education, I show that researchers use all three approaches and that the most important consideration in determining a method to pursue an investigation in this area is the purpose of the investigation. As the purpose of this investigation is to gain understanding of the multi-dimensional construct of assessment literacy <em>and</em> its effects on learners, a highly subjective phenomenon, a mixed research approach is justified. I discuss a possible method to gather and analyze quantitative and qualitative data from higher education instructors and their learners, along with strategies for drawing inferences and meta-inferences based on the integration of the separate analyses.</p>
<p>[^1] If funding for translation services is secured, I would extend the sample population to include French-speaking instructors and learners.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-StandardsTeacherCompetence1990" class="csl-entry">
AFT, NCME, and NEA. 1990. <span>“Standards for <span>Teacher Competence</span> in <span>Educational Assessment</span> of <span>Students</span>.”</span> <a href="https://eric.ed.gov/?id=ED323186">https://eric.ed.gov/?id=ED323186</a>.
</div>
<div id="ref-alkharusiEducationalAssessmentAttitudes2012" class="csl-entry">
Alkharusi, Hussain, Said Aldhafri, Hilal Alnabhani, and Muna Alkalbani. 2012. <span>“Educational <span>Assessment Attitudes</span>, <span>Competence</span>, <span>Knowledge</span>, and <span>Practices</span>: <span>An Exploratory Study</span> of <span>Muscat Teachers</span> in the <span>Sultanate</span> of <span>Oman</span>.”</span> <em>Journal of Education and Learning</em> 1 (2): p217. <a href="https://doi.org/r7f">https://doi.org/r7f</a>.
</div>
<div id="ref-bazeleyIntegratingAnalysesMixed2018" class="csl-entry">
Bazeley, Patricia. 2018. <em>Integrating <span>Analyses</span> in <span>Mixed Methods Research</span></em>. <span>1 Oliver’s Yard, 55 City Road London EC1Y 1SP</span>: <span>SAGE Publications Ltd</span>. <a href="https://doi.org/10.4135/9781526417190">https://doi.org/10.4135/9781526417190</a>.
</div>
<div id="ref-bearmanHowUniversityTeachers2017" class="csl-entry">
Bearman, Margaret, Phillip Dawson, Sue Bennett, Matt Hall, Elizabeth Molloy, David Boud, and Gordon Joughin. 2017. <span>“How University Teachers Design Assessments: A Cross-Disciplinary Study.”</span> <em>Higher Education</em> 74 (1): 49–64. <a href="https://doi.org/gbhr94">https://doi.org/gbhr94</a>.
</div>
<div id="ref-benediktssonImmigrantStudentsExperiences2020" class="csl-entry">
Benediktsson, Artëm Ingmar, and Hanna Ragnarsdóttir. 2020. <span>“Immigrant Students’ Experiences of Assessment Methods Used in <span>Icelandic</span> Universities.”</span> <em>Multicultural Education Review</em> 12 (2): 98–116. <a href="https://doi.org/gmcv29">https://doi.org/gmcv29</a>.
</div>
<div id="ref-bernsteinVerticalHorizontalDiscourse1999" class="csl-entry">
Bernstein, Basil. 1999. <span>“Vertical and <span>Horizontal Discourse</span>: <span>An Essay</span>.”</span> <em>British Journal of Sociology of Education</em> 20 (2): 157–73. <a href="https://doi.org/ftmsvc">https://doi.org/ftmsvc</a>.
</div>
<div id="ref-boudWhatFeedbackLiterate2021" class="csl-entry">
Boud, David, and Phillip Dawson. 2021. <span>“What Feedback Literate Teachers Do: An Empirically-Derived Competency Framework.”</span> <em>Assessment &amp; Evaluation in Higher Education</em>, April, 1–14. <a href="https://doi.org/gjrsqx">https://doi.org/gjrsqx</a>.
</div>
<div id="ref-brookhartEducationalAssessmentKnowledge2011" class="csl-entry">
Brookhart, Susan M. 2011. <span>“Educational <span>Assessment Knowledge</span> and <span>Skills</span> for <span>Teachers</span>.”</span> <em>Educational Measurement: Issues and Practice</em> 30: 3–12. <a href="https://doi.org/cwcqj4">https://doi.org/cwcqj4</a>.
</div>
<div id="ref-bulutLikertScalesFriend2021" class="csl-entry">
Bulut, Okan. 2021. <span>“Likert <span>Scales</span>: <span>Friend</span> or <span>Foe</span>?”</span> <span>Medium</span>. August 4, 2021. <a href="https://towardsdatascience.com/likert-scales-friend-or-foe-76f865786fb7">https://towardsdatascience.com/likert-scales-friend-or-foe-76f865786fb7</a>.
</div>
<div id="ref-coombsChangingApproachesClassroom2018" class="csl-entry">
Coombs, Andrew, Christopher DeLuca, Danielle LaPointe-McEwan, and Agnieszka Chalas. 2018. <span>“Changing Approaches to Classroom Assessment: <span>An</span> Empirical Study Across Teacher Career Stages.”</span> <em>Teaching and Teacher Education</em> 71 (April): 134–44. <a href="https://doi.org/gdfmcv">https://doi.org/gdfmcv</a>.
</div>
<div id="ref-coombsPersoncenteredAnalysisTeacher2020" class="csl-entry">
Coombs, Andrew, Christopher DeLuca, and Stephen MacGregor. 2020. <span>“A Person-Centered Analysis of Teacher Candidates’ Approaches to Assessment.”</span> <em>Teaching and Teacher Education</em> 87 (January): 102952. <a href="https://doi.org/gh5k6v">https://doi.org/gh5k6v</a>.
</div>
<div id="ref-coombsSeaSeaCanadian2020" class="csl-entry">
Coombs, Andrew, Jenny Ge, and Christopher DeLuca. 2020. <span>“From Sea to Sea: <span>The Canadian</span> Landscape of Assessment Education.”</span> <em>Educational Research</em>, October, 1–17. <a href="https://doi.org/gh5k4z">https://doi.org/gh5k4z</a>.
</div>
<div id="ref-creamerIntroductionFullyIntegrated2018" class="csl-entry">
Creamer, Elizabeth G. 2018. <em>An <span>Introduction</span> to <span>Fully Integrated Mixed Methods Research</span></em>. <span>2455 Teller Road, Thousand Oaks California 91320</span>: <span>SAGE Publications, Inc.</span> <a href="https://doi.org/10.4135/9781071802823">https://doi.org/10.4135/9781071802823</a>.
</div>
<div id="ref-creswellResearchDesignQualitative2009" class="csl-entry">
Creswell, John W. 2009. <em>Research <span>Design</span>: <span>Qualitative</span>, Quantitative and Mixed Method Approaches</em>. 3rd ed. <span>Thousand Oaks, CA</span>: <span>Sage Publications</span>.
</div>
<div id="ref-delucaACAIInstrumentSpecificationsND" class="csl-entry">
DeLuca, Christopher. ND. <span>“<span>ACAI Instrument</span>, <span>Specifications</span>, and <span>Related Material</span>.”</span> <span>Classroom Assessment Research Team (CART)</span>.
</div>
<div id="ref-delucaStudentPerspectivesAssessment2018" class="csl-entry">
DeLuca, Christopher, Allison E. A. Chapman-Chin, Danielle LaPointe-McEwan, and Don A. Klinger. 2018. <span>“Student Perspectives on Assessment for Learning.”</span> <em>The Curriculum Journal</em> 29 (1): 77–94. <a href="https://doi.org/gf4xvc">https://doi.org/gf4xvc</a>.
</div>
<div id="ref-delucaDifferentialSituatedView2019" class="csl-entry">
DeLuca, Christopher, A. Coombs, S. Macgregor, and A. Rasooli. 2019. <span>“Toward a Differential and Situated View of Assessment Literacy: <span>Studying</span> Teachers’ Responses to Classroom Assessment Scenarios.”</span> <em>Frontiers in Education</em> 4. <a href="https://doi.org/gh5k63">https://doi.org/gh5k63</a>.
</div>
<div id="ref-delucaApproachesClassroomAssessment2016" class="csl-entry">
DeLuca, Christopher, Danielle LaPointe-McEwan, and Ulemu Luhanga. 2016a. <span>“Approaches to Classroom Assessment Inventory: <span>A</span> New Instrument to Support Teacher Assessment Literacy.”</span> <em>Educational Assessment</em> 21: 248–66. <a href="https://doi.org/gfgtsg">https://doi.org/gfgtsg</a>.
</div>
<div id="ref-delucaTeacherAssessmentLiteracy2016" class="csl-entry">
———. 2016b. <span>“Teacher Assessment Literacy: A Review of International Standards and Measures.”</span> <em>Educational Assessment, Evaluation and Accountability</em> 28: 251–72. <a href="https://doi.org/f828mh">https://doi.org/f828mh</a>.
</div>
<div id="ref-delucaExploringAssessmentCultures2021" class="csl-entry">
DeLuca, Christopher, Nathan Rickey, and Andrew Coombs. 2021. <span>“Exploring Assessment Across Cultures: <span>Teachers</span>’ Approaches to Assessment in the <span>U</span>.<span>S</span>., <span>China</span>, and <span>Canada</span>.”</span> Edited by Sammy King Fai Hui. <em>Cogent Education</em> 8 (1): 1921903. <a href="https://doi.org/gjxvc7">https://doi.org/gjxvc7</a>.
</div>
<div id="ref-delucaPedagogySlowSignificant2021" class="csl-entry">
DeLuca, Christopher, Michelle Searle, Katrina Carbone, Jenny Ge, and Danielle LaPointe-McEwan. 2021. <span>“Toward a Pedagogy for Slow and Significant Learning about Assessment in Teacher Education.”</span> <em>Teaching and Teacher Education</em> 101 (May): 103316. <a href="https://doi.org/gh7mjt">https://doi.org/gh7mjt</a>.
</div>
<div id="ref-delucaTeachersApproachesClassroom2016" class="csl-entry">
DeLuca, Christopher, Adelina Valiquette, A. Coombs, Danielle LaPointe-McEwan, and Ulemu Luhanga. 2016. <span>“Teachers’ Approaches to Classroom Assessment: A Large-Scale Survey.”</span> <em>Assessment in Education: Principles, Policy &amp; Practice</em> 25: 355–75. <a href="https://doi.org/gh5k6p">https://doi.org/gh5k6p</a>.
</div>
<div id="ref-delucaPoliciesProgramsPractices2019" class="csl-entry">
DeLuca, Christopher, Jill Willis, Bronwen Cowie, Christine Harrison, Andrew Coombs, Andrew Gibson, and Suzanne Trask. 2019. <span>“Policies, <span>Programs</span>, and <span>Practices</span>: <span>Exploring</span> the <span>Complex Dynamics</span> of <span>Assessment Education</span> in <span>Teacher Education Across Four Countries</span>.”</span> <em>Frontiers in Education</em> 4 (November): 132. <a href="https://doi.org/gh5k2r">https://doi.org/gh5k2r</a>.
</div>
<div id="ref-earleBalancingDemandsValidity2020" class="csl-entry">
Earle, Sarah. 2020. <span>“Balancing the Demands of Validity and Reliability in Practice: Case Study of a Changing System of Primary Science Summative Assessment.”</span> <em>London Review of Education</em>. <a href="https://doi.org/gmgbps">https://doi.org/gmgbps</a>.
</div>
<div id="ref-esfandiariMixedmethodsCrosssectionalStudy2016" class="csl-entry">
Esfandiari, Rajab, Imam Khomeini International University, Qazvin, Razieh Nouri, and Imam Khomeini International University, Qazvin. 2016. <span>“A <span>Mixed</span>-Methods, <span>Cross</span>-Sectional <span>Study</span> of <span>Assessment Literacy</span> of <span>Iranian University Instructors</span>: <span>Implications</span> for <span>Teachers</span>’ <span>Professional Development</span>.”</span> <em>Iranian Journal of Applied Linguistics</em> 19 (2): 115–54. <a href="https://doi.org/gmdnd9">https://doi.org/gmdnd9</a>.
</div>
<div id="ref-fieldDiscoveringStatisticsUsing2018" class="csl-entry">
Field, Andy P. 2018. <em>Discovering Statistics Using <span>IBM SPSS</span> Statistics</em>. 5th edition, North American edition. <span>Thousand Oaks, California</span>: <span>Sage Publications Inc</span>.
</div>
<div id="ref-fivesNavigatingComplexCognitive2020" class="csl-entry">
Fives, Helenrose, and Nicole Barnes. 2020. <span>“Navigating the Complex Cognitive Task of Classroom Assessment.”</span> <em>Teaching and Teacher Education</em> 92 (June): 103063. <a href="https://doi.org/gmbws5">https://doi.org/gmbws5</a>.
</div>
<div id="ref-floresPortugueseUniversityStudents2020" class="csl-entry">
Flores, Maria Assunção, Gavin Brown, Diana Pereira, Clara Coutinho, Patrícia Santos, and Cláudia Pinheiro. 2020. <span>“Portuguese University Students’ Conceptions of Assessment: Taking Responsibility for Achievement.”</span> <em>Higher Education</em> 79 (3): 377–94. <a href="https://doi.org/gk4j65">https://doi.org/gk4j65</a>.
</div>
<div id="ref-floresPerceptionsEffectivenessFairness2015" class="csl-entry">
Flores, Maria Assunção, Ana Margarida Veiga Simão, Alexandra Barros, and Diana Pereira. 2015. <span>“Perceptions of Effectiveness, Fairness and Feedback of Assessment Methods: A Study in Higher Education.”</span> <em>Studies in Higher Education</em> 40 (9): 1523–34. <a href="https://doi.org/gfz39k">https://doi.org/gfz39k</a>.
</div>
<div id="ref-gaoNonparametricStatistics2010" class="csl-entry">
Gao, Xin. 2010. <span>“Nonparametric <span>Statistics</span>.”</span> In <em>Encyclopedia of <span>Research Design</span></em>, edited by Neil Salkind. <span>2455 Teller Road, Thousand Oaks California 91320 United States</span>: <span>SAGE Publications, Inc.</span> <a href="https://doi.org/10.4135/9781412961288.n272">https://doi.org/10.4135/9781412961288.n272</a>.
</div>
<div id="ref-garveyMixedMethodExploration2021" class="csl-entry">
Garvey, Loretta, Yvonne Hodgson, and Josephine Tighe. 2021. <span>“A Mixed Method Exploration of Student Perceptions of Assessment in Nursing and Biomedicine.”</span> <em>Journal of Further and Higher Education</em>, March, 1–14. <a href="https://doi.org/gmb9k6">https://doi.org/gmb9k6</a>.
</div>
<div id="ref-gotchSystematicReviewAssessment2014" class="csl-entry">
Gotch, Chad, and Brian F. French. 2014. <span>“A Systematic Review of Assessment Literacy Measures.”</span> <em>Educational Measurement: Issues and Practice</em> 33: 14–18. <a href="https://doi.org/gcpg4p">https://doi.org/gcpg4p</a>.
</div>
<div id="ref-gubaFourthGenerationEvaluation1989" class="csl-entry">
Guba, E. G., and Y. S. Lincoln. 1989. <em>Fourth Generation Evaluation</em>. <span>SAGE Publications</span>. <a href="https://books.google.ca/books?id=k_zxEUst46UC">https://books.google.ca/books?id=k_zxEUst46UC</a>.
</div>
<div id="ref-gubaParadigmDialog1990" class="csl-entry">
Guba, Egon G., ed. 1990. <em>The <span>Paradigm</span> Dialog</em>. <span>Newbury Park, Calif</span>: <span>Sage Publications</span>.
</div>
<div id="ref-heldDecolonizingResearchParadigms2019" class="csl-entry">
Held, Mirjam B. E. 2019. <span>“Decolonizing <span>Research Paradigms</span> in the <span>Context</span> of <span>Settler Colonialism</span>: <span>An Unsettling</span>, <span>Mutual</span>, and <span>Collaborative Effort</span>.”</span> <em>International Journal of Qualitative Methods</em> 18 (January). <a href="https://doi.org/10.1177/1609406918821574">https://doi.org/10.1177/1609406918821574</a>.
</div>
<div id="ref-hibbertsCommonSurveySampling2012" class="csl-entry">
Hibberts, Mary, R. Burke Johnson, and Kenneth Hudson. 2012. <span>“Common <span>Survey Sampling Techniques</span>.”</span> In <em>Handbook of <span>Survey Methodology</span> for the <span>Social Sciences</span></em>, edited by Lior Gideon, 53–74. <span>New York, NY</span>: <span>Springer New York</span>. <a href="https://doi.org/10.1007/978-1-4614-3876-2_5">https://doi.org/10.1007/978-1-4614-3876-2_5</a>.
</div>
<div id="ref-iannoneImpactHighStakes2020" class="csl-entry">
Iannone, Paola, Christoph Czichowsky, and Johannes Ruf. 2020. <span>“The Impact of High Stakes Oral Performance Assessment on Students’ Approaches to Learning: A Case Study.”</span> <em>Educational Studies in Mathematics</em> 103 (3): 313–37. <a href="https://doi.org/gmcvbf">https://doi.org/gmcvbf</a>.
</div>
<div id="ref-jamiesonLikertScalesHow2004" class="csl-entry">
Jamieson, Susan. 2004. <span>“Likert Scales: How to (Ab)use Them.”</span> <em>Medical Education</em> 38 (12): 1217–18. <a href="https://doi.org/b5gxwx">https://doi.org/b5gxwx</a>.
</div>
<div id="ref-jarrEducationPractitionersInterpretation2012" class="csl-entry">
Jarr, Karoline Ann. 2012. <span>“Education Practitioners’ Interpretation and Use of Assessment Results.”</span> Doctor of Philosophy, <span>University of Iowa</span>. <a href="https://doi.org/10.17077/etd.35vh2oc1">https://doi.org/10.17077/etd.35vh2oc1</a>.
</div>
<div id="ref-johnsonEducationalResearchQuantitative2017" class="csl-entry">
Johnson, R. Burke, and Larry Christensen. 2017. <em>Educational Research: <span>Quantitative</span>, Qualitative and Mixed Approaches</em>. 6th ed. <span>Thousand Oaks, CA</span>: <span>Sage Publications</span>.
</div>
<div id="ref-johnsonMixedMethodsResearch2004" class="csl-entry">
Johnson, R. Burke, and Anthony J. Onwuegbuzie. 2004. <span>“Mixed <span>Methods Research</span>: <span>A Research Paradigm Whose Time Has Come</span>.”</span> <em>Educational Researcher</em> 33 (7): 14–26. <a href="https://doi.org/cg679w">https://doi.org/cg679w</a>.
</div>
<div id="ref-johnsonDefinitionMixedMethods2007" class="csl-entry">
Johnson, R. Burke, Anthony J. Onwuegbuzie, and Lisa A. Turner. 2007. <span>“Toward a Definition of Mixed Methods Research.”</span> <em>Journal of Mixed Methods Research</em> 1 (April): 112–33. <a href="https://doi.org/10.1177/1558689806298224">https://doi.org/10.1177/1558689806298224</a>.
</div>
<div id="ref-klingerClassroomAssessmentStandards2015" class="csl-entry">
Klinger, Don, Patricia McDivitt, Barbara Howard, Todd Rogers, Marco Munoz, and Caroline Wylie. 2015. <em>Classroom <span>Assessment Standards</span> for <span>PreK</span>-12 <span>Teachers</span></em>. <span>Joint Committee on Standards for Educational Evaluation</span>. <a href="https://www.amazon.ca/Classroom-Assessment-Standards-PreK-12-Teachers-ebook/dp/B00V6C9RVO?asin=B00V6C9RVO&amp;revisionId=d45424dd&amp;format=1&amp;depth=1">https://www.amazon.ca/Classroom-Assessment-Standards-PreK-12-Teachers-ebook/dp/B00V6C9RVO?asin=B00V6C9RVO&amp;revisionId=d45424dd&amp;format=1&amp;depth=1</a>.
</div>
<div id="ref-knightSummativeAssessmentHigher2002" class="csl-entry">
Knight, Peter T. 2002. <span>“Summative <span>Assessment</span> in <span>Higher Education</span>: <span>Practices</span> in Disarray.”</span> <em>Studies in Higher Education</em> 27 (3): 275–86. <a href="https://doi.org/b25nb2">https://doi.org/b25nb2</a>.
</div>
<div id="ref-lipnevichWhatGradesMean2020" class="csl-entry">
Lipnevich, Anastasiya A., Thomas R. Guskey, Dana M. Murano, and Jeffrey K. Smith. 2020. <span>“What Do Grades Mean? <span>Variation</span> in Grading Criteria in <span>American</span> College and University Courses.”</span> <em>Assessment in Education: Principles, Policy &amp; Practice</em> 27 (5): 480–500. <a href="https://doi.org/ghjw3k">https://doi.org/ghjw3k</a>.
</div>
<div id="ref-masseyAssessmentLiteracyCollege2020" class="csl-entry">
Massey, Kyle D., Christopher DeLuca, and Danielle LaPointe-McEwan. 2020. <span>“Assessment <span>Literacy</span> in <span>College Teaching</span>: <span>Empirical Evidence</span> on the <span>Role</span> and <span>Effectiveness</span> of a <span>Faculty Training Course</span>.”</span> <em>To Improve the Academy</em> 39 (1). <a href="https://doi.org/gj5ngz">https://doi.org/gj5ngz</a>.
</div>
<div id="ref-mckimValueMixedMethods2017" class="csl-entry">
McKim, Courtney A. 2017. <span>“The <span>Value</span> of <span>Mixed Methods Research</span>: <span>A Mixed Methods Study</span>.”</span> <em>Journal of Mixed Methods Research</em> 11 (2): 202–22. <a href="https://doi.org/f952f7">https://doi.org/f952f7</a>.
</div>
<div id="ref-medlandAssessmentIlliterateShared2019" class="csl-entry">
Medland, Emma. 2019. <span>“<span>‘<span>I</span>’m an Assessment Illiterate’</span>: Towards a Shared Discourse of Assessment Literacy for External Examiners.”</span> <em>Assessment &amp; Evaluation in Higher Education</em> 44 (4): 565–80. <a href="https://doi.org/gj47j3">https://doi.org/gj47j3</a>.
</div>
<div id="ref-mertlerMeasuringTeachersKnowledge2005" class="csl-entry">
Mertler, Craig A, and Cynthia Campbell. 2005. <span>“Measuring <span>Teachers</span>’ <span>Knowledge</span> &amp; <span>Application</span> of <span>Classroom Assessment Concepts</span>: <span>Development</span> of the "<span>Assessment Literacy Inventory</span>".”</span> Presented at the Annual <span>Meeting</span> of the <span>American Educational Research Association</span>, <span>Montreal, QC, CA</span>, April 11–15. <a href="https://eric.ed.gov/?id=ED490355">https://eric.ed.gov/?id=ED490355</a>.
</div>
<div id="ref-nameyDataReductionTechniques2008" class="csl-entry">
Namey, Emily, Greg Guest, Lucy Thairu, and Laura Johnson. 2008. <span>“Data Reduction Techniques for Large Qualitative Data Sets.”</span> In <em>Handbook for <span>Team</span>-<span>Based Qualitative Research</span></em>, 137–62.
</div>
<div id="ref-nayaginPreserviceTeachersApproaches2020" class="csl-entry">
Nayagi N, Kothai, and M. Rajendran. 2020. <span>“Pre-Service Teachers’ Approaches to Assessment.”</span> <em>Humanities &amp; Social Sciences Reviews</em> 8 (1): 666–73. <a href="https://doi.org/10.18510/hssr.2020.8180">https://doi.org/10.18510/hssr.2020.8180</a>.
</div>
<div id="ref-nicholsonEnhancingStudentEngagement2018" class="csl-entry">
Nicholson, Dawn Theresa. 2018. <span>“Enhancing Student Engagement Through Online Portfolio Assessment.”</span> <em>Practitioner Research in Higher Education</em> 11 (1): 15–31.
</div>
<div id="ref-niglasMultidimensionalModelResearch2010" class="csl-entry">
Niglas, Katrin. 2010. <span>“The <span>Multidimensional Model</span> of <span>Research Methodology</span>: <span>An Integrated Set</span> of <span>Continua</span>.”</span> In <em><span>SAGE Handbook</span> of <span>Mixed Methods</span> in <span>Social</span> &amp; <span>Behavioral Research</span></em>, edited by Abbas Tashakkori and Charles Teddlie, 215–36. <span>2455 Teller Road, Thousand Oaks California 91320 United States</span>: <span>SAGE Publications, Inc.</span> <a href="http://methods.sagepub.com/book/sage-handbook-of-mixed-methods-social-behavioral-research-2e/n9.xml">http://methods.sagepub.com/book/sage-handbook-of-mixed-methods-social-behavioral-research-2e/n9.xml</a>.
</div>
<div id="ref-ogan-bekirogluPreServiceTeachers2014" class="csl-entry">
Ogan‐Bekiroglu, Feral, and Erol Suzuk. 2014. <span>“Pre‐service Teachers’ Assessment Literacy and Its Implementation into Practice.”</span> <em>The Curriculum Journal</em> 25 (3): 344–71. <a href="https://doi.org/gmdnfr">https://doi.org/gmdnfr</a>.
</div>
<div id="ref-pereiraPerceptionsPortugueseUndergraduate2017" class="csl-entry">
Pereira, Diana, Maria Assunção Flores, and Alexandra Barros. 2017. <span>“Perceptions of <span>Portuguese</span> Undergraduate Students about Assessment: A Study in Five Public Universities.”</span> <em>Educational Studies</em> 43 (4): 442–63. <a href="https://doi.org/gmb98n">https://doi.org/gmb98n</a>.
</div>
<div id="ref-pereiraHowUndergraduatesPerceive2021" class="csl-entry">
Pereira, Diana, Irene Cadime, Gavin Brown, and Maria Assunção Flores. 2021. <span>“How Do Undergraduates Perceive the Use of Assessment? <span>A</span> Study in Higher Education.”</span> <em>European Journal of Higher Education</em>, January, 1–17. <a href="https://doi.org/gk36vq">https://doi.org/gk36vq</a>.
</div>
<div id="ref-plakeTeacherAssessmentLiteracy1993" class="csl-entry">
Plake, Barbara S. 1993. <span>“Teacher Assessment Literacy: <span>Teachers</span>’ Competencies in the Educational Assessment of Students.”</span> <em>Mid-Western Educational Researcher</em> 6 (1): 21–27. <a href="http://www.mwera.org/MWER/documents/MWER-1993-Winter-6-1.pdf">http://www.mwera.org/MWER/documents/MWER-1993-Winter-6-1.pdf</a>.
</div>
<div id="ref-plakeAssessmentCompetenciesTeachers2005" class="csl-entry">
Plake, Barbara S., James C. Impara, and Jennifer J. Fager. 2005. <span>“Assessment <span>Competencies</span> of <span>Teachers</span>: <span>A National Survey</span>.”</span> <em>Educational Measurement: Issues and Practice</em> 12 (4): 10–12. <a href="https://doi.org/fw7g9p">https://doi.org/fw7g9p</a>.
</div>
<div id="ref-rencklyAirUniversitySampling2002" class="csl-entry">
Renckly, Thomas R. 2002. <em>Air <span>University Sampling</span> and <span>Surveying Handbook</span></em>. <span>United States of America Department of Defense</span>.
</div>
<div id="ref-schneiderLinkingPersonalityTeachers2020" class="csl-entry">
Schneider, Christoph, Christopher DeLuca, Marcela Pozas, and Andrew Coombs. 2020. <span>“Linking Personality to Teachers’ Literacy in Classroom Assessment: A Cross-Cultural Study.”</span> <em>Educational Research and Evaluation</em> 26 (1-2): 53–74. <a href="https://doi.org/gk5r6r">https://doi.org/gk5r6r</a>.
</div>
<div id="ref-shepardRoleAssessmentLearning2000" class="csl-entry">
Shepard, Lorrie A. 2000. <span>“The <span>Role</span> of <span>Assessment</span> in a <span>Learning Culture</span>.”</span> <em>Educational Researcher</em> 29 (7): 4–14. <a href="https://doi.org/cw9jwc">https://doi.org/cw9jwc</a>.
</div>
<div id="ref-smithPreparingTeachersUse2014" class="csl-entry">
Smith, Lisa F., Mary F. Hill, Bronwen Cowie, and Alison Gilmore. 2014. <span>“Preparing <span>Teachers</span> to <span>Use</span> the <span>Enabling Power</span> of <span>Assessment</span>.”</span> In <em>Designing <span>Assessment</span> for <span>Quality Learning</span></em>, edited by Claire Wyatt-Smith, Valentina Klenowski, and Peta Colbert, 1:303–23. The <span>Enabling Power</span> of <span>Assessment</span>. <span>Dordrecht</span>: <span>Springer Netherlands</span>. <a href="https://doi.org/10.1007/978-94-007-5902-2_19">https://doi.org/10.1007/978-94-007-5902-2_19</a>.
</div>
<div id="ref-solomonidouStudentsConceptionsAssessment2017" class="csl-entry">
Solomonidou, Georgia, and Michalis Michaelides. 2017. <span>“Students’ Conceptions of Assessment Purposes in a Low Stakes Secondary-School Context: <span>A</span> Mixed Methodology Approach.”</span> <em>Studies in Educational Evaluation</em> 52 (March): 35–41. <a href="https://doi.org/f9w8gd">https://doi.org/f9w8gd</a>.
</div>
<div id="ref-tashakkoriFoundationsMixedMethods2020" class="csl-entry">
Tashakkori, Abbas, Burke Johnson, and Charles Teddlie. 2020. <em>Foundations of Mixed Methods Research: Integrating Quantitative and Qualitative Approaches in the Social and Behavioral Sciences</em>. Second Edition. <span>Thousand Oaks</span>: <span>SAGE Publications, Inc</span>.
</div>
<div id="ref-tekirAlignmentIntendedEnacted2021" class="csl-entry">
Tekir, Serpil. 2021. <span>“Alignment of the <span>Intended</span>, <span>Enacted</span>, <span>Received</span> and <span>Assessed Curriculum</span> in <span>EFL Pre</span>-<span>Service Measurement</span> and <span>Evaluation Education</span>.”</span> <em>Education and Science</em>. <a href="https://doi.org/gmgbpz">https://doi.org/gmgbpz</a>.
</div>
<div id="ref-watsonSmallDataOnline2017" class="csl-entry">
Watson, Cate, Anna Wilson, Valerie Drew, and Terrie Lynn Thompson. 2017. <span>“Small Data, Online Learning and Assessment Practices in Higher Education: A Case Study of Failure?”</span> <em>Assessment &amp; Evaluation in Higher Education</em> 42 (7): 1030–45. <a href="https://doi.org/ggrgxn">https://doi.org/ggrgxn</a>.
</div>
<div id="ref-willisConceptualisingTeachersAssessment2013" class="csl-entry">
Willis, Jill, Lenore Adie, and Val Klenowski. 2013. <span>“Conceptualising Teachers’ Assessment Literacies in an Era of Curriculum and Assessment Reform.”</span> <em>The Australian Educational Researcher</em> 40 (2): 241–56. <a href="https://doi.org/gh5k7d">https://doi.org/gh5k7d</a>.
</div>
<div id="ref-xuTeacherAssessmentLiteracy2016" class="csl-entry">
Xu, Yueting, and Gavin T. L. Brown. 2016. <span>“Teacher Assessment Literacy in Practice: <span>A</span> Reconceptualization.”</span> <em>Teaching and Teacher Education</em> 58: 149–62. <a href="https://doi.org/f8txgm">https://doi.org/f8txgm</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="candidacy-question-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cmadland/assessment/edit/master/02-candidacy-question-2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
