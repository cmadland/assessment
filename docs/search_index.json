[["a-review-of-the-literature-on-assessment-in-technology-mediated-higher-education.html", "Approaches to Assessment in Higher Education A Review of the Literature on Assessment in Technology-mediated Higher Education Introduction A Brief History of Assessment Research Factors", " Approaches to Assessment in Higher Education Colin Madland 2022-06-08 A Review of the Literature on Assessment in Technology-mediated Higher Education A Review of the Literature on Assessment in Technology-mediated Higher Education {-} Introduction {-} A Brief History of Assessment Research {-} Factors {-} Assessment Cultures {-} Impact of Technology on Approaches to Assessment in higher Education Introduction The assessment of learning is a critical component of the work of higher education instructors, yet not enough is known about how instructors plan for and make decisions about how they will assess their students’ learning, particularly in the context of technology-mediated environments. In her important article, Shepard (2000) argued at the turn of the century that contemporary (at the time) assessment structures originated in past models of curriculum and instruction which were popular in the early 1900s. These curricular models emphasized the work of psychologists like Thorndike (Thorndike, 1905) and Skinner (Skinner, 1938) who viewed the process of learning as being grounded in the mechanistic view of behaviourism where learning is the result of the precise and controlled input of ‘knowledge’ and reinforced with rewards for correct responses. As such, instructors (appropriately at the time) designed their assessments to align with the curricular goals of the time and assessed learning by determining whether or not a learner could provide the single correct response to a given question at a time removed from the instruction. However, in the latter half of the 20th century, when western psychologists discovered the ideas of Lev Vygotsky (Vygotsky, 1978), a contemporary of the previously mentioned psychologists, curricula began to take a more social-constructivist approach that emphasized higher-order thinking, problem-solving in social contexts, and metacognitive skills over rote memorization. Unfortunately, it seemed that the efficiencies of testing memory, recognition, and recall through selected-response tests were too deeply embedded in the practices of HE instructors who resisted changing their assessments to match the new curricular goals. Barnett (2007) contends that these new goals lead to a state of supercomplexity with which both learners and instructors must grapple, and which Garrison et al. (2015) argue requires deep changes to how instructors approach teaching and how learners approach learning. Shepard argues for the need to integrate assessment and instruction in such a way as to engage learners in authentic performance tasks more suited to modern understandings of cognition. It appears now that, in the twenty-two years since Shepard wrote her paper, the goals of early 21st century curricula have continued to diverge from those of the 20th century with the World Economic Forum identifying competencies in collaboration, analytical thinking, creative problem-solving, and the continual learning as being priorities for 21st century employers (Forum, 2020). Additionally, digital and networked technologies have become ubiquitous in society and higher education (Broadfoot, 2016; Pellegrino &amp; Quellmalz, 2010; Webb &amp; Ifenthaler, 2018) and the influence of digital technologies on instructors approaches to assessment remains under researched. Models of assessment which prioritize testing skills in a manner aligned with 20th century curricular models are no longer adequate because they no longer align with the priorities of modern HE (Broadfoot, 2016; Crooks, 1988; Pellegrino &amp; Quellmalz, 2010; Timmis et al., 2016). Despite Shepard’s exhortation to integrate modern curricular goals with aligned assessment practices, both local and systemic change in this area has been elusive (Broadfoot, 2016; Earl, 2013). This is in part because the approaches that individual instructors take to assessment are driven by complex factors at the individual level as well as local and systemic levels (Black &amp; Wiliam, 1998; DeLuca et al., 2019; Willis et al., 2013). The variety of factors, combined with the relative freedom instructors have to design their courses and assessments leads to approaches to assessment that are variable, idiosyncratic, and often influenced by individual instructors’ past experiences rather than a deep understanding of assessment theory (Lipnevich et al., 2020; Massey et al., 2020). {–Researchers are engaged in important work exploring how K12 teachers approach assessment in their classrooms, including DeLuca et al.’s (2016) Approaches to Classroom Assessment Inventory, but there has been less work in relation to technology-mediated higher education. –} Given the confluence of the preceding factors, the purpose of this literature review will be to synthesize and analyze the literature related to approaches to assessment among higher education instructors. The review will begin with a survey of historical conceptions of assessment, followed by a deeper analysis of the literature since 2010 with a focus on assessment in technologically-mediated higher education. This review is guided by four research questions: What factors shape higher education instructors’ approaches to assessment? What are the major themes or patterns in the literature related to approaches to assessment in higher education? What are the major themes or patterns in the literature related to the impact of technology on assessment in higher education? What gaps exist in the literature related to approaches to assessment in technology-mediated higher education? A Brief History of Assessment Research There are deep and rich bodies of literature addressing educational assessment writ large, and it would not be possible in the scope of this article to adequately cover the full breadth and depth of the published research (Joughin, 2009). Consequently, while any starting point may seem somewhat arbitrary, the author chose to begin with the genesis of modern thinking of assessment being divided into “summative assessment” (for the purposes of providing a final judgement of learning) and “formative assessment” (for the purposes of providing feedback to learners and instructors to improve learning). This framing of assessment was popularized by Benjamin Bloom and remains prominent in the literature. Among the more influential publications related to modern views of assessment (then usually called “evaluation”) was Scriven’s (1967) article in which he drew distinctions between “formative” and “summative” evaluation. Formative evaluation was described as evaluation for the purposes of improvement, and summative evaluation was seen as a validation of the quality of work at the end of a process. This distinction was quickly incorporated into Bloom’s (1968) ideas related to mastery learning and began to be promoted as a model for educational reform. However, by the late 1990s, when Black and Wiliam (1998) published their landmark review of the literature on assessment, the idea of formative assessment was still not well-defined or implemented. Black and Wiliam framed formative assessment as “encompassing all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the teaching and learning activities in which they are engaged” (1998, pp. 7–8). Although Black and Wiliam came to very strongly-stated conclusions about the value of formative assessments (e.g. “The research reported here shows conclusively that formative assessment does improve learning. The gains in achievement appear to be quite considerable, and as noted earlier, amongst the largest ever reported for educational interventions.” (1998, p. 61)), reliance on summative assessments in HE has remained high Lipnevich et al. (2020). The National Research Council’s (NRC) 2001 foundational report Knowing What Students Know: The Science and Design of Educational Assessment, advanced understanding of assessment with their definition of assessment as “a process of drawing reasonable inferences about what students know on the basis of evidence derived from observations of what they say, do, or make in selected situations” (Pellegrino et al., 2001, p. 112) or, more simply, “reasoning from evidence” (Pellegrino et al., 2001, p. 43), based on Mislevy’s (1994, p. 4) assertion that “test theory is machinery for reasoning from students’ behavior to conjectures about their competence, as framed in a particular conception of competence.”. Such a parsimonious description, however, may hide some of the complexities of fairly and equitably coming to know what learners know and can do in relation to particular outcomes. Since knowledge of a particular domain cannot be directly observed in a learner, and therefore cannot be quantified, instructors must rely on data gathered during the teaching process to support a particular inference about what a learner probably knows. The data gathered from performance tasks such as exams, essays, portfolios, labs, etc, become evidence when they support an inference about what a learner knows and can do. Hence, all assessments are probabilistic, not deterministic. Brown (2004) showed in a study of K12 teachers in New Zealand that teachers’ “conceptions of assessment” (p. 302), or the organizing frameworks teachers use to understand the phenomenon of assessment, are not simple constructs, but are complex and interconnected. Brown’s model shows four common conceptions of assessment among K12 teachers: - improvement of teaching and learning, - school accountability, - student accountability, or - treating assessment as irrelevant. James (2008) highlights important connections between assessment practices and learning theory, noting that both behaviourism and cognitivism emphasize the acquisition of knowledge as a sort of commodity and thus both lead to assessment strategies that emphasize the individual reproduction of knowlege that has either been taught, or has been constructed in meaning-making exercises. Contrary to these approaches, socio-cultural learning theory emphasizes the communal production of meaning through social tools and social interactions between peers and more capable ‘others’. Thus, assessment in socio-cultural contexts, which James argues is under-theorized, ought to be holistic and situated within the communal learning process, rather than an event that happens sometime after the learning has happened. Similarly, Fletcher et al. (2012) note that teachers who view teaching as the transmission of knowledge tend to rely more heavily on assessment tasks which require learners to reproduce what they have been taught. On the other hand, teachers who take a more socio-constructivist view that knowledge is built over time tend to conceive of assessment as being an integral part of the teaching and learning process as opposed to a culminating activity. In a study that bridged K12 and higher education by examining pre-service teachers’ conceptions of assessment, DeLuca et al. (2013) Factors Offerdahl and Tomanek (2011) investigated instructors’ assessment thinking in relation to being presented with new assessment strategies. They found that changes in assessment practice didn’t necessarily follow from changes in assessment thinking and that there are many factors which influence both assessment thinking and practice. Assessment Cultures (DeLuca et al., 2021) (Birenbaum, 2016) [delucaCrossculturalComparisonGerman2020] [willisConceptualisingTeachersAssessment2013] (Hébert, 2021) Assessment approaches including assessment purposes, assessment processes, fairness, and measurement theory, and English teachers’ professional development needs remain underexplored in the Middle East and North Africa regions. This study provided empirical evidence on English language teachers approaches in the Saudi higher education context. A survey was used to examine the teachers’ current approaches to classroom assessment. A total of 287 subjects (191 women and 94 men) participated in the survey. The results revealed that both the male and female participants valued and endorsed assessments alike. However, female participants were found to value assessment purposes more than their male counterparts. Fairness in assessment approaches was the least valued item in teachers’ identified assessment approaches. Experienced teachers who identified themselves as competent in their role valued assessment fairness and measurement theory more than novice teachers. The present work broadens our knowledge on teachers’ assessment approaches in relation to gender, career stage, and academic position, which support interested researchers and policy-makers in decision-making regarding designing professional development programs. 0.0.1 Impact of Technology on Approaches to Assessment in higher Education Garrison et al. (2015) introduce the idea of shared metacognition as being a necessary competency for learner in 21st century higher education. They build a case that modern learners must contend with supercomplexity (Barnett, 2007), which refers to open-endedness and ambiguity characteristic of modern society and culture. Supercomplexity is both compounded and mitigated through the integration of digital technologies in higher education. While there is additional cognitive load associated with learning through technology, it is also true that technology can help learners organize information through a cognitive offloading. - shared metacognition - community of inquiry - supercomplexity (Barnett, 2007) - &gt;Although there is an increasing awareness of technology capable of supporting meaningful learning through engagement, too often technology in education has been subservient to dated approaches, such as enhancing lectures with visuals or providing optional online discussions where information transmission remains core to the classroom experience. In these situations, technology has continued to reinforce the disengagement of students in the classroom. (p. 40) - Some researchers use terms, such as “technology-enhanced”, “technology-enabled, or”technology-rich” (Eyal, 2012, p. 37) assessment, which show a positivity bias towards the use of technoloogy in higher education. Although this terminology will be a component of the search process, this review will use the more neutral term “technology-mediated assessment” whenever possible in light of the fact that assessment is not always “enhanced”, “enabled”, or “enriched” with the use of technology. Modern universities are under pressure from public and private funding agencies (Hébert, 2021) and employers to demonstrate that graduates are equipped for the demands of citizenship in the 21st century (Pellegrino &amp; Quellmalz, 2010). Technological changes in society have impacted how people live, work, play, and learn in many ways leading to additional pressures on universities to respond to new realities (Forum, 2020). As a result, universities have incorporated many technologies into how they operate, including student information systems, faculty career tracking systems, and learning management systems, to name a few. Technologies have also impacted how instructors teach, with many instructors incorporating digital tools such as the aforementioned learning management systems, but also in-class slide-decks to accompany lectures (often replacing older technologies, such as overhead projectors and chalk boards), digital response systems, digital distribution and gathering of documents, digital feedback, networked learning environments (i.e., blogs, git-based repositories, wikis, and other collaborative digital learning environments), and, more recently, artificially intelligent agents and algorithms used to interact with learners and even evaluate learner artifacts. Many of these technologies have allowed both universities and instructors to automate, and therefore scale up, processes and procedures that formerly consumed significant time and labour, however, in most cases, they have not fundamentally changed the kind of work that is being done (Broadfoot, 2016). For example, automated grading of selected-response tests using a learning management system or a bubble sheet has greatly reduced the amount of time it takes to score selected-response tests, saving instructors significant time. However, this technology has not fundamentally changed the selected-response test itself. Similarly, collecting digital artifacts, like essays, has improved tracking and likely reduced the number of lost essays, but it has not fundamentally changed the nature of the assessment task. Despite the widespread adoption of technologies for many tasks in higher education, it would seem that technology has not yet significantly transformed how instructors encourage or assess learning in their classes (Garrison &amp; Akyol, 2015). References Barnett, R. (2007). A Will to Learn: Being a Student in an Age of Uncertainty. McGraw-Hill Education. Birenbaum, M. (2016). Assessment Culture Versus Testing Culture: The Impact on Assessment for Learning. In D. Laveault &amp; L. Allal (Eds.), Assessment for Learning: Meeting the Challenge of Implementation (Vol. 4, pp. 275–292). Springer International Publishing. https://doi.org/10.1007/978-3-319-39211-0_16 Black, P., &amp; Wiliam, D. (1998). Assessment and Classroom Learning. Assessment in Education: Principles, Policy &amp; Practice, 5(1), 7–74. https://doi.org/fpnss4 Bloom, B. (1968). Learning for Mastery. Instruction and Curriculum. Regional Education Laboratory for the Carolinas and Virginia, Topical Papers and Reprints, Number 1. Evaluation Comment, 1(2), 12. Broadfoot, P. (2016). Assessment for Twenty-First-Century Learning: The Challenges Ahead. In M. J. Spector, B. B. Lockee, &amp; M. D. Childress (Eds.), Learning, Design, and Technology (pp. 1–23). Springer International Publishing. https://doi.org/10.1007/978-3-319-17727-4_64-1 Brown, G. T. L. (2004). Teachers’ conceptions of assessment: Implications for policy and professional development. Assessment in Education: Principles, Policy &amp; Practice, 11(3), 301–318. https://doi.org/10.1080/0969594042000304609 Crooks, T. J. (1988). The Impact of Classroom Evaluation Practices on Students. Review of Educational Research, 58(4), 438–481. https://doi.org/dvd8nf DeLuca, C., Chavez, T., &amp; Cao, C. (2013). Establishing a foundation for valid teacher judgement on student learning: The role of pre-service assessment education. Assessment in Education: Principles, Policy &amp; Practice, 20(1), 107–126. https://doi.org/gj5v98 DeLuca, C., Coombs, A., Macgregor, S., &amp; Rasooli, A. (2019). Toward a differential and situated view of assessment literacy: Studying teachers’ responses to classroom assessment scenarios. Frontiers in Education, 4. https://doi.org/gh5k63 DeLuca, C., LaPointe-McEwan, D., &amp; Luhanga, U. (2016). Approaches to classroom assessment inventory: A new instrument to support teacher assessment literacy. Educational Assessment, 21, 248–266. https://doi.org/gfgtsg DeLuca, C., Rickey, N., &amp; Coombs, A. (2021). Exploring assessment across cultures: Teachers’ approaches to assessment in the U.S., China, and Canada. Cogent Education, 8(1), 1921903. https://doi.org/gjxvc7 Earl, L. M. (2013). Assessment as learning: Using classroom assessment to maximize student learning (Second edition). Corwin Press. Eyal, L. (2012). Digital Assessment Literacy the Core Role of the Teacher in a Digital Environment. Journal of Educational Technology &amp; Society, 15(2), 37–49. Fletcher, R. B., Meyer, L. H., Anderson, H., Johnston, P., &amp; Rees, M. (2012). Faculty and Students Conceptions of Assessment in Higher Education. Higher Education, 64(1), 119–133. https://doi.org/ctccpq Forum, W. E. (2020). The Future of Jobs Report 2020. World Economic Forum. Garrison, D. R., &amp; Akyol, Z. (2015). Thinking Collaboratively in Educational Environments: Shared Metacognition and Co-Regulation in Communities of Inquiry. In J. Lock, P. Redmond, &amp; P. A. Danaher (Eds.), Educational Developments, Practices and Effectiveness (pp. 39–52). Palgrave Macmillan UK. https://doi.org/10.1057/9781137469939_3 Hébert, C. (2021). Online Remote Proctoring Software in the Neoliberal Institution: Measurement, Accountability, and Testing Culture. In Education, 27(1), 23–40. https://doi.org/10.37119/ojs2021.v27i1.507 James, M. (2008). Assessment and learning. In S. Swaffield (Ed.), Unlocking Assessment. David Fulton Publishers. https://doi.org/10.4324/9780203930939 Joughin, G. (Ed.). (2009). Assessment, Learning and Judgement in Higher Education. Springer Netherlands. https://doi.org/10.1007/978-1-4020-8905-3 Lipnevich, A. A., Guskey, T. R., Murano, D. M., &amp; Smith, J. K. (2020). What do grades mean? Variation in grading criteria in American college and university courses. Assessment in Education: Principles, Policy &amp; Practice, 27(5), 480–500. https://doi.org/ghjw3k Massey, K. D., DeLuca, C., &amp; LaPointe-McEwan, D. (2020). Assessment Literacy in College Teaching: Empirical Evidence on the Role and Effectiveness of a Faculty Training Course. To Improve the Academy, 39(1). https://doi.org/gj5ngz Mislevy, R. J. (1994). Test theory reconcieved. ETS Research Report Series, 1994(1), i–38. https://doi.org/gjm236 Offerdahl, E. G., &amp; Tomanek, D. (2011). Changes in instructors’ assessment thinking related to experimentation with new strategies. Assessment and Evaluation in Higher Education, 36(7), 781–795. https://doi.org/d89z7p Pellegrino, J. W., Chudowsky, N., &amp; Glaser, R. (2001). Knowing What Students Know: The Science and Design of Educational Assessment. National Academies Press. https://doi.org/10.17226/10019 Pellegrino, J. W., &amp; Quellmalz, E. S. (2010). Perspectives on the Integration of Technology and Assessment. Journal of Research on Technology in Education, 43(2), 119–134. https://doi.org/ggfh8z Scriven, M. (1967). The methodology of evaluation. In B. O. Smith (Ed.), Perspectives of curriculum evaluation. Rand McNally. Shepard, L. A. (2000). The Role of Assessment in a Learning Culture. Educational Researcher, 29(7), 4–14. https://doi.org/cw9jwc Skinner, B. (1938). The behaviour of organisms. Appleton-Century-Crofts. Thorndike, E. L. (1905). The Elements of psychology. A.G. Seiler. Timmis, S., Broadfoot, P., Sutherland, R., &amp; Oldfield, A. (2016). Rethinking assessment in a digital age: Opportunities, challenges and risks. British Educational Research Journal, 42(3), 454–476. https://doi.org/gftz95 Vygotsky, L. S. (1978). Mind in society (M. Cole, V. John-Steiner, S. Scribner, &amp; E. Souberman, Eds.; A. R. Luria, Trans.). Harvard University Press. Webb, M., &amp; Ifenthaler, D. (2018). Assessment as, for, and of Twenty-First Century Learning Using Information Technology: An Overview. In J. Voogt, G. Knezek, R. Christensen, &amp; K.-W. Lai (Eds.), Second Handbook of Information Technology in Primary and Secondary Education (pp. 581–600). Springer International Publishing. https://doi.org/10.1007/978-3-319-71054-9_37 Willis, J., Adie, L., &amp; Klenowski, V. (2013). Conceptualising teachers’ assessment literacies in an era of curriculum and assessment reform. The Australian Educational Researcher, 40(2), 241–256. https://doi.org/gh5k7d "],["references.html", "References", " References Barnett, R. (2007). A Will to Learn: Being a Student in an Age of Uncertainty. McGraw-Hill Education. Birenbaum, M. (2016). Assessment Culture Versus Testing Culture: The Impact on Assessment for Learning. In D. Laveault &amp; L. Allal (Eds.), Assessment for Learning: Meeting the Challenge of Implementation (Vol. 4, pp. 275–292). Springer International Publishing. https://doi.org/10.1007/978-3-319-39211-0_16 Black, P., &amp; Wiliam, D. (1998). Assessment and Classroom Learning. Assessment in Education: Principles, Policy &amp; Practice, 5(1), 7–74. https://doi.org/fpnss4 Bloom, B. (1968). Learning for Mastery. Instruction and Curriculum. Regional Education Laboratory for the Carolinas and Virginia, Topical Papers and Reprints, Number 1. Evaluation Comment, 1(2), 12. Broadfoot, P. (2016). Assessment for Twenty-First-Century Learning: The Challenges Ahead. In M. J. Spector, B. B. Lockee, &amp; M. D. Childress (Eds.), Learning, Design, and Technology (pp. 1–23). Springer International Publishing. https://doi.org/10.1007/978-3-319-17727-4_64-1 Brown, G. T. L. (2004). Teachers’ conceptions of assessment: Implications for policy and professional development. Assessment in Education: Principles, Policy &amp; Practice, 11(3), 301–318. https://doi.org/10.1080/0969594042000304609 Crooks, T. J. (1988). The Impact of Classroom Evaluation Practices on Students. Review of Educational Research, 58(4), 438–481. https://doi.org/dvd8nf DeLuca, C., Chavez, T., &amp; Cao, C. (2013). Establishing a foundation for valid teacher judgement on student learning: The role of pre-service assessment education. Assessment in Education: Principles, Policy &amp; Practice, 20(1), 107–126. https://doi.org/gj5v98 DeLuca, C., Coombs, A., Macgregor, S., &amp; Rasooli, A. (2019). Toward a differential and situated view of assessment literacy: Studying teachers’ responses to classroom assessment scenarios. Frontiers in Education, 4. https://doi.org/gh5k63 DeLuca, C., LaPointe-McEwan, D., &amp; Luhanga, U. (2016). Approaches to classroom assessment inventory: A new instrument to support teacher assessment literacy. Educational Assessment, 21, 248–266. https://doi.org/gfgtsg DeLuca, C., Rickey, N., &amp; Coombs, A. (2021). Exploring assessment across cultures: Teachers’ approaches to assessment in the U.S., China, and Canada. Cogent Education, 8(1), 1921903. https://doi.org/gjxvc7 Earl, L. M. (2013). Assessment as learning: Using classroom assessment to maximize student learning (Second edition). Corwin Press. Eyal, L. (2012). Digital Assessment Literacy the Core Role of the Teacher in a Digital Environment. Journal of Educational Technology &amp; Society, 15(2), 37–49. Fletcher, R. B., Meyer, L. H., Anderson, H., Johnston, P., &amp; Rees, M. (2012). Faculty and Students Conceptions of Assessment in Higher Education. Higher Education, 64(1), 119–133. https://doi.org/ctccpq Forum, W. E. (2020). The Future of Jobs Report 2020. World Economic Forum. Garrison, D. R., &amp; Akyol, Z. (2015). Thinking Collaboratively in Educational Environments: Shared Metacognition and Co-Regulation in Communities of Inquiry. In J. Lock, P. Redmond, &amp; P. A. Danaher (Eds.), Educational Developments, Practices and Effectiveness (pp. 39–52). Palgrave Macmillan UK. https://doi.org/10.1057/9781137469939_3 Harlen, W., &amp; Deakin Crick, R. (2002). A systematic review of the impact of summative assessment and tests on students’ motivation for learning. Research Evidence in Education Library, 1, 151. Hébert, C. (2021). Online Remote Proctoring Software in the Neoliberal Institution: Measurement, Accountability, and Testing Culture. In Education, 27(1), 23–40. https://doi.org/10.37119/ojs2021.v27i1.507 James, M. (2008). Assessment and learning. In S. Swaffield (Ed.), Unlocking Assessment. David Fulton Publishers. https://doi.org/10.4324/9780203930939 Joughin, G. (Ed.). (2009). Assessment, Learning and Judgement in Higher Education. Springer Netherlands. https://doi.org/10.1007/978-1-4020-8905-3 Lipnevich, A. A., Guskey, T. R., Murano, D. M., &amp; Smith, J. K. (2020). What do grades mean? Variation in grading criteria in American college and university courses. Assessment in Education: Principles, Policy &amp; Practice, 27(5), 480–500. https://doi.org/ghjw3k Massey, K. D., DeLuca, C., &amp; LaPointe-McEwan, D. (2020). Assessment Literacy in College Teaching: Empirical Evidence on the Role and Effectiveness of a Faculty Training Course. To Improve the Academy, 39(1). https://doi.org/gj5ngz Mislevy, R. J. (1994). Test theory reconcieved. ETS Research Report Series, 1994(1), i–38. https://doi.org/gjm236 Offerdahl, E. G., &amp; Tomanek, D. (2011). Changes in instructors’ assessment thinking related to experimentation with new strategies. Assessment and Evaluation in Higher Education, 36(7), 781–795. https://doi.org/d89z7p Pellegrino, J. W., Chudowsky, N., &amp; Glaser, R. (2001). Knowing What Students Know: The Science and Design of Educational Assessment. National Academies Press. https://doi.org/10.17226/10019 Pellegrino, J. W., &amp; Quellmalz, E. S. (2010). Perspectives on the Integration of Technology and Assessment. Journal of Research on Technology in Education, 43(2), 119–134. https://doi.org/ggfh8z Scriven, M. (1967). The methodology of evaluation. In B. O. Smith (Ed.), Perspectives of curriculum evaluation. Rand McNally. Shepard, L. A. (2000). The Role of Assessment in a Learning Culture. Educational Researcher, 29(7), 4–14. https://doi.org/cw9jwc Skinner, B. (1938). The behaviour of organisms. Appleton-Century-Crofts. Thorndike, E. L. (1905). The Elements of psychology. A.G. Seiler. Timmis, S., Broadfoot, P., Sutherland, R., &amp; Oldfield, A. (2016). Rethinking assessment in a digital age: Opportunities, challenges and risks. British Educational Research Journal, 42(3), 454–476. https://doi.org/gftz95 Vygotsky, L. S. (1978). Mind in society (M. Cole, V. John-Steiner, S. Scribner, &amp; E. Souberman, Eds.; A. R. Luria, Trans.). Harvard University Press. Webb, M., &amp; Ifenthaler, D. (2018). Assessment as, for, and of Twenty-First Century Learning Using Information Technology: An Overview. In J. Voogt, G. Knezek, R. Christensen, &amp; K.-W. Lai (Eds.), Second Handbook of Information Technology in Primary and Secondary Education (pp. 581–600). Springer International Publishing. https://doi.org/10.1007/978-3-319-71054-9_37 Willis, J., Adie, L., &amp; Klenowski, V. (2013). Conceptualising teachers’ assessment literacies in an era of curriculum and assessment reform. The Australian Educational Researcher, 40(2), 241–256. https://doi.org/gh5k7d "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
