[["index.html", "Assessment in Online Higher Education 1 Welcome", " Assessment in Online Higher Education Colin Madland 2021-09-07 1 Welcome This site hosts the open development of my PhD dissertation at University of Victoria. Now you can stop asking me if I am done. I’m working on getting the reference list to look a bit better and need to get my images on track. "],["candidacy-question-1.html", " 2 Candidacy Question 1 2.1 Topic of the Research 2.2 Problem to be Researched 2.3 Purpose of the Research 2.4 Structure of the Paper 2.5 Defining Assessment 2.6 Conceptions of Assessment 2.7 Assessment Literacy 2.8 Assessment and Technology 2.9 Impact on Learners 2.10 Theoretical Framework Overview 2.11 Summary and Research Questions 2.12 Significance of the Research", " 2 Candidacy Question 1 Critically analyze the literature related to assessment literacy - how educators conceptualize and practice assessment in relation to student learning. Consider the multiple conceptions of assessment literacy that have operated in the literature with attention to their implications for practice. Consider also the relationship between assessment practices and their impacts on learners. Clearly state your research topic and include the problem, purpose, and research questions that have emerged based on the higher education landscape and considering online contexts. Provide an overview of the theoretical framework(s) within which you choose to situate your study. Assessing learning is both a critical component of the work of teaching in higher education (HE) and also a major factor in learners’ experiences of HE (J. Biggs 1999; Menucha Birenbaum 2007; Woldeab and Brothen 2019). Instructors in HE often inherit the assessment practices of their supervisors and instructors (Lipnevich et al. 2020), potentially leading to over-reliance on practices that may not align with modern assessment theories (Boud 2020; Massey, DeLuca, and LaPointe-McEwan 2020; Shepard 2000). These traditional practices tend to emphasize summative, selected-response tests (Flores et al. 2015; Harlen and Deakin Crick 2002; Lipnevich et al. 2020) which may be characterized by low reliability, leading to inferences which may not be valid reflections of learner achievement (Knight 2002; Smith Glasgow, Dreher, and Schreiber 2019). Also, the influence of an increasingly technological society is pushing higher education institutions (HEI) to integrate digital tools into the teaching and learning process (Pellegrino and Quellmalz 2010). This trend saw a massive increase in activity as a result of the COVID-19 pandemic that forced most HEI in Canada to pivot to emergency remote teaching in the spring of 2020 through at least the summer of 2021 and perhaps beyond. While this increased use of technology may be understood by some as progress, research suggests that digital technologies typically are used in ways which mimic and re-inscribe traditional assessment practices and simply increase the efficiency with which they can be administered (Broadfoot 2016). Additionally, the needs of 21st century employers, who are looking for employees with demonstrated ability in collaboration, creative problem-solving, analytical thinking, and the ability to learn, competencies not easily measured in traditional testing formats, as opposed to those who have simply demonstrated high levels of domain-specific knowledge, have changed (Forum 2020). Finally, assessment practices have significant effects on learners including anxiety about high-stakes testing and their prospects for future studies and employment (Harlen and Deakin Crick 2002), and also the approaches they take to learning activities (J. Biggs and Tang 2011; DeLuca, Coombs, et al. 2019). The confluence of these influences suggests that traditional approaches to assessment in HEI are becoming inadequate to meet the needs of modern HEI and ought to be reconsidered. 2.1 Topic of the Research A starting point for this process of examining assessment in higher education ought to be intentionally considered as there are myriad examples of formal and informal initiatives in higher education which have not realized significant local or systemic change (Broadfoot 2016; Earl 2013). Assessment practices are highly resistant to change, in part, as Broadfoot claims, because they are so important, but also, as will be discussed later, because the approaches that individual instructors take to assessing the work of learners are driven by complex forces (Black and Wiliam 1998; DeLuca, Coombs, et al. 2019; Stiggins 1991; Willis, Adie, and Klenowski 2013). Accordingly, it is incumbent upon researchers who wish to influence assessment practice to begin with a nuanced investigation of how HE instructors approach their assessment practice and what are the impacts of those practices on learners. 2.2 Problem to be Researched In her important article, Shepard (2000) argues that traditional assessment structures originated in past models of curriculum and instruction which were popular in the early 1900s. These curricular models emphasized the work of psychologists like Thorndike and Skinner who viewed the process of learning as being grounded in the mechanistic view of behaviourism where learning is the result of the precise and controlled input of ‘knowledge’ and reinforced with rewards for correct responses. As such, instructors (appropriately) designed their assessments to align with the curricular goals of the time and assessed learning by determining whether or not a learner could provide the single correct response to a given question at a time removed from the instruction. However, in the latter half of the 20th century, when western psychologists discovered the ideas of Lev Vygotsky (who was actually a contemporary of the previously mentioned psychologists), curricula began to take a more social-constructivist approach that emphasized higher-order thinking, problem-solving in social contexts, and metacognitive skills over rote memorization. Unfortunately, it seemed that the efficiencies of testing memory, recognition, and recall through selected-response tests were too deeply embedded in the practices of HE instructors who resisted changing their assessments to match the new curricular goals. Shepard argues for the need to integrate assessment and instruction in such a way as to engage learners in authentic performance tasks more suited to modern understandings of cognition. It appears now that, in the twenty years since Shepard wrote her paper, the goals of early 21st century curricula have continued to diverge from those of the 20th century with the World Economic Forum identifying competencies in collaboration, analytical thinking, creative problem-solving, and the continual learning as being priorities for 21st century employers (Forum 2020). Consequently, models of assessment which prioritize testing skills in a manner aligned with 20th century curricular models are no longer adequate because they no longer align with the priorities of modern HE (Crooks 1988). 2.3 Purpose of the Research Following previous research by DeLuca and colleagues (DeLuca et al. 2016; DeLuca, Rickey, and Coombs 2021; DeLuca, LaPointe-McEwan, and Luhanga 2016a) in the K-12 sector, and Massey et al (2020) in the higher education sector, the purpose of this research is to investigate current assessment literacies and practices among higher education instructors and the impacts of those approaches on learners. In order to better respond to actual assessment practices, it is critical to understand the conceptions of HE instructors with respect to assessment (DeLuca, Rickey, and Coombs 2021; Offerdahl and Tomanek 2011). Similarly, due to the significant influence assessment practices have on learners and learning, understanding the relationship between instructors’ assessment conceptions and practices and the experience of learners will be important in order to provide a foundation for moving into the remainder of the 21st century with assessment practices aligned with both pedagogical models and learner contexts. 2.4 Structure of the Paper This paper will begin with establishing a clear definition of assessment followed by an analysis of the literature related to the concept of assessment literacy (AL) (DeLuca, Coombs, et al. 2019) as it relates to the approaches to assessment (DeLuca, LaPointe-McEwan, and Luhanga 2016a) taken by HE instructors and the perceptions and experiences of HE learners. The last section will introduce a framework for understanding assessment in HE, situated within Biggs’ (1999; 1993) 3P model of teaching and learning, and research questions which emerge from the literature. 2.5 Defining Assessment There are deep and rich bodies of literature addressing educational assessment writ large, both from a summative, psychometric perspective, and from a formative perspective. There is notably more research in the K-12 context, especially in relation to teacher preparation, compared to HE. Among the more influential publications related to modern views of assessment (then usually called “evaluation”) was Scriven’s (1967) article in which he drew distinctions between “formative” and “summative” evaluation. Formative evaluation was described as evaluation for the purposes of improvement, and summative evaluation was seen as a validation of the quality of work at the end of a process. This distinction was quickly incorporated into Bloom’s (1968) ideas related to mastery learning and began to be promoted as a model for educational reform. However, by the late 1990s, when Black and Wiliam (1998) published their thorough review of the literature, the idea of formative assessment was still not well-defined or implemented. Black and Wiliam framed formative assessment as “encompassing all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the teaching and learning activities in which they are engaged” (1998, 7–8). Although Black and Wiliam came to very strongly-stated conclusions about the value of formative assessments (e.g. “The research reported here shows conclusively that formative assessment does improve learning. The gains in achievement appear to be quite considerable, and as noted earlier, amongst the largest ever reported for educational interventions.” (1998, 61)), reliance on summative assessments in HE has remained high Lipnevich et al. (2020). The National Research Council’s (NRC) 2001 report Knowing what students know, advanced understanding of assessment with their definition of assessment as “a process of drawing reasonable inferences about what students know on the basis of evidence derived from observations of what they say, do, or make in selected situations” (Pellegrino, Chudowsky, and Glaser 2001, 112) or, more simply, “reasoning from evidence” (Pellegrino, Chudowsky, and Glaser 2001, 43), based on Mislevy’s assertion that “test theory is machinery for reasoning from students’ behavior to conjectures about their competence, as framed in a particular conception of competence.” (1994, 4). Such a parsimonious description, however, may hide some of the complexities of fairly and equitably coming to know what learners know and can do in relation to particular outcomes. Since knowledge of a particular domain cannot be directly observed in a learner, and therefore cannot be quantified, instructors must rely on data gathered during the teaching process to support a particular inference about what a learner probably knows. The data gathered from performance tasks such as exams, essays, portfolios, labs, etc, become evidence when they support an inference about what a learner knows and can do. Hence, all summative assessments are probablistic, not deterministic. More recently, Earl (2013) further clarified the role assessment can play in learning by highlighting a distinction between assessment of learning (summative assessment) and assessment for learning (formative assessment by way of feedback) and also distinguishing both of those from assessment as learning (a subset of assessment for learning in which learners employ metacognitive skills to regulate their own learning tasks). Earl’s delineation between types of assessment reflects the modern view that assessment and learning are, or ought to be, tightly integrated. This relationship will be explored in more detail in relation to Biggs’ 3P Model of Teaching and Learning (1996, 1999). The definitions of assessment above are typically understood as being classroom assessment, language which is more readily applied to face-to-face K-12 learning environments as opposed to HE environments mediated by technology. For the purposes of this paper, I will consider assessment of learning and summative assessment to be essentially synonymous, and I will differentiate between assessment for and as learning. I will use the term classroom assessment to differentiate from large-scale assessment, understood to be assessments deployed at levels above individual classrooms, such as school-, system-, or provincial/federal-levels, and I will use online assessment or technology-mediated assessment to refer specifically to classroom assessment in learning environments mediated by technology whether the learners are remote or not. 2.6 Conceptions of Assessment Instructors in HE typically receive little formal preparation in either teaching practices or assessment during their graduate studies (Lipnevich et al. 2020; Massey, DeLuca, and LaPointe-McEwan 2020). Consequently, their own practice tends to follow from what they experienced as learners, which likely emphasized high-stakes summative tests which were either in alignment with outdated pedagogical practices or out of alignment with modern pedagogical practices. These prior conceptions of assessment carry significant weight in how HE instructors approach the assessment of learners in their own courses. Instructors bring to an educational environment a host of influences related to their institutional context, their past experiences with assessment, their own course policies, and affective beliefs or conceptions about assessment and its purposes (J. B. Biggs 1993; G. T. L. Brown, Lake, and Matters 2011). These influences play a significant role in determining the approaches taken by both instructors with respect to assessment and learners with respect to how they approach learning tasks (G. T. L. Brown, Lake, and Matters 2011). DeLuca et al. (2013) argue that there are categories of conceptions exhibited by K-12 preservice teachers: assessment as testing, assessment as format, assessment purpose, and assessment as process. These conceptions are seen as increasingly complex, with those who see assessment as testing believing that assessment is primarily concerned with summative assessment of learning, usually using teacher-created selected-response tests. Those who see assessment as format tend to focus on whether the assessment is a “performance, product, or objectively-scored assessment” (p. 110). Assessment as purpose is delineated according to the summative/formative binary or Earl’s (2013) assessment of/for/as learning model. DeLuca et al. also identify other purposes of assessment such as accountability, gatekeeping, and teacher evaluation. Lastly, assessment as process, which is based on the National Research Council’s description of assessment being a process of reasoning from evidence (2001). Fletcher et al. (2012) used Brown’s (2017) abridged Conceptions of Assessment (CoA) questionnaire to measure learners’ and instructors’ conceptions as follows: “assessment makes institutions accountable, assessment makes students accountable, assessment describes improvements in student abilities, assessment improves student learning, assessment improves teaching, assessment is valid, assessment is irrelevant and bad, assessment is irrelevant and ignored, and assessment is irrelevant and inaccurate” (p. 122). They report that instructors were more likely than learners to view assessment as consistent and trustworthy methods to understand and improve learning and that learners were more likely to have negative views of assessment and viewed it as a measure of student and institutional accountability. Massey et al. (Massey, DeLuca, and LaPointe-McEwan 2020) used DeLuca et al.’s (2013) framework of conceptions in their study of HE instructors’ conceptions of assessment before and after an instructional development course focussed on assessment. They also considered the idea that there are two general orientations towards assessment in HE, an “assessment culture” and a “testing culture” Massey, DeLuca, and LaPointe-McEwan (2020). They report that they saw significant shifts in participants’ conceptions of assessment from more simplistic views of assessment as testing pre-treatment, to more complex and nuanced views or assessment as process post-treatment. From the literature, it is clear that instructors’ conceptions of assessment are deeply influenced by many internal and external factors and, especially in HE, where there are few constraints on assessment practice (Lipnevich et al. 2020), there are many ways to describe or delineate different conceptions of assessment. Accordingly, there are multiple ways to conceptualize the skills and dispositions that comprise the idea of assessment literacy. 2.7 Assessment Literacy The idea of AL is relatively recent in the K-12 literature and is nascent and under-theorized with respect to HE contexts (Medland 2015). AL has been defined variously as “the skills and knowledge teachers require to measure and support student learning through assessment” (DeLuca, LaPointe-McEwan, and Luhanga 2016a), “a basic understanding of educational assessment and related skills to apply such knowledge to various measures of student achievement” (Xu and Brown 2016), “an individual’s understandings of the fundamental assessment concepts and procedures deemed likely to influence educational decisions” (Popham 2011) and “a dynamic context-dependent social practice that involves teachers articulating and negotiating classroom and cultural knowledges with one another and with learners, in the initiation, development and practice of assessment to achieve the learning goals of students” (Willis, Adie, and Klenowski 2013). Key to these definitions are the ideas that AL is a complex, multi-faceted construct, that AL requires adequate (not high) levels of psychometric or statistical analyses, and that it is intended to enable learner success. The recognition of AL as a critical competency for educators was influenced by the growing demands for teacher and school accountability in the post-WWII era in the USA and Canada, particularly the Elementary and Secondary Education Act (ESEA), passed in 1965, and the No Child Left Behind (NCLB) act, passed in 2002 (see DeLuca 2012 for a detailed discussion). As such, conceptualizations of AL have tended to be based on sets of standards to which K-12 teachers are obligated. The first set of standards was the Standards for Teacher Competence in Educational Assessment of Students (the Standards), published by a committee of representatives from the American Federation of Teachers, the National Council on Measurement in Education, and the National Education Association (AFT, NCME, and NEA 1990). The Standards are a list of seven skills expected of teachers: Teachers should be skilled in choosing assessment methods appropriate for instructional decisions. Teachers should be skilled in developing assessment methods appropriate for instructional decisions. The teacher should be skilled in administering, scoring, and interpreting the results of both externally-produced and teacher-produced assessment methods. Teachers should be skilled in using assessment results when making decisions about individual students, planning teaching, developing curriculum, and school improvement. Teachers should be skilled in developing valid pupil grading procedures which use pupil assessments. Teachers should be skilled in communicating assessment results to students, parents, other lay audiences, and other educators. Teachers should be skilled in recognizing unethical, illegal, and otherwise inappropriate assessment methods and uses of assessment information. Shortly after the publication of the Standards, the term assessment literacy appeared in the literature with Stiggins’ (1991) article called Assessment Literacy. Stiggins initial article was an account of his observation that teacher education programs at the time spent very little time training teachers in the methods and dispositions of educational measurement. Stiggins followed this with another article (Stiggins 1995) where he outlined five characteristics of sound assessments, which: arise from and serve clear purposes; arise from and reflect clear and appropriate achievement targets; rely on a proper assessment method, given the purpose and the target; sample student achievement appropriately; and control for all relevant sources of bias and distortion. (1995, 240) At around the same time, a group of Canadian educators published the Principles for Fair Student Assessment Practices for Education in Canada, Part A of which was a list of 37 guidelines related to five principles of fair student classroom assessment and was based on the 1990 Standards (see Appendix A). Part B was focused on externally-developed standardized tests. While these three sets of recommendations varied widely in their granularity, all tended to reflect an emphasis on the 20th century conceptions of curriculum which prioritized linear and sequential teaching of knowledge followed sometime later by selected-response tests of knowledge. This required teachers to be literate in the psychometric skills required to administer and interpret these tests (DeLuca, LaPointe-McEwan, and Luhanga 2016b; Shepard 2000; Xu and Brown 2016) or assessment of learning. One notable distinction between the sets of standards is that the Canadian committee specifically noted their principles could be applied to K-12 as well as higher education, although the latter context would require changes in how assessment data are reported. Twenty years following the publication of the Standards, Brookhart (2011) argued that the Standards had become outdated because they did not address either the growing practices and ideas of formative assessment (assessment for and as learning) or standards-based assessment and that they needed to be revised. Brookhart suggested a list of 11 skills (see Appendix A) to adjust the focus of the 1990 Standards to be in greater alignment with more modern conceptions of assessment. Finally, in 2015, the Joint Committee on Standards for Educational Evaluation (JCSEE), with key representatives from both Canada and the USA, published the most recent set of standards, called the Classroom Assessment Standards for PreK-12 Teachers (see Appendix A). The JCSEE standards are grouped into three broad domains (foundations, use, and quality), each with five or six related standards. Despite the similarities to the Principles for Fair Student Assessment Practices for Education in Canada, including at least one common committee member, the JCSEE Standards are specifically not intended for use in HE. As traditional conceptions of assessment and the standards expected of teachers, grounded in behaviourism and the need for objectivity tended to focus on assessment as a set of skill-based competencies to be employed by instructors, so AL could be defined as a set of sequential tasks in which instructors should engage to ensure objectivity and fairness (e.g. (Natriello 1987)). Recently, as curriculum and pedagogy have changed, several researchers have proposed models related to AL grounded in socio-constructivist views of learning (DeLuca 2012; Pastore and Andrade 2019; Xu and Brown 2016). DeLuca’s (2012) model, developed in the context of the No Child Left Behind accountability mandate in K-12 schools in the USA, is a coherent lens through which to understand how a pre-service teacher could develop assessment expertise throughout their teacher education program. DeLuca frames AL within Fostaty Young and Wilson’s (fostatyyoungAssessmentLearningICE2000?) ICE model which presents an integrated progression of learning through three levels of complexity, ideas, connections, and extensions. Underlying the three levels is a foundational level, where teacher candidates come to know and understand assessment as being situated within a model of teaching and learning, experiential and inclusive pedagogies, and an orientation towards continual professional learning. At the ideas level, teacher candidates gain expertise in the big ideas related to assessment (theories of learning, planning educational experiences, classroom assessment, and issues of diversity and inclusion). At the connections level, teacher candidates begin to construct their own cognitive conceptions of assessment as they integrate their own past experiences with their new knowledge and add their experience in practica. At the extensions level, teacher candidates begin to hone their practice as they deepen their understanding of assessment. Willis et al. (2013) describe assessment literacy in alignment with Bernstien’s (1999) idea that there are “horizontal” and “vertical” discourses (p. 159) with respect to assessment. A horizontal discourse is the local, contextualized discourse around assessment which influences local practice, while a vertical discourse is the formalized, structured discourse on assessment in the literature and other more authoritative venues. They argue that any given instructor’s conceptions and practice of assessment will be informed by and negotiated within multiple horizontal and vertical discourses. Similarly, Xu and Brown’s (2016) model, teacher AL in practise (TALiP) presents a pathway for pre-service teachers to gain expertise in assessment. Based on a synthesis of 100 peer-reviewed publications between 1985 and 2015, they present a six-component model: the knowledge base teacher conceptions of assessment, institutional and socio-cultural contexts, teacher assessment literacy in practice, teacher learning, and teacher identity as assessor. Finally, Pastore and Andrade (2019) developed their model through a Delphi inquiry of 35 international experts in educational assessment and teacher education. They propose a model with three dimensions, conceptual, praxeological, and socio-emotional. The authors of each of these three models recognize that AL is conceptualized as a multi-dimensional construct encompassing psychometric skills, affective beliefs and values, external and regulatory environments, and socially negotiated practices. However, these models are largely specific to the K-12 environment in general, and more specifically related to the preparation of K-12 teachers. Given that there are very few parallels between the preparation of K-12 teachers for their role and the preparation of HE instructors for theirs, it seems that a framework for understanding AL among HE instructors should consider their general lack of formal preparation for teaching or assessing learning (Lipnevich et al. 2020; Massey, DeLuca, and LaPointe-McEwan 2020). Bearman et al. (2016) proposed a model for assessment decision-making in Australian HE, the Assessment Design Decisions Framwork (ADDF) in which the authors acknowledge the difficulty in translating idealized beliefs about assessment into actual practice as well as the lack of literature regarding how HE instructors go about designing assessments. While not specifically an AL framework, there are overlaps in terms of the dimensions they identified. They argue, like those previously mentioned, that assessment is a complex and messy process. Unique to their model is the idea that there is often a disconnect between what instructors know or believe to be true about assessment, and how instructors’ practice is impacted (or not) by their beliefs. Additionally, they argue, in alignment with Price et al. (2011), that AL ought to be considered from both instructor and learner perspectives. Their framework is comprised of six dimensions: purposes of assessments contexts of assessments learner outcomes tasks feedback processes interactions Herppich et al. (2018) frame a model of assessment competence beginning with the idea that the purpose of educational assessment is to inform both formative (ongoing learning) and summative (credentialling or certification) decisions locating the educational decision subsequent and subordinate to the judgement. They use an example where a teacher observes a learner struggling with a test and might come to an appropriate judgement of a learner’s knowledge, but make an inappropriate instructional decision based on that information. This distinction is useful when, like Herppich and colleagues, the construct under investigation is assessment competence, which is observed after instructional activities. However, in the present paper, conceptions of assessment and AL are the relevant constructs and they are embedded in a model of teaching and learning where conceptions of assessment precede instructional activities. DeLuca et al.’s model (2016a), approaches to classroom assessment defines and conceptualizes the assessment component of Biggs’ 3P model (1999; 1993) and so aligns well with a concise model of teaching and learning in higher education. The Approaches to Classroom Assessment model is based on the JCSEE standards (Klinger et al. 2015) and describes four themes of AL, each with three dimensions. The model represents somewhat of a break from previous models in that it references approaches to assessment rather than assessment literacies. This is a reflection of the authors’ view that language around literacies and competencies may indicate a reliance on “correct” views or methods rather than the complex array of influences that lead to multiple legitimate approaches as identified in the literature (DeLuca, Coombs, et al. 2019; Willis, Adie, and Klenowski 2013). The themes DeLuca et al (2021, 10) describe along with their associated dimensions are listed below and illustrated in figure 2: Assessment purposes. Assessment of learning Teachers’ use of evidence to summate student learning and assign a grade in relation to students’ achievement of learning objectives Assessment for learning Teachers’ and students’ use of evidence to provide feedback on progress towards learning objectives (i.e., inform next steps for learning and instructions). Involves both teacher-directed and student-centred approaches to formative assessment. Assessment as learning Focuses on how the student is learning by providing feedback or experiences that foster students’ metacognitive abilities and learning skills (e.g., self-assessment, goal-setting, learning plans). Involves teachers but is primarily student-centred. Assessment process Design Focuses on the development of reliable assessments and items that measure student learning in relation to learning objectives. Use/scoring Focuses of the adjustment and use of scoring protocols and grading schemes to respond to assessment scenarios. Communication focuses on the interpretation of assessment results and feedback through communication to students and parents. Assessment fairness Standard Maintains the equal assessment protocols for all students. Equitable Differentiates assessment protocols for formally identified students (i.e., special education or English language learners) Differentiated Individualizes learning opportunities and assessments that address each student’s unique learning needs and goals. Assessment theory Consistent Works to ensure consistency in results within assessments, across time periods, and between teachers. Contextual Works to ensure assessment or evaluation measures what it claims to measure (i.e., learning objectives) and promote valid interpretations of results. Balanced Works to ensure consistency in measuring what an assessment or evaluation intends to measure, and degree to which an assessment or evaluation measures what it claims to measure. Figure 2. Approaches to Classroom Assessment. (DeLuca, Rickey, and Coombs 2021, 10) 2.8 Assessment and Technology Educational technologies are often viewed and reported on with a distinct positivity bias (Irvine 2020) wherein ‘new’ technologies are presumed to represent progress and will inevitably have a positive effect on learning. This can be seen in the titles given to some initiatives, such as “Technology-Enhanced Assessment” (Oldfield et al. 2012; Timmis et al. 2016), “IT-enabled assessment” (Webb and Ifenthaler 2018), or “technology-rich” (Lin et al. 2020). As such, I will use the more neutral term “technology-mediated” to indicate that adding digital technology to an assessment environment does not necessarily improve that environment. Similar to assessment practices being grounded in (both philosophically, as in ‘based upon,’ and figuratively, as in ‘stuck in’) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment, so too, many educational technologies are grounded in (based upon and stuck in) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment. This can be seen in the progressively more advanced technologies beginning with Pressey’s teaching machines (Benjamin 1988; Pressey 1927; A. Watters 2021) which was built to automate the process of “drilling” learners in an effort to teach them some concept. The machine needed to be pre-programmed with a series of selected-response questions along with distractors and correct answers. As a learner answered each question, the machine was programmed to match the response to the programmed correct response, and if it matched, the learner was determined to have “mastered” that question and it was dropped from the bank of questions the learner had not yet mastered. If it did not match, the question was cycled back into the bank to be repeated. Clearly, this technology was promoted as a tool to be used to modernize and increase the efficiency of tasks that aligned with the dominant pedagigical paradigm at the time. A second example, although not one marketed directly to schools, but to parents, was the Speak &amp; Spell, released in 1978 by Texas Instruments (Braguinski 2018; Frantz 2014), which represented an advance in technology and an increase in efficiency, as the Speak &amp; Spell could be programmed to store and reproduce voice recordings of words as well as multiple recordings of feedback messages (Frantz 2014). While the Speak &amp; Spell was a leap forward in processing power, memory storage, and therefore complexity, the underlying pedagogy remained identical to that of Pressey’s teaching machine (Audrey Watters 2015). Moving forward again, and modern technologies are vastly more powerful than teaching machines or the Speak &amp; Spell and power very complex adaptive tests, such as the NCLEX-RN, the national licensing exam for Registered Nurses in Canada and the USA (Smith Glasgow, Dreher, and Schreiber 2019). These advances in both hardware and software allow for still greater efficiencies in testing, yet the NCLEX-RN must still be programmed with selected-response questions, their distractors, and correct responses, all still in alignment with behavioural models of pedagogy. The NCLEX-RN is an example of a large-scale, standardized assessment (LSA), so is not parallel to the classroom assessment practices which are the subject of this paper, but I mention it here to draw the distinction between professionally-created LSAs and most instructor-created classroom assessments. The NCLEX-RN is continually revised and updated and reflects very robust psychometric properties (validity and reliability) (Smith Glasgow, Dreher, and Schreiber 2019). More importantly, however, the NCLEX-RN has been updated to “shift away from a primary focus on content and the indirect testing of clinical judgment to a major focus on clinical judgment” (Caputi 2019, 2). Caputi, in her reflections on the next-generation NCLEX-RN (NGN) asks 2 questions (the second of which is most relevant here): “1) Are students ready for this type of NCLEX? 2) If our students already pass the NCLEX, can we keep doing the same type of preparation for the NGN?” (p. 2). Her answer to both questions is “No.” She goes on to argue, So, what can faculty do? I propose that nurse faculty, at all levels of nursing education, revise their curricula to teach a detailed thinking process that students must employ over and over throughout the nursing curriculum. Just as students practice psychomotor skills until they are perfected, they must do so with thinking skills and strategies A new model for teaching clinical judgment is needed\", (p. 2, emphasis in original). In this case, the technology-mediated assessment instrument has been designed to measure 21st century skills, and Caputi recognizes that if pre-service nurses are going to have to pass the NGN, nursing instructors will need to realign their pedagogy. This example illustrates how LSAs exert pressure on instructors and schools to adjust their pedagogy, in the case of nursing, to encourage 21st century pedagogy that teaches thinking skills. The opposite is also true, however, in that LSAs which emphasize the lower-level cognitive skills prized by the behaviourists cause instructors to match their pedagogy to that model (Caputi 2019; Clarke-Midura and Dede 2010; DeLuca, Rickey, and Coombs 2021; Pellegrino and Quellmalz 2010). Further, the American Educational Research Association (AERA), the National Council on Measurement in Education (NCME), and the American Psychological Association (APA) argue that LSAs tend to have other negative effects on education systems, namely the narrowing of curricula (teaching to the test), reduced instructional strategies (previously mentioned), higher dropout rates, and the enactment of policies and practices that increase test scores without increasing learning (2014). The NCLEX-RN notwithstanding, many implementations of technology in assessment remain focused on increasing the efficiencies of summative test administration (Broadfoot 2016; Pellegrino and Quellmalz 2010; Webb and Ifenthaler 2018). 2.9 Impact on Learners Aside from the effects on learners’ approaches to their learning, assessment, particularly summative assessment, has profound effects in other ways, such as determining status or progression in a course or program, determining eligibility for scholarships and awards, determining career paths and on other affective constructs such as motivation (Crooks 1988) and anxiety (Menucha Birenbaum 2007; Harlen and Deakin Crick 2002). Jones et al. (2021) report that assessment practices impact learners’ well-being, which they define as including physical and mental health as well as the ability for learners to “fully exercise their cognitive, emotional, physical and social powers, leading to flourishing” (p. 439). They note that summative assessment practices are associated in the literature with “anxiety, depression, disordered eating, self-harm, panic attacks, burnout, … thoughts of suicide … disordered sleep, loss of appetite, physical inactivity, poor physical health, … substance misuse … poorer productivity, motivation and test scores” and that changing from norm-referenced scoring (where learners are ranked relative to each other) to criterion-referenced or pass-fail scoring is associated with lower levels of stress and anxiety among medical students in the USA. 2.10 Theoretical Framework Overview Given the wide variety of models, mostly in K-12 contexts but increasingly in HE, and the degree to which there appears to be overlap between those models, I take the position that introducing a new model into an already crowded landscape would serve little purpose. Furthermore, models do exist which have been validated empirically (DeLuca, LaPointe-McEwan, and Luhanga 2016a) and the authors of which have indicated that, despite the model being developed for K-12 contexts, adjusting the model to fit the HE context would be a worthwhile contribution to the literature (personal communication). Here, I present an argument that DeLuca et al.’s idea of approaches to classroom assessment (2016a) aligns well with the landscape of teaching, learning, and assessment in HE and may provide a blueprint for assessment reform in HE. In order to situate Approaches to Classroom Assessment within the context of teaching and learning in HE, I begin with Biggs’ (J. Biggs 1999; J. B. Biggs 1993) 3P model of teaching and learning (see Figure 3) which presents three stages of the teaching and learning process. In the first stage, presage, Biggs models both learners and the teaching context. The learners’ contexts include their previous learning, cognitive abilities, and their beliefs about the learning context, and all of these influence their preferred approaches to learning. The teaching context includes the instructors beliefs and conceptions of the curriculum and assessment, and also institutional and sometimes regulatory requirements, and each of these influence the instructor’s approach to teaching, and more to the point for this paper, their approach to assessment. The process stage represents the activities in which learners engage in order to meet the learning outcomes. Biggs notes that learners tend to either take surface approaches, where they use low-level cognitive skills (memorization of facts) in activities that require high-level cognitive skills (analysis or critique), or they take deep approaches, where they use high-level cognitive skills for activities which require them. Learner activity in this stage is influenced by their own backgrounds, abilities, and affective views of the purpose of the task and it is influenced by the choices the instructor makes based on their presage influences. In this way, Earl’s (2013) delineation between assessment of, for, and as learning can be seen to influence student learning approaches. For example, researchers note that when instructors’ approach prioritizes assessment of learning, learners tend to take a surface approach to their learning (Menucha Birenbaum 2007). Finally, the product stage represents the learners’ achievement in relation to the outcomes. Moving left to right, each of the first two stages represents an input into the system, and the third, an output. However, as the arrows between the components are two-way, the product stage also becomes an input, the results of which are fed back into the system (right to left) informing learners of their status in relation to the outcomes as well as the utility of their learning activities. It also informs the instructor about the nature of the learners’ achievement and the utility of the learning tasks. The achievement component provides formative feedback directly to both instructors and learners, informing both about next steps in the learning process. It also provides insight into the learning activities and, when that is fed back to learners, provides metacognitive cues as to the strategies learners might use in future tasks. It also provides information to instructors about the nature of the learning activities and how their approaches to assessment are impacting learning. Figure 3. 3P Model of Teaching and Learning. (J. Biggs 1999; J. B. Biggs 1993) 2.11 Summary and Research Questions Assessment in HE is a tremendously complex enterprise with myriad forces influencing the approaches instructors take in their assessment practice, which, in turn, becomes one of the myriad forces influencing the approaches learners take in their learning activities. Instructors conceptions of assessment are often derived from their past experiences as learners who were the objects of assessment, leading to an over-reliance on summative assessment strategies which may not obtain sufficient levels of reliability or validity. Increasing assessment literacy among HE instructors is a necessary, but not likely sufficient, step in advancing fair and balanced assessment practices in an increasingly technology-mediated HE landscape. These concepts lead to the following research questions: Are there distinct patterns in higher education instructors’ approaches to assessment in Canada? Does the prevalence of these patterns differ by: instructors’ levels of experience in teaching face-to-face versus online? instructors’ levels of experience using technology? What factors influence instructors’ approaches to assessment? How do different assessment strategies affect learners’ experiences? 2.12 Significance of the Research This research will form the basis for an ongoing research agenda focused on research-supported strategies for assessment reform in Canadian HEI. Understanding both how instructors think about assessment and how learners are impacted by assessment decisions will be critical to informing assessment practices and policies as higher education emerges from the pandemic and moves forward into the 21st century. References "],["candidacy-question-2.html", " 3 Candidacy Question 2 3.1 Overview of the Problem, Purpose, and Questions 3.2 Methodological Approaches 3.3 Visualization of a Mixed Research Project 3.4 Participants and Sampling Strategy 3.5 Data Collection Strategy 3.6 Types of Data and Analysis 3.7 Research Procedures 3.8 Chapter Summary", " 3 Candidacy Question 2 3.0.1 A Methodology for Investigating Assessment Approaches and their Impact on Learners Colin Madland Department of Curriculum and Instruction, Faculty of Education, University of Victoria EDCI 693: Candidacy Examination - Curriculum and Instruction Dr. Valerie Irvine (Supervisor), Dr. Christopher DeLuca, and Dr. Okan Bulut Monday, August 9, 2021 Describe a methodological approach to investigate higher education instructors’ assessment literacies, practices, and the impacts of those on learners. Consider and justify your methodological decisions based on extant literature. Explore how the underpinning epistemology aligns with the purpose of the investigation. Consider the practical methods as based on previous research (where relevant) that could be used to pursue such an investigation, explaining the type of data you would collect, potential tools to collect data, and how you would use the data to address your research question(s). 3.1 Overview of the Problem, Purpose, and Questions Research suggests that higher education faculty receive little formal preparation for assessing learning (Lipnevich et al. 2020) and that there may be misalignment between modern pedagogical practices and the approaches to assessment taken by higher education instructors (Knight 2002; Massey, DeLuca, and LaPointe-McEwan 2020; Shepard 2000). The purpose of this paper will be to outline a methodological approach to investigating higher education instructors’ assessment literacies and practices as well as the impact of those practices on learners. Assessment literacy is the result of a complex interplay of skills, beliefs, and values about four domains of assessment practice: (1) the purposes of assessment; (2) the processes associated with designing, administering, scoring, and communicating the results of assessment tasks; (3) fairness in assessment practices; and (4) assessment theory, each with three priority areas. (DeLuca, Coombs, et al. 2019). Specific research questions to be addressed are: Are there distinct patterns in higher education instructors’ approaches to assessment in Canada? Does the prevalence of these patterns differ by: instructors’ levels of experience in teaching face-to-face versus online? instructors’ levels of experience using technology? What factors influence instructors’ approaches to assessment? How do instructors’ assessment approaches affect learners’ experiences? In reviewing the extant literature concerning these topics and questions, and how researchers approach them, I searched the Educational Resource Information Center (ERIC), Google Scholar, and the University of Victoria Library Summon 2.0 databases. I also searched specific journals, such as Frontiers in Education, International Review of Research in Open and Distributed Learning, the Journal of Mixed Methods Research, Educational Measurement: Issues and Practice, and Assessment and Evaluation in Higher Education. Search terms included “assessment literacy,” “assessment,” “online,” “higher education,” “learning,” and “learner impact” in various combinations. I also used researchrabbit.ai to assist with tracking citations of seminal and other important articles and for visualizing the relationships between found articles. 3.2 Methodological Approaches All research is grounded in a particular tradition or worldview, often called a paradigm. A research paradigm is a set of philosophical assumptions and values about the universe and the ways in which we can learn and know about it (Johnson and Christensen 2017). According to Johnson and Christenson (2017), research paradigms are generally concerned with the answers to questions related to: - how we learn about something (methodology), - how we know something is true (epistemology) - how we know reality, or if something exists (ontology), - how we know if something is valuable or ethical (axiology), and; - how we communicate and present arguments (rhetoric). The three primary research paradigms are quantitative, qualitative, and mixed research (often called mixed methods research). Quantitative researchers traditionally followed the epistemological position of positivism, characterized by the idea that knowledge is singular, objective, and can be discovered through the empirical analysis of numerical data (Held 2019), although most modern quantitative researchers take a post-positivist view which recognizes that the values and beliefs of the researcher play a role in research and challenge the idea of pure objectivity (Tashakkori, Johnson, and Teddlie 2020). Quantitative research is often confirmatory in nature, with the researcher gathering data to either confirm or disconfirm a specific hypothesis (Johnson and Christensen 2017). Qualitative researchers, on the other hand, tend to take the epistemological view of constructivism which posits that meaning is constructed by the researcher as they observe individuals and groups in relation to each other and particular phenomena. Qualitative research is exploratory in nature and uses non-numerical data such as texts, transcripts, images, and categories (Johnson and Christensen 2017) in response to open-ended questions (Tashakkori, Johnson, and Teddlie 2020). During the latter half of the 20th century, when qualitative approaches were becoming more popular the research community tended towards a dualistic, ‘either/or’ view of these two paradigms (Niglas 2010; Tashakkori, Johnson, and Teddlie 2020) to the extent that the two paradigms were seen to be fundamentally incompatible. For example, Guba (1990, 81 as quoted in; Johnson and Onwuegbuzie 2004, 14) emphasized “accommodation between paradigms is impossible … we are led to vastly diverse, disparate, and totally antithetical ends.” Early in the 21st century, however, Johnson and Onwuegbuzie countered that argument and proposed that paradigms are better visualized on a continuum, that both paradigms are useful, and that mixing methods can “draw from the strengths and minimize the weaknesses of both in single research studies and across studies” (2004, 13–14). So mixed research is characterized by an epistemologically pragmatic view that both quantitative and qualitative approaches are valuable and that combining numerical and non-numerical data, analyses, and results can lead to deeper understanding (Bazeley 2018; Tashakkori, Johnson, and Teddlie 2020). More specifically, Johnson, et al. (2007) provide the following definition of mixed research: Mixed methods research is the type of research in which a researcher or team of researchers combines elements of qualitative and quantitative research approaches (e.g., use of qualitative and quantitative viewpoints, data collection, analysis, inference techniques) for the broad purposes of breadth and depth of understanding and corroboration. (p. 123) I will use the term mixed research in alignment with Johnson and Christensen (2017) who advocate for that term rather than mixed methods research because it is more than the method of research that is mixed. Indeed, mixing occurs at the level of paradigms, questions, methods, analyses, and reporting of results (Bazeley 2018). Niglas (2010) contends that while the epistemological and philosophical stances of the researcher are important in informing the method of research, the more important considerations are the problem, purpose and questions the researcher intends to investigate. Similarly, Bazeley (2018) argues that the most important factor that researchers should consider when choosing a methodology is “whether the methods chosen and the strategies used … serve the purpose of the research” (p. 8, emphasis in original). Following Bazeley’s argument, and considering the purpose of this research project, a mixed approach is justified. Further justification is indicated in the diversity of approaches taken by researchers investigating assessment literacy or the impact of assessment on learners using quantitative (DeLuca et al. 2016; DeLuca, Rickey, and Coombs 2021; Massey, DeLuca, and LaPointe-McEwan 2020; Nayagi N and Rajendran 2020; Pereira et al. 2021), qualitative (Boud and Dawson 2021; Coombs, Ge, and DeLuca 2020; DeLuca et al. 2021; Earle 2020; Fives and Barnes 2020; Medland 2019; Watson et al. 2017), and mixed (DeLuca et al. 2018; Nicholson 2018; Iannone, Czichowsky, and Ruf 2020; Tekir 2021) approaches. 3.2.1 Quantitative approaches The construct of assessment literacy has a history of being grounded in sets of standards published by researchers, governments, or regulatory agencies and of being assessed through the collection and analysis of numeric and categorical data (DeLuca et al. 2016; Gotch and French 2014). Gotch and French (2014), in their systematic review of the literature on assessment literacy inventories, found 36 instruments, all but one of which (Jarr 2012) gathered only quantitative (numeric and categorical) data. Studies based on these instruments (for example, Alkharusi, et al. (2012) based on Plake, et al. (2005)) generally included quantitative analyses because the data gathered are quantitative. Early instruments (Mertler and Campbell 2005; Barbara S. Plake 1993) were based on the 1990 Standards for Teacher Competency in Educational Assessment of Students (AFT, NCME, and NEA 1990). Since then, Brookhart (2011) published a critique of the 1990 standards arguing that they were out of date and not in alignment with modern pedagogy. This was followed a few years later by Gotch and French (2014) publishing the aforementioned systematic review, during which they found that the psychometric properties of the 36 instruments published between 1991 and 2012 were low. Finally, Klinger et al. (2015) published the Classroom Assessment Standards for PreK-12 Teachers (Klinger et al. 2015). These cumulative advances led to DeLuca et al. (2016b) to develop the Approaches to Classroom Assessment Inventory (ACAI) and enabled researchers to base their quantitative investigations on an instrument shown to produce valid results in relation to a modern model of assessment literacy. There are numerous examples of research teams using quantitative approaches to investigate assessment literacy, some of which are mentioned here. DeLuca et al. (2016) surveyed 404 K-12 teachers in Canada and the USA using DeLuca et al.’s (2016a) Approaches to Classroom Assessment Inventory (ACAI), an instrument based on the 2015 Standards. DeLuca et al. (2021), recently used the same inventory in an international study comparing assessment practices in the USA, Canada, and China. Nayagi and Rajendran (2020) also used the ACAI to investigate approaches to assessment, although they modified it for their context of pre-service teachers in India. Massey et al. (2020) surveyed higher education instructors to investigate their approaches to and confidence with assessment. Lastly, with respect to the impact of instructors’ assessment practices on learners, Pereira et al. (2021), Pereira et al. (2017), and Flores et al. (2015) used surveys to collect quantitative data about the perceptions of Portuguese undergraduate learners regarding the assessment practices of their instructors. While there are more studies of assessment literacy in the K-12 sector (Medland 2019), there is a growing number in the higher education sector (e.g. (Massey, DeLuca, and LaPointe-McEwan 2020; Nayagi N and Rajendran 2020; Pereira et al. 2021)). Research questions 1 and 2 in the present study are intended to answer questions about the assessment literacies of higher education instructors in Canada, a construct which has been shown to be amenable to quantitative survey approaches in diverse settings, including K-12 teachers, pre-service teachers, and higher education instructors both in Canada and internationally. 3.2.2 Qualitative approaches Assessment literacy has been the focus of significant attention in quantitative research, but it is also becoming clear that the construct is much deeper and more complex than models based on sets of standards and competencies (DeLuca, Willis, et al. 2019, 2019; Willis, Adie, and Klenowski 2013; Xu and Brown 2016). Willis (2013) framed assessment literacy within multiple “horizontal and vertical structures of educational discourse” following Bernstein (1999), who described vertical discourses as structured disciplinary knowledge passed down from specialists, and horizontal discourses as more contextualized knowledge practiced and passed along at a local level. Following on this idea, DeLuca et al. (2019) describe assessment literacy as being a “situated and differential practice predicated on negotiated knowledges” (p. 7) and elsewhere, DeLuca et al. (2019) argue that “Assessment capability involves situated professional judgement, that is the ability to draw on learning and assessment theories and experiences to purposefully design, interpret, and use a range of assessment evidence in the service of student learning” (p. 2). Similarly, Xu and Brown’s (2016) model, Teacher Assessment Literacy in Practice (TALiP), explicitly includes socio-cultural factors, teacher judgement, and teacher identity. The methodological implication of this shift to a more complex understanding of assessment literacy is that there seems to have been an attendant shift in researchers engaging in qualitative investigations of assessment literacy (Bearman et al. 2017; DeLuca et al. 2021; Fives and Barnes 2020; Medland 2019; Watson et al. 2017). Qualitative methodologies exist because of the view that, according to Guba and Lincoln (1989), reality is constructed through social processes, and is not simply observed in the world. In my view, this epistemological stance within the community of qualitative researchers aligns well with the nature of assessment literacy as a phenomenon impacted by horizontal discourses. In my own review of the literature on assessment literacy, there seems to be a marked increase in the number of qualitative studies published since 2016, which may reflect a deepening understanding of the characteristics of assessment literacy. While the trend to use qualitative approaches to investigate assessment literacy and the impact of assessment on learners is more recent compared to quantitative approaches, there are good examples of qualitative studies, some of which I briefly describe here. Watson et al. (2017) published a case study of how instructor assessment practices impacted a single learner who struggled through a graduate-level learning module. Benediktsson and Ragnarsdóttir (2020) used focus group interviews followed by semi-structured interviews with individual learners to investigate the assessment experiences of immigrant learners in Icelandic universities. Bearman et al. (2017) interviewed thirty-three higher education instructors from a variety of disciplines and institutions to investigate how they designed assessments and found that there is a need for professional development activities about assessment to have a relational focus. In the UK context of having external peers provide regulatory feedback, Medland (2019) used an exploratory case study approach to investigate the assessment literacy of external examiners in UK higher education. In another case study with a single subject, Fives and Barnes (2020), recognizing the complexity of classroom assessment, investigated the assessment-related routines of an experienced teacher. Their goal was to engage in a close examination of how an experienced teacher enacted assessment practices in their classroom in order to “make explicit [the] cognitive tasks involved in their work” (p. 3). More recently, DeLuca et al. ((2021)) used a case study methodology to analyze data gathered from interviews, reflections, and course work to investigate the learning trajectories of 35 pre-service teachers during their program. Each of these studies was exploratory in nature, and/or designed specifically to uncover deeper meanings than could be discerned with the use of a survey or other quantitative instrument, which would require the researcher to anticipate and direct respondents toward a limited set of possible answers. Research questions 3 and 4 of the present study are intended to explore the qualitative and subjective experiences of instructors, what factors influence their approaches to assessment and the experiences of learners as they engage in assessment activities. Both of these questions will require responses to open-ended questions and will be exploratory in nature, aligning with a qualitative paradigm and epistemology. 3.2.3 Mixed approaches Researchers have argued that using mixed approaches to research is a way to strengthen and increase confidence in the validity of results by maximizing the strengths of each paradigm while minimizing the weaknesses (Creswell 2009; McKim 2017). Creamer (2018) argues that mixing of qualitative and quantitative methods is the defining characteristic of mixed research. She defines mixing as “the linking, merging, or embedding of qualitative and quantitative strands of a mixed methods study” (p. 6, emphasis in original). In a departure from the historical paradigm wars between quantitative and qualitative approaches, Niglas (2010) contends for the idea that mixed research studies fall on a continuum between qualitative and quantitative approaches with varying degrees of epistemological stances and methods. Given the abundance of literature exploring instructor assessment literacy and practices and the impact of those practices on learners from both quantitative and qualitative paradigms, it is not surprising that there is also a body of literature investigating these constructs using mixed research approaches (Esfandiari et al. 2016; Ogan‐Bekiroglu and Suzuk 2014; Solomonidou and Michaelides 2017). Creamer (2018) suggests evaluating mixed research projects using a table to make the structure of the study more plainly visible. The following tables are overviews of selected mixed research projects investigating assessment literacy in K-12 and higher education. Table 1 Overview of “Students’ conceptions of assessment purposes in a low stakes secondary-school context: A mixed methodology approach”* (Solomonidou and Michaelides 2017)* Study Component Details Yes/No Comments Rationale/Purpose for Mixing Enhancement Priority QUAN → qual Timing of Data Collection Sequential Timing of Data Analysis Sequential Mixing Design Yes Qualitative data were used to confirm tentative inferences from quantitative analysis. Data Collection Yes Semi-structured interview questions were revised in light of quantitative inferences, and participants could view their survey answers during their interviews. Data Analysis Yes Blended in the report. Inferences Yes Blended in the report. Fully Integrated Yes Qualitative Inference Learners value assessment information but it takes work to use the information formatively. - - Quantitative Inference Learners agreed more with the idea that assessment is a tool for improvement and less with the idea that assessment is negative. Meta-inference Teacher education and development should systematically include alternative assessment practices. - Value Added Linking data from both surveys and interviews provided opportunity for learners to elaborate on their views and provide a richer picture of their conceptions of assessment. Table 2 Overview of “A Mixed-methods, Cross-sectional Study of Assessment Literacy of Iranian University Instructors: Implications for Teachers’ Professional Development”* (Esfandiari et al. 2016)* Study Component Details Yes/No Comments Rationale/Purpose for Mixing Complementarity Priority QUAN → qual Timing of Data Collection Sequential Timing of Data Analysis Sequential Mixing Design Yes The qualitative phase was used to elaborate on the quantiative phase. Data Collection No Data Analysis Yes Qualitative analysis filled gaps in quantitative inferences. Inferences Yes Blended in the report. Fully Integrated No Qualitative Inference Subgroups of participants assessed learners differently, used assessment for different purposes, had different levels of training in assessment, and different views on the psychometric properties of tests. Quantitative Inference Results showed support for a three-component model of assessment literacy. Meta-inference Assessment literacy is a multi-dimensional concept affected my many factors, including teaching experience, professional development and training, and the local teaching context. Value Added Qualitative analysis was critical to helping the researchers understand the differences seen between subgroups in the quantitative analysis. Table 3 Overview of “Pre-service teachers’ assessment literacy and its implementation into practice” (Ogan‐Bekiroglu and Suzuk 2014) Study Component Details Yes/No Comments Rationale/Purpose for Mixing Triangulation and Enhancement Priority QUAN → qual+qual Timing of Data Collection Sequential Timing of Data Analysis Side-by-side comparison (concurrent) Mixing Design Yes The qualitative phase was used to validate and elaborate on the quantitative phase. Data Collection No Data Analysis Yes Qualitative and quantitative data were analyzed concurrently by presenting them side-by-side for comparison. Inferences Yes Blended in the report. Fully Integrated No Qualitative Inference Qualitative analysis showed alignment with quantitative analysis Quantitative Inference Pre-service teachers’ assessment literacy was determined to be “close to constructivist” (p. 352) Meta-inference Although participants were able to demonstrate knowledge of assessment literacy, they had difficulty translating that knowledge into practice. Value Added Qualitative analysis was important for determining the difficulty in translating knowledge of assessment literacy into practice of good assessment. Table 4 Overview of “A mixed method exploration of student perceptions of assessment in nursing and biomedicine” (Garvey, Hodgson, and Tighe 2021) Study Component Details Yes/No Comments Rationale/Purpose for Mixing Triangulation Priority QUAN+QUAL Timing of Data Collection Concurrent Timing of Data Analysis Consecutive Mixing Design Yes The qualitative phase was used to validate and elaborate on the quantitative phase. Data Collection Yes Open-ended (qualitative) responses were gathered along with quantitative responses and were designed to elicit clarifying views. Data Analysis No Inferences Yes Blended in the report. Fully Integrated No Qualitative Inference Qualitative analysis deeper levels of complexity compared to quantitative analysis. Quantitative Inference First-year university learners indicated positive views of assessment, regardless of their program. Meta-inference Universities should recognize that first-year learners tend to have positive views of assessment and should communicate often with them about the purposes of assessment. Value Added Qualitative analysis showed areas of misalignment with the quantitative analysis. Each of the preceding four studies shows different ways in which mixed research can enhance validity or provide avenues for deeper and more complex understanding of instructors’ assessment literacy and practices and the impact of those on learners. The tables illustrate the complexity of mixed studies and highlight the challenges associated with integrating quantitative and qualitative approaches at each stage of the research. However, even those studies which do not demonstrate all of the qualities of fully integrated mixed research contain valuable insights based on the integration of the two analyses. 3.2.4 Epistemological Alignment with Purpose of the Research The purpose of this project is to investigate a multi-dimensional construct, assessment literacy, that is influenced by factors both internal (for example: beliefs, values, and past experiences) and external (for example: assessment culture (DeLuca, Rickey, and Coombs 2021; Massey, DeLuca, and LaPointe-McEwan 2020) or government regulations) to instructors (DeLuca, Coombs, et al. 2019). In addition, the project will investigate the factors that influence instructors in higher education to take particular approaches to their assessment practice and how those practices impact learners. In envisioning this multi-dimensional research project and formulating the research questions, I recognize value in both the discovery and the creation of meaning in the research process, a pragmatic epistemology that aligns with the ethos of many mixed researchers. In alignment with the examples from the literature described in the preceding sections, this project will draw from both quantitative and qualitative paradigms, will use both quantitative and qualitative data connected in multiple ways, engage in analyses in alignment with both traditions, and will present results as inferences from both quantitative and qualitative analyses in addition to meta-inferences (Bazeley 2018, 62) from the integration of analyses. As such, I propose a mixed research investigation, which will be described in greater detail in the following sections. 3.3 Visualization of a Mixed Research Project Researchers who use mixed approaches often include visual overviews of their research project as mixed research projects can be very complex (Creamer 2018). Figure 1 below is a visual representation of the present project. It is helpful to understand some of the conventions of visualizing mixed research projects as seen in figure 1. According to Tashakkori et al. (2020), quantitative components are depicted as ovals and qualitative components are depicted as rectangles. Additionally, in figure 1, barred rectangles indicate mixed inferences. Solid arrows indicate the general direction of the workflow throughout the project and dashed arrows represent potential feedback loops where the results and inferences from one phase inform one or more components of a subsequent phase. Mixed researchers use upper- and lower-case letters to indicate the priority in any given phase. QUAN (all-caps) indicates that quantitative data and analyses are prioritized, while qual (lower-case) indicates that qualitative data and analyses are secondary. Further, a ‘+’ indicates that two or more types of data are collected and/or analyzed concurrently, while a ’ → ’ indicates that two or more types of data are collected or analyzed consecutively or sequentially. So, the notation ‘QUAN+qual’ indicates that both quantitative and qualitative data are collected and analyzed concurrently with quantitative data and analysis being prioritized. The notation ‘QUAN → QUAL’ indicates that quantitative data are being collected and/or analyzed before qualitative data, and both types of data and analysis have equal weight. Figure 1 A Visualization of a QUAN+qual → QUAL Sequential Exploratory Mixed Research Study. Following a pilot phase, I intend to engage in a QUAN+qual → QUAL sequential exploratory research project comprising two separate data collection and analysis phases. In phase 1a, I will use a survey of higher education instructors to collect and analyze primarily quantitative data to investigate research questions 1 and 2. Are there distinct patterns in higher education instructors’ approaches to assessment in Canada? Does the prevalence of these patterns differ by: instructors’ levels of experience in teaching face-to-face versus online? instructors’ levels of experience using technology? This will be supplemented in phase 1b by the concurrent collection and subsequent analysis of qualitative data to answer research question 3. What factors influence instructors’ approaches to assessment? The second phase will begin following the analysis and integrationof phase 1a+b data and will collect and analyze qualitative data in semi-structured interviews with learners nominated by instructors in phase 1. Phase 2 analysis is intended to answer research question 4. How do instructors’ assessment approaches affect learners’ experiences? Since research question 4 is an integrative question (Creamer 2018), it cannot be answered fully until the inferences from phase 1 and phase 2 are integrated into one or more meta-inferences. 3.4 Participants and Sampling Strategy The target population for the research will be instructors who have taught at least one semester-length, accredited course in the previous 12 months at an English-speaking [^1] public higher education institution in Canada. Ideally, inferences from phase 1 of the study would be generalizable to the total population of English-speaking higher education instructors in Canada, which would require a random sample in which every member of the target population would have equal opportunity to be selected and no selection would have been influenced by a previous selection (Hibberts, Burke Johnson, and Hudson 2012; Renckly 2002). Care will be taken to ensure the sample is representative of the whole population by engaging in a stratified random sampling technique at the institutional level (Hibberts, Burke Johnson, and Hudson 2012). This would involve using regions within Canada as strata (for example, British Columbia, the prairie provinces, the north, Ontario, Quebec, eastern Canada), randomly choosing 2-3 higher education institutions from within those strata, and sending the survey to all instructors at those institutions. Recruitment of participants for phase 2 will involve phase 1 participants (instructors) nominating 8-10 learners from their courses, I will randomly select 2-3 respondents from phase 1 and then randomly select 4-5 of their nominated learners to receive invitations to the semi-structured interviews. This procedure will ensure that instructors will not know which of their nominated learners participated in the research and it will create “linked data” (2018, 126), allowing for greater explanatory power due to being able to match specific instructor approaches with learner impacts. 3.5 Data Collection Strategy 3.5.1 Phase 1 I propose that the phase 1 data collection instrument be DeLuca et al.’s (2016a) ACAI (see Appendix 1 for the full ACAI v3.0 and its specifications). The ACAI was designed by a team of researchers, led by Dr. Christopher DeLuca at Queen’s University in Canada to investigate the approaches that K-12 educators take to assessment in their classrooms. It is based on an analysis of assessment standards published in five majority English-speaking countries since 1990. The instrument is based on a four-dimensional framework of assessment literacy (assessment purposes, assessment processes, assessment fairness, and measurement theory), and includes one open-ended question and several closed-ended questions about how teachers form their approaches to assessment. It also includes a section regarding educator beliefs about assessment based on the Beliefs about Assessment scale (Smith et al. 2014). The ACAI has been used in a wide variety of studies within the instrument’s author’s research group (Coombs et al. 2018; Coombs, DeLuca, and MacGregor 2020; DeLuca, Coombs, et al. 2019; DeLuca et al. 2016; DeLuca, Willis, et al. 2019; Schneider et al. 2020), and also by other researchers (Nayagi N and Rajendran 2020). In several of these studies, the ACAI has been modified somewhat to fit the particular context or research design. As the ACAI was designed for use in K-12 contexts and has been used less often to investigate higher education instructors’ approaches to assessment, I will modify the language in the instrument to be more suitable for the higher education context and to include more open-ended questions. Following these modifications, I will undertake a pilot phase to determine whether the modified ACAI is suitable for the higher education context. The pilot phase will inform the version of the ACAI used in Phase 1 of the study and will include both expert review and the recruitment of a small sample from the target population to preview the survey in its intended mode and offer specific and guided feedback (Hibberts, Burke Johnson, and Hudson 2012; Renckly 2002). 3.5.2 Phase 2 The semi-structured interviews in phase 2 will include questions similar to the following: Please describe what you understand assessment to be. Did the assessment strategies that your instructor used help you to learn the course material? Did [instructor] use technology to administer assessments? How did their use/not use of technology impact your performance on the assessments? Please describe your favourite assessment from [instructor]’s course [course name]? What made that assessment your favourite? Please describe your least favourite assessment from [instructor]’s course [course name]? What made that assessment your least favourite? What factors made it easier to do well on the assessments in [course]? What factors made it more difficult to do well on the assessments in [course]? How did the assessment strategies your instructor used make you feel? Do you receive any support or accommodations to allow you to complete assessments? 3.6 Types of Data and Analysis As shown in the visualization of the research project (Figure 1), both quantitative and qualitative data will be gathered for analysis. The following sections will describe the data I intend to gather and how it may address the research questions. 3.6.1 Quantitative data During phase 1a, quantitative data will be gathered using the ACAI and will primarily be in the form of responses to 6-point Likert scale items. Similar to Flores et al. (2020), the first step will be to engage in a confirmatory factor analysis to determine the fit of the data with the Approaches to Classroom Assessment model. This step will address research question 1. Then, as the data will be ordinal data (i.e. a score of five is greater than a score of three), but not interval data (i.e. it is not possible to determine how much greater a score of five is compared to a score of three) (Bulut 2021; Jamieson 2004), it will not be appropriate to use the data to calculate common statistics such as mean or standard deviation (i.e. there is no way to calculate the mean of “highly likely” and “highly unlikely”). Consequently, more advanced statistical techniques which rely on the mean and standard deviation (i.e. t test, analysis of variance), otherwise known as parametric tests, are not appropriate for ordinal data (Field 2018). Fortunately, there are non-parametric tests that enable analysis of data that do not meet the requirements for parametric tests. One example, which will be useful in comparing two different demographic groups within my data (e.g. instructors with more than two years experience teaching with technology compared to instructors with less than two years of experience teaching with technology), is the Mann-Whitney-Wilcoxon test (Gao 2010) (MWW). Instead of calculating based on the mean scores of different samples, the MWW test requires the researcher to first rank the scores and then calculate the differences between the samples based on the ranked scores. If I am to compare 3 or more groups, the Kruskal-Wallis test (Gao 2010), based on the same ranking procedure, would be more appropriate. These two tests will be used to address research question 2. 3.6.2 Qualitative data During phase 1b, I will gather and analyze qualitative data through the use of open-ended questions embedded in the ACAI. Part B of the ACAI currently contains open-ended questions asking respondents to describe how they would respond to the scenario if their response isn’t listed as an option in the instrument (DeLuca ND). Below are some open-ended questions which might be added to Part A (demographics) of the ACAI: In a few sentences or point form notes, please describe the most important factors you consider when planning how you will assess learners in an upcoming course. In a few sentences or point form notes, please describe what you understand assessment to be. In what ways do you use digital technology in your assessment practice? The current and proposed open-ended questions in the ACAI are intended to provide qualitative data which will be analyzed using a theory-driven thematic coding process (DeLuca et al. 2018; Namey et al. 2008) to explore in greater depth the factors that influence instructors to approach assessment as they do in their courses (research question 3). Namey et al. (2008) describe two general approaches to the analysis of qualitative data: content analysis and thematic analysis. In content analysis, the researcher “evaluates the frequency and saliency of particular words or phrases in a body of original text data in order to identify keywords or repeated ideas” (p. 138), but with the drawback that there is limited interpretation or consideration of context. In thematic analysis, researchers consider not only specific words and phrases, but also “implicit and explicit ideas” (p. 138). They further describe “data-driven” (themes emerge from the data) and “theory-driven” (the researcher identifies themes a priori, without consulting the data)(p. 138) approaches (see also: Tashakkori, Johnson, and Teddlie 2020). In phase 1b of the proposed project, I will take a theory-driven approach to a thematic analysis where data will be coded in alignment with the four dimensions of the Approaches to Classroom Assessment framework (DeLuca et al. 2016) (the dotted line connecting “Phase 1a Inferences” to “Phase 1b Analysis” in figure 1). In addition, I will be particularly mindful that emergent themes may become clear in the data and I will be especially interested in those not represented in, or which are in contradiction to, the quantitative analysis (Tashakkori, Johnson, and Teddlie 2020). After completing both phase 1a and 1b, I will be able to synthesize inferences from the two phase 1 analyses, and also one or more meta-inferences from the integrated analysis of the two sub-phases. These inferences will inform the phase 2 semi-structured interview questions, providing an integration point (the dotted lines from “Phase 1a Analysis,” “Phase 1b Analysis,” and “Phase 1 Meta-inferences” to “Semi-structured Interviews”). Raw qualitative data from phase 2 of the project will be in the form of audio recordings of semi-structured interviews, which will be transcribed verbatim and stored as plain text files. As this data is less connected to phase 1 data because it is a different sample population, I will use a data-driven thematic approach to uncover emergent themes in the data while not precluding the possibility that theory-driven themes may emerge. Following the phase 1 and phase 2 analyses, I will synthesize the inferences from phase 1a and 1b, the meta-inferences from phase 1, and the inferences from phase 2 to draw project-level meta-inferences from the whole of the data, thereby addressing research question 4. 3.7 Research Procedures Following approval from the University of Victoria Human Research Ethics Board (HREB) and a brief pilot phase, I will use a stratified random sampling technique to randomly select 2-3 higher education institutions from each of six regions in Canada (British Columbia, the prairie provinces, Ontario, Quebec, eastern Canada, and northern Canada), for a total of 12-18 institutions. I will contact the faculty associations at each of these institutions to request that they forward my letter of invitation and embedded link to a web-based survey. Each participant will be asked to self-identify and to nominate 8-10 learners in one of their courses and include the nominees’ contact information. To begin phase 2, I will randomly select 2-3 instructors from phase 1, then randomly select 4-5 of their nominated learners to invite to semi-structured interviews. The interviews will be conducted via web-conferencing software that allows recording and the automatic generation of transcripts. Table 5 shows a projected timeline for the research. Table 5 Projected Timeline Projected Date Project Stage December 2021 UVic HREB Approval January 2022 Pilot Phase February 2022 Phase 1 Data Collection July 2022 Phase 1a and 1b Data Analysis September 2022 Phase 2 Data Collection November 2022 Phase 2 Data Analysis January 2023 Integration and Write-Up 3.8 Chapter Summary In this chapter, I have discussed the three major approaches to research, quantitative, qualitative, and mixed, along with their underlying epistemologies. Through a review of the literature on assessment literacy and its impact on learners in higher education, I show that researchers use all three approaches and that the most important consideration in determining a method to pursue an investigation in this area is the purpose of the investigation. As the purpose of this investigation is to gain understanding of the multi-dimensional construct of assessment literacy and its effects on learners, a highly subjective phenomenon, a mixed research approach is justified. I discuss a possible method to gather and analyze quantitative and qualitative data from higher education instructors and their learners, along with strategies for drawing inferences and meta-inferences based on the integration of the separate analyses. [^1] If funding for translation services is secured, I would extend the sample population to include French-speaking instructors and learners. References "],["references.html", " 4 References", " 4 References "],["introduction.html", " 5 Introduction 5.1 Topic of the Research 5.2 Problem to be Researched 5.3 Purpose of the Research 5.4 Research Questions 5.5 Significance of the Research", " 5 Introduction Assessing learning is both a critical component of the work of teaching in higher education (HE) and also a major factor in learners’ experiences of HE (J. Biggs 1999; Menucha Birenbaum 2007; Woldeab and Brothen 2019). Instructors in HE often inherit the assessment practices of their supervisors and instructors (Lipnevich et al. 2020), potentially leading to over-reliance on practices that may not align with modern assessment theories (Boud 2020; Massey, DeLuca, and LaPointe-McEwan 2020; Shepard 2000). These traditional practices tend to emphasize summative, selected-response tests (Flores et al. 2015; Harlen and Deakin Crick 2002; Lipnevich et al. 2020) which may be characterized by low reliability, leading to inferences which may not be valid reflections of learner achievement (Knight 2002; Smith Glasgow, Dreher, and Schreiber 2019). Also, the influence of an increasingly technological society is pushing higher education institutions (HEI) to integrate digital tools into the teaching and learning process (Pellegrino and Quellmalz 2010). This trend saw a massive increase in activity as a result of the COVID-19 pandemic that forced most HEI in Canada to pivot to emergency remote teaching in the spring of 2020 through at least the summer of 2021 and perhaps beyond. While this increased use of technology may be understood by some as progress, research suggests that digital technologies typically are used in ways which mimic and re-inscribe traditional assessment practices and simply increase the efficiency with which they can be administered (Broadfoot 2016). Additionally, the needs of 21st century employers, who are looking for employees with demonstrated ability in collaboration, creative problem-solving, analytical thinking, and the ability to learn, competencies not easily measured in traditional testing formats, as opposed to those who have simply demonstrated high levels of domain-specific knowledge, have changed (Forum 2020). Finally, assessment practices have significant effects on learners including anxiety about high-stakes testing and their prospects for future studies and employment (Harlen and Deakin Crick 2002), and also the approaches they take to learning activities (J. Biggs and Tang 2011; DeLuca, Coombs, et al. 2019). The confluence of these influences suggests that traditional approaches to assessment in HEI are becoming inadequate to meet the needs of modern HEI and ought to be reconsidered. 5.1 Topic of the Research A starting point for this process of examining assessment in higher education ought to be intentionally considered as there are myriad examples of formal and informal initiatives in higher education which have not realized significant local or systemic change (Broadfoot 2016; Earl 2013). Assessment practices are highly resistant to change, in part, as Broadfoot claims, because they are so important, but also, as will be discussed later, because the approaches that individual instructors take to assessing the work of learners are driven by complex forces (Black and Wiliam 1998; DeLuca, Coombs, et al. 2019; Stiggins 1991; Willis, Adie, and Klenowski 2013). Accordingly, it is incumbent upon researchers who wish to influence assessment practice to begin with a nuanced investigation of how HE instructors approach their assessment practice and what are the impacts of those practices on learners. 5.2 Problem to be Researched In her important article, Shepard (2000) argues that traditional assessment structures originated in past models of curriculum and instruction which were popular in the early 1900s. These curricular models emphasized the work of psychologists like Thorndike and Skinner who viewed the process of learning as being grounded in the mechanistic view of behaviourism where learning is the result of the precise and controlled input of ‘knowledge’ and reinforced with rewards for correct responses. As such, instructors (appropriately) designed their assessments to align with the curricular goals of the time and assessed learning by determining whether or not a learner could provide the single correct response to a given question at a time removed from the instruction. However, in the latter half of the 20th century, when western psychologists discovered the ideas of Lev Vygotsky (who was actually a contemporary of the previously mentioned psychologists), curricula began to take a more social-constructivist approach that emphasized higher-order thinking, problem-solving in social contexts, and metacognitive skills over rote memorization. Unfortunately, it seemed that the efficiencies of testing memory, recognition, and recall through selected-response tests were too deeply embedded in the practices of HE instructors who resisted changing their assessments to match the new curricular goals. Shepard argues for the need to integrate assessment and instruction in such a way as to engage learners in authentic performance tasks more suited to modern understandings of cognition. It appears now that, in the twenty years since Shepard wrote her paper, the goals of early 21st century curricula have continued to diverge from those of the 20th century with the World Economic Forum identifying competencies in collaboration, analytical thinking, creative problem-solving, and the continual learning as being priorities for 21st century employers (Forum 2020). Consequently, models of assessment which prioritize testing skills in a manner aligned with 20th century curricular models are no longer adequate because they no longer align with the priorities of modern HE (Crooks 1988). 5.3 Purpose of the Research Following previous research by DeLuca and colleagues (DeLuca et al. 2016; DeLuca, Rickey, and Coombs 2021; DeLuca, LaPointe-McEwan, and Luhanga 2016a) in the K-12 sector, and Massey et al (2020) in the higher education sector, the purpose of this research is to investigate current assessment literacies and practices among higher education instructors and the impacts of those approaches on learners. In order to better respond to actual assessment practices, it is critical to understand the conceptions of HE instructors with respect to assessment (DeLuca, Rickey, and Coombs 2021; Offerdahl and Tomanek 2011). Similarly, due to the significant influence assessment practices have on learners and learning, understanding the relationship between instructors’ assessment conceptions and practices and the experience of learners will be important in order to provide a foundation for moving into the remainder of the 21st century with assessment practices aligned with both pedagogical models and learner contexts. 5.4 Research Questions Assessment in HE is a tremendously complex enterprise with myriad forces influencing the approaches instructors take in their assessment practice, which, in turn, becomes one of the myriad forces influencing the approaches learners take in their learning activities. Instructors conceptions of assessment are often derived from their past experiences as learners who were the objects of assessment, leading to an over-reliance on summative assessment strategies which may not obtain sufficient levels of reliability or validity. Increasing assessment literacy among HE instructors is a necessary, but not likely sufficient, step in advancing fair and balanced assessment practices in an increasingly technology-mediated HE landscape. These concepts lead to the following research questions: Are there distinct patterns in higher education instructors’ approaches to assessment in Canada? Does the prevalence of these patterns differ by: instructors’ levels of experience in teaching face-to-face versus online? instructors’ levels of experience using technology? What factors influence instructors’ approaches to assessment? How do different assessment strategies affect learners’ experiences? 5.5 Significance of the Research This research will form the basis for an ongoing research agenda focused on research-supported strategies for assessment reform in Canadian HEI. Understanding both how instructors think about assessment and how learners are impacted by assessment decisions will be critical to informing assessment practices and policies as higher education emerges from the pandemic and moves forward into the 21st century. References "],["review-of-the-literature.html", " 6 Review of the Literature 6.1 Structure of the Paper 6.2 Defining Assessment 6.3 Conceptions of Assessment 6.4 Assessment Literacy 6.5 Assessment and Technology 6.6 Impact on Learners 6.7 Theoretical Framework Overview", " 6 Review of the Literature 6.1 Structure of the Paper This paper will begin with establishing a clear definition of assessment followed by an analysis of the literature related to the concept of assessment literacy (AL) (DeLuca, Coombs, et al. 2019) as it relates to the approaches to assessment (DeLuca, LaPointe-McEwan, and Luhanga 2016a) taken by HE instructors and the perceptions and experiences of HE learners. The last section will introduce a framework for understanding assessment in HE, situated within Biggs’ (1999; 1993) 3P model of teaching and learning, and research questions which emerge from the literature. 6.2 Defining Assessment There are deep and rich bodies of literature addressing educational assessment writ large, both from a summative, psychometric perspective, and from a formative perspective. There is notably more research in the K-12 context, especially in relation to teacher preparation, compared to HE. Among the more influential publications related to modern views of assessment (then usually called “evaluation”) was Scriven’s (1967) article in which he drew distinctions between “formative” and “summative” evaluation. Formative evaluation was described as evaluation for the purposes of improvement, and summative evaluation was seen as a validation of the quality of work at the end of a process. This distinction was quickly incorporated into Bloom’s (1968) ideas related to mastery learning and began to be promoted as a model for educational reform. However, by the late 1990s, when Black and Wiliam (1998) published their thorough review of the literature, the idea of formative assessment was still not well-defined or implemented. Black and Wiliam framed formative assessment as “encompassing all those activities undertaken by teachers, and/or by their students, which provide information to be used as feedback to modify the teaching and learning activities in which they are engaged” (1998, 7–8). Although Black and Wiliam came to very strongly-stated conclusions about the value of formative assessments (e.g. “The research reported here shows conclusively that formative assessment does improve learning. The gains in achievement appear to be quite considerable, and as noted earlier, amongst the largest ever reported for educational interventions.” (1998, 61)), reliance on summative assessments in HE has remained high Lipnevich et al. (2020). The National Research Council’s (NRC) 2001 report Knowing what students know, advanced understanding of assessment with their definition of assessment as “a process of drawing reasonable inferences about what students know on the basis of evidence derived from observations of what they say, do, or make in selected situations” (Pellegrino, Chudowsky, and Glaser 2001, 112) or, more simply, “reasoning from evidence” (Pellegrino, Chudowsky, and Glaser 2001, 43), based on Mislevy’s assertion that “test theory is machinery for reasoning from students’ behavior to conjectures about their competence, as framed in a particular conception of competence.” (1994, 4). Such a parsimonious description, however, may hide some of the complexities of fairly and equitably coming to know what learners know and can do in relation to particular outcomes. Since knowledge of a particular domain cannot be directly observed in a learner, and therefore cannot be quantified, instructors must rely on data gathered during the teaching process to support a particular inference about what a learner probably knows. The data gathered from performance tasks such as exams, essays, portfolios, labs, etc, become evidence when they support an inference about what a learner knows and can do. Hence, all summative assessments are probablistic, not deterministic. More recently, Earl (2013) further clarified the role assessment can play in learning by highlighting a distinction between assessment of learning (summative assessment) and assessment for learning (formative assessment by way of feedback) and also distinguishing both of those from assessment as learning (a subset of assessment for learning in which learners employ metacognitive skills to regulate their own learning tasks). Earl’s delineation between types of assessment reflects the modern view that assessment and learning are, or ought to be, tightly integrated. This relationship will be explored in more detail in relation to Biggs’ 3P Model of Teaching and Learning (1996, 1999). The definitions of assessment above are typically understood as being classroom assessment, language which is more readily applied to face-to-face K-12 learning environments as opposed to HE environments mediated by technology. For the purposes of this paper, I will consider assessment of learning and summative assessment to be essentially synonymous, and I will differentiate between assessment for and as learning. I will use the term classroom assessment to differentiate from large-scale assessment, understood to be assessments deployed at levels above individual classrooms, such as school-, system-, or provincial/federal-levels, and I will use online assessment or technology-mediated assessment to refer specifically to classroom assessment in learning environments mediated by technology whether the learners are remote or not. 6.3 Conceptions of Assessment Instructors in HE typically receive little formal preparation in either teaching practices or assessment during their graduate studies (Lipnevich et al. 2020; Massey, DeLuca, and LaPointe-McEwan 2020). Consequently, their own practice tends to follow from what they experienced as learners, which likely emphasized high-stakes summative tests which were either in alignment with outdated pedagogical practices or out of alignment with modern pedagogical practices. These prior conceptions of assessment carry significant weight in how HE instructors approach the assessment of learners in their own courses. Instructors bring to an educational environment a host of influences related to their institutional context, their past experiences with assessment, their own course policies, and affective beliefs or conceptions about assessment and its purposes (J. B. Biggs 1993; G. T. L. Brown, Lake, and Matters 2011). These influences play a significant role in determining the approaches taken by both instructors with respect to assessment and learners with respect to how they approach learning tasks (G. T. L. Brown, Lake, and Matters 2011). DeLuca et al. (2013) argue that there are categories of conceptions exhibited by K-12 preservice teachers: assessment as testing, assessment as format, assessment purpose, and assessment as process. These conceptions are seen as increasingly complex, with those who see assessment as testing believing that assessment is primarily concerned with summative assessment of learning, usually using teacher-created selected-response tests. Those who see assessment as format tend to focus on whether the assessment is a “performance, product, or objectively-scored assessment” (p. 110). Assessment as purpose is delineated according to the summative/formative binary or Earl’s (2013) assessment of/for/as learning model. DeLuca et al. also identify other purposes of assessment such as accountability, gatekeeping, and teacher evaluation. Lastly, assessment as process, which is based on the National Research Council’s description of assessment being a process of reasoning from evidence (2001). Fletcher et al. (2012) used Brown’s (2017) abridged Conceptions of Assessment (CoA) questionnaire to measure learners’ and instructors’ conceptions as follows: “assessment makes institutions accountable, assessment makes students accountable, assessment describes improvements in student abilities, assessment improves student learning, assessment improves teaching, assessment is valid, assessment is irrelevant and bad, assessment is irrelevant and ignored, and assessment is irrelevant and inaccurate” (p. 122). They report that instructors were more likely than learners to view assessment as consistent and trustworthy methods to understand and improve learning and that learners were more likely to have negative views of assessment and viewed it as a measure of student and institutional accountability. Massey et al. (Massey, DeLuca, and LaPointe-McEwan 2020) used DeLuca et al.’s (2013) framework of conceptions in their study of HE instructors’ conceptions of assessment before and after an instructional development course focussed on assessment. They also considered the idea that there are two general orientations towards assessment in HE, an “assessment culture” and a “testing culture” Massey, DeLuca, and LaPointe-McEwan (2020). They report that they saw significant shifts in participants’ conceptions of assessment from more simplistic views of assessment as testing pre-treatment, to more complex and nuanced views or assessment as process post-treatment. From the literature, it is clear that instructors’ conceptions of assessment are deeply influenced by many internal and external factors and, especially in HE, where there are few constraints on assessment practice (Lipnevich et al. 2020), there are many ways to describe or delineate different conceptions of assessment. Accordingly, there are multiple ways to conceptualize the skills and dispositions that comprise the idea of assessment literacy. 6.4 Assessment Literacy The idea of AL is relatively recent in the K-12 literature and is nascent and under-theorized with respect to HE contexts (Medland 2015). AL has been defined variously as “the skills and knowledge teachers require to measure and support student learning through assessment” (DeLuca, LaPointe-McEwan, and Luhanga 2016a), “a basic understanding of educational assessment and related skills to apply such knowledge to various measures of student achievement” (Xu and Brown 2016), “an individual’s understandings of the fundamental assessment concepts and procedures deemed likely to influence educational decisions” (Popham 2011) and “a dynamic context-dependent social practice that involves teachers articulating and negotiating classroom and cultural knowledges with one another and with learners, in the initiation, development and practice of assessment to achieve the learning goals of students” (Willis, Adie, and Klenowski 2013). Key to these definitions are the ideas that AL is a complex, multi-faceted construct, that AL requires adequate (not high) levels of psychometric or statistical analyses, and that it is intended to enable learner success. The recognition of AL as a critical competency for educators was influenced by the growing demands for teacher and school accountability in the post-WWII era in the USA and Canada, particularly the Elementary and Secondary Education Act (ESEA), passed in 1965, and the No Child Left Behind (NCLB) act, passed in 2002 (see DeLuca 2012 for a detailed discussion). As such, conceptualizations of AL have tended to be based on sets of standards to which K-12 teachers are obligated. The first set of standards was the Standards for Teacher Competence in Educational Assessment of Students (the Standards), published by a committee of representatives from the American Federation of Teachers, the National Council on Measurement in Education, and the National Education Association (AFT, NCME, and NEA 1990). The Standards are a list of seven skills expected of teachers: Teachers should be skilled in choosing assessment methods appropriate for instructional decisions. Teachers should be skilled in developing assessment methods appropriate for instructional decisions. The teacher should be skilled in administering, scoring, and interpreting the results of both externally-produced and teacher-produced assessment methods. Teachers should be skilled in using assessment results when making decisions about individual students, planning teaching, developing curriculum, and school improvement. Teachers should be skilled in developing valid pupil grading procedures which use pupil assessments. Teachers should be skilled in communicating assessment results to students, parents, other lay audiences, and other educators. Teachers should be skilled in recognizing unethical, illegal, and otherwise inappropriate assessment methods and uses of assessment information. Shortly after the publication of the Standards, the term assessment literacy appeared in the literature with Stiggins’ (1991) article called Assessment Literacy. Stiggins initial article was an account of his observation that teacher education programs at the time spent very little time training teachers in the methods and dispositions of educational measurement. Stiggins followed this with another article (Stiggins 1995) where he outlined five characteristics of sound assessments, which: arise from and serve clear purposes; arise from and reflect clear and appropriate achievement targets; rely on a proper assessment method, given the purpose and the target; sample student achievement appropriately; and control for all relevant sources of bias and distortion. (1995, 240) At around the same time, a group of Canadian educators published the Principles for Fair Student Assessment Practices for Education in Canada, Part A of which was a list of 37 guidelines related to five principles of fair student classroom assessment and was based on the 1990 Standards (see Appendix A). Part B was focused on externally-developed standardized tests. While these three sets of recommendations varied widely in their granularity, all tended to reflect an emphasis on the 20th century conceptions of curriculum which prioritized linear and sequential teaching of knowledge followed sometime later by selected-response tests of knowledge. This required teachers to be literate in the psychometric skills required to administer and interpret these tests (DeLuca, LaPointe-McEwan, and Luhanga 2016b; Shepard 2000; Xu and Brown 2016) or assessment of learning. One notable distinction between the sets of standards is that the Canadian committee specifically noted their principles could be applied to K-12 as well as higher education, although the latter context would require changes in how assessment data are reported. Twenty years following the publication of the Standards, Brookhart (2011) argued that the Standards had become outdated because they did not address either the growing practices and ideas of formative assessment (assessment for and as learning) or standards-based assessment and that they needed to be revised. Brookhart suggested a list of 11 skills (see Appendix A) to adjust the focus of the 1990 Standards to be in greater alignment with more modern conceptions of assessment. Finally, in 2015, the Joint Committee on Standards for Educational Evaluation (JCSEE), with key representatives from both Canada and the USA, published the most recent set of standards, called the Classroom Assessment Standards for PreK-12 Teachers (see Appendix A). The JCSEE standards are grouped into three broad domains (foundations, use, and quality), each with five or six related standards. Despite the similarities to the Principles for Fair Student Assessment Practices for Education in Canada, including at least one common committee member, the JCSEE Standards are specifically not intended for use in HE. As traditional conceptions of assessment and the standards expected of teachers, grounded in behaviourism and the need for objectivity tended to focus on assessment as a set of skill-based competencies to be employed by instructors, so AL could be defined as a set of sequential tasks in which instructors should engage to ensure objectivity and fairness (e.g. (Natriello 1987)). Recently, as curriculum and pedagogy have changed, several researchers have proposed models related to AL grounded in socio-constructivist views of learning (DeLuca 2012; Pastore and Andrade 2019; Xu and Brown 2016). DeLuca’s (2012) model, developed in the context of the No Child Left Behind accountability mandate in K-12 schools in the USA, is a coherent lens through which to understand how a pre-service teacher could develop assessment expertise throughout their teacher education program. DeLuca frames AL within Fostaty Young and Wilson’s (fostatyyoungAssessmentLearningICE2000?) ICE model which presents an integrated progression of learning through three levels of complexity, ideas, connections, and extensions. Underlying the three levels is a foundational level, where teacher candidates come to know and understand assessment as being situated within a model of teaching and learning, experiential and inclusive pedagogies, and an orientation towards continual professional learning. At the ideas level, teacher candidates gain expertise in the big ideas related to assessment (theories of learning, planning educational experiences, classroom assessment, and issues of diversity and inclusion). At the connections level, teacher candidates begin to construct their own cognitive conceptions of assessment as they integrate their own past experiences with their new knowledge and add their experience in practica. At the extensions level, teacher candidates begin to hone their practice as they deepen their understanding of assessment. Willis et al. (2013) describe assessment literacy in alignment with Bernstien’s (1999) idea that there are “horizontal” and “vertical” discourses (p. 159) with respect to assessment. A horizontal discourse is the local, contextualized discourse around assessment which influences local practice, while a vertical discourse is the formalized, structured discourse on assessment in the literature and other more authoritative venues. They argue that any given instructor’s conceptions and practice of assessment will be informed by and negotiated within multiple horizontal and vertical discourses. Similarly, Xu and Brown’s (2016) model, teacher AL in practise (TALiP) presents a pathway for pre-service teachers to gain expertise in assessment. Based on a synthesis of 100 peer-reviewed publications between 1985 and 2015, they present a six-component model: the knowledge base teacher conceptions of assessment, institutional and socio-cultural contexts, teacher assessment literacy in practice, teacher learning, and teacher identity as assessor. Finally, Pastore and Andrade (2019) developed their model through a Delphi inquiry of 35 international experts in educational assessment and teacher education. They propose a model with three dimensions, conceptual, praxeological, and socio-emotional. The authors of each of these three models recognize that AL is conceptualized as a multi-dimensional construct encompassing psychometric skills, affective beliefs and values, external and regulatory environments, and socially negotiated practices. However, these models are largely specific to the K-12 environment in general, and more specifically related to the preparation of K-12 teachers. Given that there are very few parallels between the preparation of K-12 teachers for their role and the preparation of HE instructors for theirs, it seems that a framework for understanding AL among HE instructors should consider their general lack of formal preparation for teaching or assessing learning (Lipnevich et al. 2020; Massey, DeLuca, and LaPointe-McEwan 2020). Bearman et al. (2016) proposed a model for assessment decision-making in Australian HE, the Assessment Design Decisions Framwork (ADDF) in which the authors acknowledge the difficulty in translating idealized beliefs about assessment into actual practice as well as the lack of literature regarding how HE instructors go about designing assessments. While not specifically an AL framework, there are overlaps in terms of the dimensions they identified. They argue, like those previously mentioned, that assessment is a complex and messy process. Unique to their model is the idea that there is often a disconnect between what instructors know or believe to be true about assessment, and how instructors’ practice is impacted (or not) by their beliefs. Additionally, they argue, in alignment with Price et al. (2011), that AL ought to be considered from both instructor and learner perspectives. Their framework is comprised of six dimensions: purposes of assessments contexts of assessments learner outcomes tasks feedback processes interactions Herppich et al. (2018) frame a model of assessment competence beginning with the idea that the purpose of educational assessment is to inform both formative (ongoing learning) and summative (credentialling or certification) decisions locating the educational decision subsequent and subordinate to the judgement. They use an example where a teacher observes a learner struggling with a test and might come to an appropriate judgement of a learner’s knowledge, but make an inappropriate instructional decision based on that information. This distinction is useful when, like Herppich and colleagues, the construct under investigation is assessment competence, which is observed after instructional activities. However, in the present paper, conceptions of assessment and AL are the relevant constructs and they are embedded in a model of teaching and learning where conceptions of assessment precede instructional activities. DeLuca et al.’s model (2016a), approaches to classroom assessment defines and conceptualizes the assessment component of Biggs’ 3P model (1999; 1993) and so aligns well with a concise model of teaching and learning in higher education. The Approaches to Classroom Assessment model is based on the JCSEE standards (Klinger et al. 2015) and describes four themes of AL, each with three dimensions. The model represents somewhat of a break from previous models in that it references approaches to assessment rather than assessment literacies. This is a reflection of the authors’ view that language around literacies and competencies may indicate a reliance on “correct” views or methods rather than the complex array of influences that lead to multiple legitimate approaches as identified in the literature (DeLuca, Coombs, et al. 2019; Willis, Adie, and Klenowski 2013). The themes DeLuca et al (2021, 10) describe along with their associated dimensions are listed below and illustrated in figure 2: Assessment purposes. Assessment of learning Teachers’ use of evidence to summate student learning and assign a grade in relation to students’ achievement of learning objectives Assessment for learning Teachers’ and students’ use of evidence to provide feedback on progress towards learning objectives (i.e., inform next steps for learning and instructions). Involves both teacher-directed and student-centred approaches to formative assessment. Assessment as learning Focuses on how the student is learning by providing feedback or experiences that foster students’ metacognitive abilities and learning skills (e.g., self-assessment, goal-setting, learning plans). Involves teachers but is primarily student-centred. Assessment process Design Focuses on the development of reliable assessments and items that measure student learning in relation to learning objectives. Use/scoring Focuses of the adjustment and use of scoring protocols and grading schemes to respond to assessment scenarios. Communication focuses on the interpretation of assessment results and feedback through communication to students and parents. Assessment fairness Standard Maintains the equal assessment protocols for all students. Equitable Differentiates assessment protocols for formally identified students (i.e., special education or English language learners) Differentiated Individualizes learning opportunities and assessments that address each student’s unique learning needs and goals. Assessment theory Consistent Works to ensure consistency in results within assessments, across time periods, and between teachers. Contextual Works to ensure assessment or evaluation measures what it claims to measure (i.e., learning objectives) and promote valid interpretations of results. Balanced Works to ensure consistency in measuring what an assessment or evaluation intends to measure, and degree to which an assessment or evaluation measures what it claims to measure. Figure 2. Approaches to Classroom Assessment. (DeLuca, Rickey, and Coombs 2021, 10) 6.5 Assessment and Technology Educational technologies are often viewed and reported on with a distinct positivity bias (Irvine 2020) wherein ‘new’ technologies are presumed to represent progress and will inevitably have a positive effect on learning. This can be seen in the titles given to some initiatives, such as “Technology-Enhanced Assessment” (Oldfield et al. 2012; Timmis et al. 2016), “IT-enabled assessment” (Webb and Ifenthaler 2018), or “technology-rich” (Lin et al. 2020). As such, I will use the more neutral term “technology-mediated” to indicate that adding digital technology to an assessment environment does not necessarily improve that environment. Similar to assessment practices being grounded in (both philosophically, as in ‘based upon,’ and figuratively, as in ‘stuck in’) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment, so too, many educational technologies are grounded in (based upon and stuck in) behaviourist conceptions of pedagogy leading to practices that rely heavily on summative approaches to assessment. This can be seen in the progressively more advanced technologies beginning with Pressey’s teaching machines (Benjamin 1988; Pressey 1927; A. Watters 2021) which was built to automate the process of “drilling” learners in an effort to teach them some concept. The machine needed to be pre-programmed with a series of selected-response questions along with distractors and correct answers. As a learner answered each question, the machine was programmed to match the response to the programmed correct response, and if it matched, the learner was determined to have “mastered” that question and it was dropped from the bank of questions the learner had not yet mastered. If it did not match, the question was cycled back into the bank to be repeated. Clearly, this technology was promoted as a tool to be used to modernize and increase the efficiency of tasks that aligned with the dominant pedagigical paradigm at the time. A second example, although not one marketed directly to schools, but to parents, was the Speak &amp; Spell, released in 1978 by Texas Instruments (Braguinski 2018; Frantz 2014), which represented an advance in technology and an increase in efficiency, as the Speak &amp; Spell could be programmed to store and reproduce voice recordings of words as well as multiple recordings of feedback messages (Frantz 2014). While the Speak &amp; Spell was a leap forward in processing power, memory storage, and therefore complexity, the underlying pedagogy remained identical to that of Pressey’s teaching machine (Audrey Watters 2015). Moving forward again, and modern technologies are vastly more powerful than teaching machines or the Speak &amp; Spell and power very complex adaptive tests, such as the NCLEX-RN, the national licensing exam for Registered Nurses in Canada and the USA (Smith Glasgow, Dreher, and Schreiber 2019). These advances in both hardware and software allow for still greater efficiencies in testing, yet the NCLEX-RN must still be programmed with selected-response questions, their distractors, and correct responses, all still in alignment with behavioural models of pedagogy. The NCLEX-RN is an example of a large-scale, standardized assessment (LSA), so is not parallel to the classroom assessment practices which are the subject of this paper, but I mention it here to draw the distinction between professionally-created LSAs and most instructor-created classroom assessments. The NCLEX-RN is continually revised and updated and reflects very robust psychometric properties (validity and reliability) (Smith Glasgow, Dreher, and Schreiber 2019). More importantly, however, the NCLEX-RN has been updated to “shift away from a primary focus on content and the indirect testing of clinical judgment to a major focus on clinical judgment” (Caputi 2019, 2). Caputi, in her reflections on the next-generation NCLEX-RN (NGN) asks 2 questions (the second of which is most relevant here): “1) Are students ready for this type of NCLEX? 2) If our students already pass the NCLEX, can we keep doing the same type of preparation for the NGN?” (p. 2). Her answer to both questions is “No.” She goes on to argue, So, what can faculty do? I propose that nurse faculty, at all levels of nursing education, revise their curricula to teach a detailed thinking process that students must employ over and over throughout the nursing curriculum. Just as students practice psychomotor skills until they are perfected, they must do so with thinking skills and strategies A new model for teaching clinical judgment is needed\", (p. 2, emphasis in original). In this case, the technology-mediated assessment instrument has been designed to measure 21st century skills, and Caputi recognizes that if pre-service nurses are going to have to pass the NGN, nursing instructors will need to realign their pedagogy. This example illustrates how LSAs exert pressure on instructors and schools to adjust their pedagogy, in the case of nursing, to encourage 21st century pedagogy that teaches thinking skills. The opposite is also true, however, in that LSAs which emphasize the lower-level cognitive skills prized by the behaviourists cause instructors to match their pedagogy to that model (Caputi 2019; Clarke-Midura and Dede 2010; DeLuca, Rickey, and Coombs 2021; Pellegrino and Quellmalz 2010). Further, the American Educational Research Association (AERA), the National Council on Measurement in Education (NCME), and the American Psychological Association (APA) argue that LSAs tend to have other negative effects on education systems, namely the narrowing of curricula (teaching to the test), reduced instructional strategies (previously mentioned), higher dropout rates, and the enactment of policies and practices that increase test scores without increasing learning (2014). The NCLEX-RN notwithstanding, many implementations of technology in assessment remain focused on increasing the efficiencies of summative test administration (Broadfoot 2016; Pellegrino and Quellmalz 2010; Webb and Ifenthaler 2018). 6.6 Impact on Learners Aside from the effects on learners’ approaches to their learning, assessment, particularly summative assessment, has profound effects in other ways, such as determining status or progression in a course or program, determining eligibility for scholarships and awards, determining career paths and on other affective constructs such as motivation (Crooks 1988) and anxiety (Menucha Birenbaum 2007; Harlen and Deakin Crick 2002). Jones et al. (2021) report that assessment practices impact learners’ well-being, which they define as including physical and mental health as well as the ability for learners to “fully exercise their cognitive, emotional, physical and social powers, leading to flourishing” (p. 439). They note that summative assessment practices are associated in the literature with “anxiety, depression, disordered eating, self-harm, panic attacks, burnout, … thoughts of suicide … disordered sleep, loss of appetite, physical inactivity, poor physical health, … substance misuse … poorer productivity, motivation and test scores” and that changing from norm-referenced scoring (where learners are ranked relative to each other) to criterion-referenced or pass-fail scoring is associated with lower levels of stress and anxiety among medical students in the USA. 6.7 Theoretical Framework Overview Given the wide variety of models, mostly in K-12 contexts but increasingly in HE, and the degree to which there appears to be overlap between those models, I take the position that introducing a new model into an already crowded landscape would serve little purpose. Furthermore, models do exist which have been validated empirically (DeLuca, LaPointe-McEwan, and Luhanga 2016a) and the authors of which have indicated that, despite the model being developed for K-12 contexts, adjusting the model to fit the HE context would be a worthwhile contribution to the literature (personal communication). Here, I present an argument that DeLuca et al.’s idea of approaches to classroom assessment (2016a) aligns well with the landscape of teaching, learning, and assessment in HE and may provide a blueprint for assessment reform in HE. In order to situate Approaches to Classroom Assessment within the context of teaching and learning in HE, I begin with Biggs’ (J. Biggs 1999; J. B. Biggs 1993) 3P model of teaching and learning (see Figure 3) which presents three stages of the teaching and learning process. In the first stage, presage, Biggs models both learners and the teaching context. The learners’ contexts include their previous learning, cognitive abilities, and their beliefs about the learning context, and all of these influence their preferred approaches to learning. The teaching context includes the instructors beliefs and conceptions of the curriculum and assessment, and also institutional and sometimes regulatory requirements, and each of these influence the instructor’s approach to teaching, and more to the point for this paper, their approach to assessment. The process stage represents the activities in which learners engage in order to meet the learning outcomes. Biggs notes that learners tend to either take surface approaches, where they use low-level cognitive skills (memorization of facts) in activities that require high-level cognitive skills (analysis or critique), or they take deep approaches, where they use high-level cognitive skills for activities which require them. Learner activity in this stage is influenced by their own backgrounds, abilities, and affective views of the purpose of the task and it is influenced by the choices the instructor makes based on their presage influences. In this way, Earl’s (2013) delineation between assessment of, for, and as learning can be seen to influence student learning approaches. For example, researchers note that when instructors’ approach prioritizes assessment of learning, learners tend to take a surface approach to their learning (Menucha Birenbaum 2007). Finally, the product stage represents the learners’ achievement in relation to the outcomes. Moving left to right, each of the first two stages represents an input into the system, and the third, an output. However, as the arrows between the components are two-way, the product stage also becomes an input, the results of which are fed back into the system (right to left) informing learners of their status in relation to the outcomes as well as the utility of their learning activities. It also informs the instructor about the nature of the learners’ achievement and the utility of the learning tasks. The achievement component provides formative feedback directly to both instructors and learners, informing both about next steps in the learning process. It also provides insight into the learning activities and, when that is fed back to learners, provides metacognitive cues as to the strategies learners might use in future tasks. It also provides information to instructors about the nature of the learning activities and how their approaches to assessment are impacting learning. Figure 3. 3P Model of Teaching and Learning. (J. Biggs 1999; J. B. Biggs 1993) References "],["method.html", " 7 Method 7.1 Overview of the Problem, Purpose, and Questions 7.2 Methodological Approaches 7.3 Visualization of a Mixed Research Project 7.4 Participants and Sampling Strategy 7.5 Data Collection Strategy 7.6 Types of Data and Analysis 7.7 Research Procedures 7.8 Chapter Summary", " 7 Method 7.1 Overview of the Problem, Purpose, and Questions Research suggests that higher education faculty receive little formal preparation for assessing learning (Lipnevich et al. 2020) and that there may be misalignment between modern pedagogical practices and the approaches to assessment taken by higher education instructors (Knight 2002; Massey, DeLuca, and LaPointe-McEwan 2020; Shepard 2000). The purpose of this paper will be to outline a methodological approach to investigating higher education instructors’ assessment literacies and practices as well as the impact of those practices on learners. Assessment literacy is the result of a complex interplay of skills, beliefs, and values about four domains of assessment practice: (1) the purposes of assessment; (2) the processes associated with designing, administering, scoring, and communicating the results of assessment tasks; (3) fairness in assessment practices; and (4) assessment theory, each with three priority areas. (DeLuca, Coombs, et al. 2019). Specific research questions to be addressed are: Are there distinct patterns in higher education instructors’ approaches to assessment in Canada? Does the prevalence of these patterns differ by: instructors’ levels of experience in teaching face-to-face versus online? instructors’ levels of experience using technology? What factors influence instructors’ approaches to assessment? How do instructors’ assessment approaches affect learners’ experiences? In reviewing the extant literature concerning these topics and questions, and how researchers approach them, I searched the Educational Resource Information Center (ERIC), Google Scholar, and the University of Victoria Library Summon 2.0 databases. I also searched specific journals, such as Frontiers in Education, International Review of Research in Open and Distributed Learning, the Journal of Mixed Methods Research, Educational Measurement: Issues and Practice, and Assessment and Evaluation in Higher Education. Search terms included “assessment literacy,” “assessment,” “online,” “higher education,” “learning,” and “learner impact” in various combinations. I also used researchrabbit.ai to assist with tracking citations of seminal and other important articles and for visualizing the relationships between found articles. 7.2 Methodological Approaches All research is grounded in a particular tradition or worldview, often called a paradigm. A research paradigm is a set of philosophical assumptions and values about the universe and the ways in which we can learn and know about it (Johnson and Christensen 2017). According to Johnson and Christenson (2017), research paradigms are generally concerned with the answers to questions related to: - how we learn about something (methodology), - how we know something is true (epistemology) - how we know reality, or if something exists (ontology), - how we know if something is valuable or ethical (axiology), and; - how we communicate and present arguments (rhetoric). The three primary research paradigms are quantitative, qualitative, and mixed research (often called mixed methods research). Quantitative researchers traditionally followed the epistemological position of positivism, characterized by the idea that knowledge is singular, objective, and can be discovered through the empirical analysis of numerical data (Held 2019), although most modern quantitative researchers take a post-positivist view which recognizes that the values and beliefs of the researcher play a role in research and challenge the idea of pure objectivity (Tashakkori, Johnson, and Teddlie 2020). Quantitative research is often confirmatory in nature, with the researcher gathering data to either confirm or disconfirm a specific hypothesis (Johnson and Christensen 2017). Qualitative researchers, on the other hand, tend to take the epistemological view of constructivism which posits that meaning is constructed by the researcher as they observe individuals and groups in relation to each other and particular phenomena. Qualitative research is exploratory in nature and uses non-numerical data such as texts, transcripts, images, and categories (Johnson and Christensen 2017) in response to open-ended questions (Tashakkori, Johnson, and Teddlie 2020). During the latter half of the 20th century, when qualitative approaches were becoming more popular the research community tended towards a dualistic, ‘either/or’ view of these two paradigms (Niglas 2010; Tashakkori, Johnson, and Teddlie 2020) to the extent that the two paradigms were seen to be fundamentally incompatible. For example, Guba (1990, 81 as quoted in; Johnson and Onwuegbuzie 2004, 14) emphasized “accommodation between paradigms is impossible … we are led to vastly diverse, disparate, and totally antithetical ends.” Early in the 21st century, however, Johnson and Onwuegbuzie countered that argument and proposed that paradigms are better visualized on a continuum, that both paradigms are useful, and that mixing methods can “draw from the strengths and minimize the weaknesses of both in single research studies and across studies” (2004, 13–14). So mixed research is characterized by an epistemologically pragmatic view that both quantitative and qualitative approaches are valuable and that combining numerical and non-numerical data, analyses, and results can lead to deeper understanding (Bazeley 2018; Tashakkori, Johnson, and Teddlie 2020). More specifically, Johnson, et al. (2007) provide the following definition of mixed research: Mixed methods research is the type of research in which a researcher or team of researchers combines elements of qualitative and quantitative research approaches (e.g., use of qualitative and quantitative viewpoints, data collection, analysis, inference techniques) for the broad purposes of breadth and depth of understanding and corroboration. (p. 123) I will use the term mixed research in alignment with Johnson and Christensen (2017) who advocate for that term rather than mixed methods research because it is more than the method of research that is mixed. Indeed, mixing occurs at the level of paradigms, questions, methods, analyses, and reporting of results (Bazeley 2018). Niglas (2010) contends that while the epistemological and philosophical stances of the researcher are important in informing the method of research, the more important considerations are the problem, purpose and questions the researcher intends to investigate. Similarly, Bazeley (2018) argues that the most important factor that researchers should consider when choosing a methodology is “whether the methods chosen and the strategies used … serve the purpose of the research” (p. 8, emphasis in original). Following Bazeley’s argument, and considering the purpose of this research project, a mixed approach is justified. Further justification is indicated in the diversity of approaches taken by researchers investigating assessment literacy or the impact of assessment on learners using quantitative (DeLuca et al. 2016; DeLuca, Rickey, and Coombs 2021; Massey, DeLuca, and LaPointe-McEwan 2020; Nayagi N and Rajendran 2020; Pereira et al. 2021), qualitative (Boud and Dawson 2021; Coombs, Ge, and DeLuca 2020; DeLuca et al. 2021; Earle 2020; Fives and Barnes 2020; Medland 2019; Watson et al. 2017), and mixed (DeLuca et al. 2018; Nicholson 2018; Iannone, Czichowsky, and Ruf 2020; Tekir 2021) approaches. 7.2.1 Quantitative approaches The construct of assessment literacy has a history of being grounded in sets of standards published by researchers, governments, or regulatory agencies and of being assessed through the collection and analysis of numeric and categorical data (DeLuca et al. 2016; Gotch and French 2014). Gotch and French (2014), in their systematic review of the literature on assessment literacy inventories, found 36 instruments, all but one of which (Jarr 2012) gathered only quantitative (numeric and categorical) data. Studies based on these instruments (for example, Alkharusi, et al. (2012) based on Plake, et al. (2005)) generally included quantitative analyses because the data gathered are quantitative. Early instruments (Mertler and Campbell 2005; Barbara S. Plake 1993) were based on the 1990 Standards for Teacher Competency in Educational Assessment of Students (AFT, NCME, and NEA 1990). Since then, Brookhart (2011) published a critique of the 1990 standards arguing that they were out of date and not in alignment with modern pedagogy. This was followed a few years later by Gotch and French (2014) publishing the aforementioned systematic review, during which they found that the psychometric properties of the 36 instruments published between 1991 and 2012 were low. Finally, Klinger et al. (2015) published the Classroom Assessment Standards for PreK-12 Teachers (Klinger et al. 2015). These cumulative advances led to DeLuca et al. (2016b) to develop the Approaches to Classroom Assessment Inventory (ACAI) and enabled researchers to base their quantitative investigations on an instrument shown to produce valid results in relation to a modern model of assessment literacy. There are numerous examples of research teams using quantitative approaches to investigate assessment literacy, some of which are mentioned here. DeLuca et al. (2016) surveyed 404 K-12 teachers in Canada and the USA using DeLuca et al.’s (2016a) Approaches to Classroom Assessment Inventory (ACAI), an instrument based on the 2015 Standards. DeLuca et al. (2021), recently used the same inventory in an international study comparing assessment practices in the USA, Canada, and China. Nayagi and Rajendran (2020) also used the ACAI to investigate approaches to assessment, although they modified it for their context of pre-service teachers in India. Massey et al. (2020) surveyed higher education instructors to investigate their approaches to and confidence with assessment. Lastly, with respect to the impact of instructors’ assessment practices on learners, Pereira et al. (2021), Pereira et al. (2017), and Flores et al. (2015) used surveys to collect quantitative data about the perceptions of Portuguese undergraduate learners regarding the assessment practices of their instructors. While there are more studies of assessment literacy in the K-12 sector (Medland 2019), there is a growing number in the higher education sector (e.g. (Massey, DeLuca, and LaPointe-McEwan 2020; Nayagi N and Rajendran 2020; Pereira et al. 2021)). Research questions 1 and 2 in the present study are intended to answer questions about the assessment literacies of higher education instructors in Canada, a construct which has been shown to be amenable to quantitative survey approaches in diverse settings, including K-12 teachers, pre-service teachers, and higher education instructors both in Canada and internationally. 7.2.2 Qualitative approaches Assessment literacy has been the focus of significant attention in quantitative research, but it is also becoming clear that the construct is much deeper and more complex than models based on sets of standards and competencies (DeLuca, Willis, et al. 2019, 2019; Willis, Adie, and Klenowski 2013; Xu and Brown 2016). Willis (2013) framed assessment literacy within multiple “horizontal and vertical structures of educational discourse” following Bernstein (1999), who described vertical discourses as structured disciplinary knowledge passed down from specialists, and horizontal discourses as more contextualized knowledge practiced and passed along at a local level. Following on this idea, DeLuca et al. (2019) describe assessment literacy as being a “situated and differential practice predicated on negotiated knowledges” (p. 7) and elsewhere, DeLuca et al. (2019) argue that “Assessment capability involves situated professional judgement, that is the ability to draw on learning and assessment theories and experiences to purposefully design, interpret, and use a range of assessment evidence in the service of student learning” (p. 2). Similarly, Xu and Brown’s (2016) model, Teacher Assessment Literacy in Practice (TALiP), explicitly includes socio-cultural factors, teacher judgement, and teacher identity. The methodological implication of this shift to a more complex understanding of assessment literacy is that there seems to have been an attendant shift in researchers engaging in qualitative investigations of assessment literacy (Bearman et al. 2017; DeLuca et al. 2021; Fives and Barnes 2020; Medland 2019; Watson et al. 2017). Qualitative methodologies exist because of the view that, according to Guba and Lincoln (1989), reality is constructed through social processes, and is not simply observed in the world. In my view, this epistemological stance within the community of qualitative researchers aligns well with the nature of assessment literacy as a phenomenon impacted by horizontal discourses. In my own review of the literature on assessment literacy, there seems to be a marked increase in the number of qualitative studies published since 2016, which may reflect a deepening understanding of the characteristics of assessment literacy. While the trend to use qualitative approaches to investigate assessment literacy and the impact of assessment on learners is more recent compared to quantitative approaches, there are good examples of qualitative studies, some of which I briefly describe here. Watson et al. (2017) published a case study of how instructor assessment practices impacted a single learner who struggled through a graduate-level learning module. Benediktsson and Ragnarsdóttir (2020) used focus group interviews followed by semi-structured interviews with individual learners to investigate the assessment experiences of immigrant learners in Icelandic universities. Bearman et al. (2017) interviewed thirty-three higher education instructors from a variety of disciplines and institutions to investigate how they designed assessments and found that there is a need for professional development activities about assessment to have a relational focus. In the UK context of having external peers provide regulatory feedback, Medland (2019) used an exploratory case study approach to investigate the assessment literacy of external examiners in UK higher education. In another case study with a single subject, Fives and Barnes (2020), recognizing the complexity of classroom assessment, investigated the assessment-related routines of an experienced teacher. Their goal was to engage in a close examination of how an experienced teacher enacted assessment practices in their classroom in order to “make explicit [the] cognitive tasks involved in their work” (p. 3). More recently, DeLuca et al. ((2021)) used a case study methodology to analyze data gathered from interviews, reflections, and course work to investigate the learning trajectories of 35 pre-service teachers during their program. Each of these studies was exploratory in nature, and/or designed specifically to uncover deeper meanings than could be discerned with the use of a survey or other quantitative instrument, which would require the researcher to anticipate and direct respondents toward a limited set of possible answers. Research questions 3 and 4 of the present study are intended to explore the qualitative and subjective experiences of instructors, what factors influence their approaches to assessment and the experiences of learners as they engage in assessment activities. Both of these questions will require responses to open-ended questions and will be exploratory in nature, aligning with a qualitative paradigm and epistemology. 7.2.3 Mixed approaches Researchers have argued that using mixed approaches to research is a way to strengthen and increase confidence in the validity of results by maximizing the strengths of each paradigm while minimizing the weaknesses (Creswell 2009; McKim 2017). Creamer (2018) argues that mixing of qualitative and quantitative methods is the defining characteristic of mixed research. She defines mixing as “the linking, merging, or embedding of qualitative and quantitative strands of a mixed methods study” (p. 6, emphasis in original). In a departure from the historical paradigm wars between quantitative and qualitative approaches, Niglas (2010) contends for the idea that mixed research studies fall on a continuum between qualitative and quantitative approaches with varying degrees of epistemological stances and methods. Given the abundance of literature exploring instructor assessment literacy and practices and the impact of those practices on learners from both quantitative and qualitative paradigms, it is not surprising that there is also a body of literature investigating these constructs using mixed research approaches (Esfandiari et al. 2016; Ogan‐Bekiroglu and Suzuk 2014; Solomonidou and Michaelides 2017). Creamer (2018) suggests evaluating mixed research projects using a table to make the structure of the study more plainly visible. The following tables are overviews of selected mixed research projects investigating assessment literacy in K-12 and higher education. Table 1 Overview of “Students’ conceptions of assessment purposes in a low stakes secondary-school context: A mixed methodology approach”* (Solomonidou and Michaelides 2017)* Study Component Details Yes/No Comments Rationale/Purpose for Mixing Enhancement Priority QUAN → qual Timing of Data Collection Sequential Timing of Data Analysis Sequential Mixing Design Yes Qualitative data were used to confirm tentative inferences from quantitative analysis. Data Collection Yes Semi-structured interview questions were revised in light of quantitative inferences, and participants could view their survey answers during their interviews. Data Analysis Yes Blended in the report. Inferences Yes Blended in the report. Fully Integrated Yes Qualitative Inference Learners value assessment information but it takes work to use the information formatively. - - Quantitative Inference Learners agreed more with the idea that assessment is a tool for improvement and less with the idea that assessment is negative. Meta-inference Teacher education and development should systematically include alternative assessment practices. - Value Added Linking data from both surveys and interviews provided opportunity for learners to elaborate on their views and provide a richer picture of their conceptions of assessment. Table 2 Overview of “A Mixed-methods, Cross-sectional Study of Assessment Literacy of Iranian University Instructors: Implications for Teachers’ Professional Development”* (Esfandiari et al. 2016)* Study Component Details Yes/No Comments Rationale/Purpose for Mixing Complementarity Priority QUAN → qual Timing of Data Collection Sequential Timing of Data Analysis Sequential Mixing Design Yes The qualitative phase was used to elaborate on the quantiative phase. Data Collection No Data Analysis Yes Qualitative analysis filled gaps in quantitative inferences. Inferences Yes Blended in the report. Fully Integrated No Qualitative Inference Subgroups of participants assessed learners differently, used assessment for different purposes, had different levels of training in assessment, and different views on the psychometric properties of tests. Quantitative Inference Results showed support for a three-component model of assessment literacy. Meta-inference Assessment literacy is a multi-dimensional concept affected my many factors, including teaching experience, professional development and training, and the local teaching context. Value Added Qualitative analysis was critical to helping the researchers understand the differences seen between subgroups in the quantitative analysis. Table 3 Overview of “Pre-service teachers’ assessment literacy and its implementation into practice” (Ogan‐Bekiroglu and Suzuk 2014) Study Component Details Yes/No Comments Rationale/Purpose for Mixing Triangulation and Enhancement Priority QUAN → qual+qual Timing of Data Collection Sequential Timing of Data Analysis Side-by-side comparison (concurrent) Mixing Design Yes The qualitative phase was used to validate and elaborate on the quantitative phase. Data Collection No Data Analysis Yes Qualitative and quantitative data were analyzed concurrently by presenting them side-by-side for comparison. Inferences Yes Blended in the report. Fully Integrated No Qualitative Inference Qualitative analysis showed alignment with quantitative analysis Quantitative Inference Pre-service teachers’ assessment literacy was determined to be “close to constructivist” (p. 352) Meta-inference Although participants were able to demonstrate knowledge of assessment literacy, they had difficulty translating that knowledge into practice. Value Added Qualitative analysis was important for determining the difficulty in translating knowledge of assessment literacy into practice of good assessment. Table 4 Overview of “A mixed method exploration of student perceptions of assessment in nursing and biomedicine” (Garvey, Hodgson, and Tighe 2021) Study Component Details Yes/No Comments Rationale/Purpose for Mixing Triangulation Priority QUAN+QUAL Timing of Data Collection Concurrent Timing of Data Analysis Consecutive Mixing Design Yes The qualitative phase was used to validate and elaborate on the quantitative phase. Data Collection Yes Open-ended (qualitative) responses were gathered along with quantitative responses and were designed to elicit clarifying views. Data Analysis No Inferences Yes Blended in the report. Fully Integrated No Qualitative Inference Qualitative analysis deeper levels of complexity compared to quantitative analysis. Quantitative Inference First-year university learners indicated positive views of assessment, regardless of their program. Meta-inference Universities should recognize that first-year learners tend to have positive views of assessment and should communicate often with them about the purposes of assessment. Value Added Qualitative analysis showed areas of misalignment with the quantitative analysis. Each of the preceding four studies shows different ways in which mixed research can enhance validity or provide avenues for deeper and more complex understanding of instructors’ assessment literacy and practices and the impact of those on learners. The tables illustrate the complexity of mixed studies and highlight the challenges associated with integrating quantitative and qualitative approaches at each stage of the research. However, even those studies which do not demonstrate all of the qualities of fully integrated mixed research contain valuable insights based on the integration of the two analyses. 7.2.4 Epistemological Alignment with Purpose of the Research The purpose of this project is to investigate a multi-dimensional construct, assessment literacy, that is influenced by factors both internal (for example: beliefs, values, and past experiences) and external (for example: assessment culture (DeLuca, Rickey, and Coombs 2021; Massey, DeLuca, and LaPointe-McEwan 2020) or government regulations) to instructors (DeLuca, Coombs, et al. 2019). In addition, the project will investigate the factors that influence instructors in higher education to take particular approaches to their assessment practice and how those practices impact learners. In envisioning this multi-dimensional research project and formulating the research questions, I recognize value in both the discovery and the creation of meaning in the research process, a pragmatic epistemology that aligns with the ethos of many mixed researchers. In alignment with the examples from the literature described in the preceding sections, this project will draw from both quantitative and qualitative paradigms, will use both quantitative and qualitative data connected in multiple ways, engage in analyses in alignment with both traditions, and will present results as inferences from both quantitative and qualitative analyses in addition to meta-inferences (Bazeley 2018, 62) from the integration of analyses. As such, I propose a mixed research investigation, which will be described in greater detail in the following sections. 7.3 Visualization of a Mixed Research Project Researchers who use mixed approaches often include visual overviews of their research project as mixed research projects can be very complex (Creamer 2018). Figure 1 below is a visual representation of the present project. It is helpful to understand some of the conventions of visualizing mixed research projects as seen in figure 1. According to Tashakkori et al. (2020), quantitative components are depicted as ovals and qualitative components are depicted as rectangles. Additionally, in figure 1, barred rectangles indicate mixed inferences. Solid arrows indicate the general direction of the workflow throughout the project and dashed arrows represent potential feedback loops where the results and inferences from one phase inform one or more components of a subsequent phase. Mixed researchers use upper- and lower-case letters to indicate the priority in any given phase. QUAN (all-caps) indicates that quantitative data and analyses are prioritized, while qual (lower-case) indicates that qualitative data and analyses are secondary. Further, a ‘+’ indicates that two or more types of data are collected and/or analyzed concurrently, while a ’ → ’ indicates that two or more types of data are collected or analyzed consecutively or sequentially. So, the notation ‘QUAN+qual’ indicates that both quantitative and qualitative data are collected and analyzed concurrently with quantitative data and analysis being prioritized. The notation ‘QUAN → QUAL’ indicates that quantitative data are being collected and/or analyzed before qualitative data, and both types of data and analysis have equal weight. Figure 1 A Visualization of a QUAN+qual → QUAL Sequential Exploratory Mixed Research Study. Following a pilot phase, I intend to engage in a QUAN+qual → QUAL sequential exploratory research project comprising two separate data collection and analysis phases. In phase 1a, I will use a survey of higher education instructors to collect and analyze primarily quantitative data to investigate research questions 1 and 2. Are there distinct patterns in higher education instructors’ approaches to assessment in Canada? Does the prevalence of these patterns differ by: instructors’ levels of experience in teaching face-to-face versus online? instructors’ levels of experience using technology? This will be supplemented in phase 1b by the concurrent collection and subsequent analysis of qualitative data to answer research question 3. What factors influence instructors’ approaches to assessment? The second phase will begin following the analysis and integrationof phase 1a+b data and will collect and analyze qualitative data in semi-structured interviews with learners nominated by instructors in phase 1. Phase 2 analysis is intended to answer research question 4. How do instructors’ assessment approaches affect learners’ experiences? Since research question 4 is an integrative question (Creamer 2018), it cannot be answered fully until the inferences from phase 1 and phase 2 are integrated into one or more meta-inferences. 7.4 Participants and Sampling Strategy The target population for the research will be instructors who have taught at least one semester-length, accredited course in the previous 12 months at an English-speaking [^1] public higher education institution in Canada. Ideally, inferences from phase 1 of the study would be generalizable to the total population of English-speaking higher education instructors in Canada, which would require a random sample in which every member of the target population would have equal opportunity to be selected and no selection would have been influenced by a previous selection (Hibberts, Burke Johnson, and Hudson 2012; Renckly 2002). Care will be taken to ensure the sample is representative of the whole population by engaging in a stratified random sampling technique at the institutional level (Hibberts, Burke Johnson, and Hudson 2012). This would involve using regions within Canada as strata (for example, British Columbia, the prairie provinces, the north, Ontario, Quebec, eastern Canada), randomly choosing 2-3 higher education institutions from within those strata, and sending the survey to all instructors at those institutions. Recruitment of participants for phase 2 will involve phase 1 participants (instructors) nominating 8-10 learners from their courses, I will randomly select 2-3 respondents from phase 1 and then randomly select 4-5 of their nominated learners to receive invitations to the semi-structured interviews. This procedure will ensure that instructors will not know which of their nominated learners participated in the research and it will create “linked data” (2018, 126), allowing for greater explanatory power due to being able to match specific instructor approaches with learner impacts. 7.5 Data Collection Strategy 7.5.1 Phase 1 I propose that the phase 1 data collection instrument be DeLuca et al.’s (2016a) ACAI (see Appendix 1 for the full ACAI v3.0 and its specifications). The ACAI was designed by a team of researchers, led by Dr. Christopher DeLuca at Queen’s University in Canada to investigate the approaches that K-12 educators take to assessment in their classrooms. It is based on an analysis of assessment standards published in five majority English-speaking countries since 1990. The instrument is based on a four-dimensional framework of assessment literacy (assessment purposes, assessment processes, assessment fairness, and measurement theory), and includes one open-ended question and several closed-ended questions about how teachers form their approaches to assessment. It also includes a section regarding educator beliefs about assessment based on the Beliefs about Assessment scale (Smith et al. 2014). The ACAI has been used in a wide variety of studies within the instrument’s author’s research group (Coombs et al. 2018; Coombs, DeLuca, and MacGregor 2020; DeLuca, Coombs, et al. 2019; DeLuca et al. 2016; DeLuca, Willis, et al. 2019; Schneider et al. 2020), and also by other researchers (Nayagi N and Rajendran 2020). In several of these studies, the ACAI has been modified somewhat to fit the particular context or research design. As the ACAI was designed for use in K-12 contexts and has been used less often to investigate higher education instructors’ approaches to assessment, I will modify the language in the instrument to be more suitable for the higher education context and to include more open-ended questions. Following these modifications, I will undertake a pilot phase to determine whether the modified ACAI is suitable for the higher education context. The pilot phase will inform the version of the ACAI used in Phase 1 of the study and will include both expert review and the recruitment of a small sample from the target population to preview the survey in its intended mode and offer specific and guided feedback (Hibberts, Burke Johnson, and Hudson 2012; Renckly 2002). 7.5.2 Phase 2 The semi-structured interviews in phase 2 will include questions similar to the following: Please describe what you understand assessment to be. Did the assessment strategies that your instructor used help you to learn the course material? Did [instructor] use technology to administer assessments? How did their use/not use of technology impact your performance on the assessments? Please describe your favourite assessment from [instructor]’s course [course name]? What made that assessment your favourite? Please describe your least favourite assessment from [instructor]’s course [course name]? What made that assessment your least favourite? What factors made it easier to do well on the assessments in [course]? What factors made it more difficult to do well on the assessments in [course]? How did the assessment strategies your instructor used make you feel? Do you receive any support or accommodations to allow you to complete assessments? 7.6 Types of Data and Analysis As shown in the visualization of the research project (Figure 1), both quantitative and qualitative data will be gathered for analysis. The following sections will describe the data I intend to gather and how it may address the research questions. 7.6.1 Quantitative data During phase 1a, quantitative data will be gathered using the ACAI and will primarily be in the form of responses to 6-point Likert scale items. Similar to Flores et al. (2020), the first step will be to engage in a confirmatory factor analysis to determine the fit of the data with the Approaches to Classroom Assessment model. This step will address research question 1. Then, as the data will be ordinal data (i.e. a score of five is greater than a score of three), but not interval data (i.e. it is not possible to determine how much greater a score of five is compared to a score of three) (Bulut 2021; Jamieson 2004), it will not be appropriate to use the data to calculate common statistics such as mean or standard deviation (i.e. there is no way to calculate the mean of “highly likely” and “highly unlikely”). Consequently, more advanced statistical techniques which rely on the mean and standard deviation (i.e. t test, analysis of variance), otherwise known as parametric tests, are not appropriate for ordinal data (Field 2018). Fortunately, there are non-parametric tests that enable analysis of data that do not meet the requirements for parametric tests. One example, which will be useful in comparing two different demographic groups within my data (e.g. instructors with more than two years experience teaching with technology compared to instructors with less than two years of experience teaching with technology), is the Mann-Whitney-Wilcoxon test (Gao 2010) (MWW). Instead of calculating based on the mean scores of different samples, the MWW test requires the researcher to first rank the scores and then calculate the differences between the samples based on the ranked scores. If I am to compare 3 or more groups, the Kruskal-Wallis test (Gao 2010), based on the same ranking procedure, would be more appropriate. These two tests will be used to address research question 2. 7.6.2 Qualitative data During phase 1b, I will gather and analyze qualitative data through the use of open-ended questions embedded in the ACAI. Part B of the ACAI currently contains open-ended questions asking respondents to describe how they would respond to the scenario if their response isn’t listed as an option in the instrument (DeLuca ND). Below are some open-ended questions which might be added to Part A (demographics) of the ACAI: In a few sentences or point form notes, please describe the most important factors you consider when planning how you will assess learners in an upcoming course. In a few sentences or point form notes, please describe what you understand assessment to be. In what ways do you use digital technology in your assessment practice? The current and proposed open-ended questions in the ACAI are intended to provide qualitative data which will be analyzed using a theory-driven thematic coding process (DeLuca et al. 2018; Namey et al. 2008) to explore in greater depth the factors that influence instructors to approach assessment as they do in their courses (research question 3). Namey et al. (2008) describe two general approaches to the analysis of qualitative data: content analysis and thematic analysis. In content analysis, the researcher “evaluates the frequency and saliency of particular words or phrases in a body of original text data in order to identify keywords or repeated ideas” (p. 138), but with the drawback that there is limited interpretation or consideration of context. In thematic analysis, researchers consider not only specific words and phrases, but also “implicit and explicit ideas” (p. 138). They further describe “data-driven” (themes emerge from the data) and “theory-driven” (the researcher identifies themes a priori, without consulting the data)(p. 138) approaches (see also: Tashakkori, Johnson, and Teddlie 2020). In phase 1b of the proposed project, I will take a theory-driven approach to a thematic analysis where data will be coded in alignment with the four dimensions of the Approaches to Classroom Assessment framework (DeLuca et al. 2016) (the dotted line connecting “Phase 1a Inferences” to “Phase 1b Analysis” in figure 1). In addition, I will be particularly mindful that emergent themes may become clear in the data and I will be especially interested in those not represented in, or which are in contradiction to, the quantitative analysis (Tashakkori, Johnson, and Teddlie 2020). After completing both phase 1a and 1b, I will be able to synthesize inferences from the two phase 1 analyses, and also one or more meta-inferences from the integrated analysis of the two sub-phases. These inferences will inform the phase 2 semi-structured interview questions, providing an integration point (the dotted lines from “Phase 1a Analysis,” “Phase 1b Analysis,” and “Phase 1 Meta-inferences” to “Semi-structured Interviews”). Raw qualitative data from phase 2 of the project will be in the form of audio recordings of semi-structured interviews, which will be transcribed verbatim and stored as plain text files. As this data is less connected to phase 1 data because it is a different sample population, I will use a data-driven thematic approach to uncover emergent themes in the data while not precluding the possibility that theory-driven themes may emerge. Following the phase 1 and phase 2 analyses, I will synthesize the inferences from phase 1a and 1b, the meta-inferences from phase 1, and the inferences from phase 2 to draw project-level meta-inferences from the whole of the data, thereby addressing research question 4. 7.7 Research Procedures Following approval from the University of Victoria Human Research Ethics Board (HREB) and a brief pilot phase, I will use a stratified random sampling technique to randomly select 2-3 higher education institutions from each of six regions in Canada (British Columbia, the prairie provinces, Ontario, Quebec, eastern Canada, and northern Canada), for a total of 12-18 institutions. I will contact the faculty associations at each of these institutions to request that they forward my letter of invitation and embedded link to a web-based survey. Each participant will be asked to self-identify and to nominate 8-10 learners in one of their courses and include the nominees’ contact information. To begin phase 2, I will randomly select 2-3 instructors from phase 1, then randomly select 4-5 of their nominated learners to invite to semi-structured interviews. The interviews will be conducted via web-conferencing software that allows recording and the automatic generation of transcripts. Table 5 shows a projected timeline for the research. Table 5 Projected Timeline Projected Date Project Stage December 2021 UVic HREB Approval January 2022 Pilot Phase February 2022 Phase 1 Data Collection July 2022 Phase 1a and 1b Data Analysis September 2022 Phase 2 Data Collection November 2022 Phase 2 Data Analysis January 2023 Integration and Write-Up 7.8 Chapter Summary In this chapter, I have discussed the three major approaches to research, quantitative, qualitative, and mixed, along with their underlying epistemologies. Through a review of the literature on assessment literacy and its impact on learners in higher education, I show that researchers use all three approaches and that the most important consideration in determining a method to pursue an investigation in this area is the purpose of the investigation. As the purpose of this investigation is to gain understanding of the multi-dimensional construct of assessment literacy and its effects on learners, a highly subjective phenomenon, a mixed research approach is justified. I discuss a possible method to gather and analyze quantitative and qualitative data from higher education instructors and their learners, along with strategies for drawing inferences and meta-inferences based on the integration of the separate analyses. [^1] If funding for translation services is secured, I would extend the sample population to include French-speaking instructors and learners. References "],["references-1.html", " 8 References", " 8 References "],["results.html", " 9 Results", " 9 Results We have finished a nice book. "],["discussion.html", " 10 Discussion", " 10 Discussion We have finished a nice book. "],["conclusion.html", " 11 Conclusion", " 11 Conclusion We have finished a nice book. "],["references-2.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
