# Introduction{-}

Assessing learning is both a critical component of the work of teaching in higher education and also a major factor in learners' experiences of higher education [@biggsWhatStudentDoes1999; @woldeab21stCenturyAssessment2019]. Faculty and instructors' approaches to assessment in higher education are shaped by a variety of factors, including the approaches to assessment that they experienced as learners [@lipnevichWhatGradesMean2020; @masseyAssessmentLiteracyCollege2020], pressure from our increasingly technological society to integrate digital tools into the teaching and learning process [@pellegrinoPerspectivesIntegrationTechnology2010], and the changing needs of 21st-century employers, who are demanding employees with demonstrated ability in collaboration, creative problem-solving, analytical thinking, and the ability to learn [@shute21stCenturyAssessment2010; @worldeconomicforumFutureJobsReport2020], competencies not easily measured in traditional testing formats. 

Massey et al., [-@masseyAssessmentLiteracyCollege2020] contend that instructors in higher education typically have few opportunities to engage in formal preparation for the task of assessing learning, and consequently there is high variability in how instructors assess learning in their courses. This is congruent with Coombs et al.'s [-@coombsSeaSeaCanadian2020] findings that even preservice teachers in teacher preparation programs are often unprepared for the challenge of assessing learning. If pre-service teachers, who complete a program of formal academic preparation for teaching, are under-prepared, it follows that those who exit doctoral programs with no formal preparation for teaching or assessment [@lipnevichWhatGradesMean2020] will be even less adequately prepared. This lack of formal preparation generally means that higher education instructors assess learning in the only way they know how, which is to follow the example of their supervisors and professors from graduate school.

In her important article, Shepard [-@shepardRoleAssessmentLearning2000] argues that traditional assessment structures originated in past models of curriculum and instruction which were popular in the early 1900s. These curricular models emphasized the work of psychologists like Thorndike [@thorndikeElementsPsychology1905] and Skinner [@skinnerBehaviourOrganisms1938] who viewed the process of learning as being grounded in the mechanistic view of behaviourism where learning is the result of the precise and controlled input of 'knowledge' and reinforced with rewards for correct responses. As such, instructors (appropriately) designed their assessments to align with the curricular goals of the time and assessed learning by determining whether or not a learner could provide the single correct response to a given question at a time removed from the instruction.  However, in the latter half of the 20th century, when western psychologists discovered the ideas of Lev Vygotsky [@vygotskyMindSociety1978], curricula began to take a more social-constructivist approach that emphasized higher-order thinking, problem-solving in social contexts, and metacognitive skills over rote memorization. 

Unfortunately, it seemed that the efficiencies of testing memory, recognition, and recall through selected-response tests were too deeply embedded in the practices of higher education instructors who resisted changing their assessments to match the new curricular goals [@shepardRoleAssessmentLearning2000]. Shepard argues for the need to integrate assessment and instruction in such a way as to engage learners in authentic performance tasks more suited to modern understandings of cognition. It appears now that, in the twenty years since Shepard wrote her paper, the goals of early 21st century curricula have continued to diverge from those of the 20th century with the World Economic Forum identifying competencies in collaboration, analytical thinking, creative problem-solving, and the continual learning as being priorities for 21st century employers [@worldeconomicforumFutureJobsReport2020]. Models of assessment which mimic and re-inscribe traditional assessment practices and prioritize testing skills in a manner aligned with 20th century curricular models are no longer adequate because they no longer align with the priorities of modern higher education [@broadfootAssessmentTwentyFirstCenturyLearning2016; @crooksImpactClassroomEvaluation1988; @pellegrinoPerspectivesIntegrationTechnology2010; @timmisRethinkingAssessmentDigital2016].

Furthermore, pressures from our increasingly technological society have also impacted instructors' approaches to their assessment practice. O'Donnell [-@odonnellAssessmentDigitalPractice2020] argues that *all* learning in higher education is now mediated in some way by technology, even if it is the superficial use of a word processor an instructor uses to create course materials or a learner uses to compose an essay. Even so, he claims that the use of technology often does little more than increase the efficiency (a term which he declines to define) of existing practices (an example might be reducing the time it takes to score a selected-response exam by using bubble sheets for examinee responses). This critique echoes Timmis et al. [-@timmisRethinkingAssessmentDigital2016] who argue that prioritizing superficial characteristics of technology, like 'efficiency', comes at the cost of more innovative applications.

Additionally, the datafication of higher education has made very large data sets available to individual instructors as well as to learning technology administrators. This is often in the form of log data from learning management systems (LMS) which has been used to explore relationships between learner behaviours in the LMS and achievement, as reported by Stadler et al. [-@stadlerFirstEqualsLog2020]. Shute and Rahimi reported on their exploration of what they call "stealth assessments" [@shuteStealthAssessmentCreativity2021, p. 4] where large amounts of data are collected as learners interact in game-based learning environments. They argue that stealth assessments, which learners do not notice because they are woven seamlessly into the learning materials [@shute21stCenturyAssessment2010] and automatically scored based on several factors related to how the learners interacts with the materials, embedded tutorials, and other learners.

Given the plurality of factors influencing higher education instructors identified above and the relative paucity of published research on assessment in higher education compared to K-12 [@lipnevichWhatGradesMean2020; @masseyAssessmentLiteracyCollege2020], this dissertation will unfold over the course of three independent publications. The common thread running through the three publications will be a focus on the broad topic of technologically-mediated assessment in higher education in the 21st-century. Each publication will explore this topic from different viewpoints, or lenses. The first paper will be a detailed review of the literature related to assessment and the impacts of technology on assessment in higher education using DeLuca et al.'s [-@delucaApproachesClassroomAssessment2016] approaches to classroom assessment framework. The second paper will be a quantitative analysis of DeLuca et al.'s *Approaches to Classroom Assessment Inventory* [-@delucaACAIInstrumentSpecificationsND] revised for a technologically-mediated higher education environment. The third paper will consist of a synthesis of findings from the first two papers with a view to reimagining assessment in 21st-century higher education and consideration given to the ethics of modern trends in technology.

The author notes that there are myriad examples of formal and informal initiatives in higher education which have not realized significant local or systemic change [@broadfootAssessmentTwentyFirstCenturyLearning2016; @earlAssessmentLearningUsing2013]. Instructors are highly resistant to changing their assessment practices, in part, as Broadfoot claims, because assessment is so important, but also, as will be discussed later, because the approaches that individual instructors take to assessing the work of learners are driven by complex forces [@blackAssessmentClassroomLearning1998; @delucaDifferentialSituatedView2019; @stigginsAssessmentLiteracy1991; @willisConceptualisingTeachersAssessment2013].


## Problem to be Researched{-}



## Purpose of the Research{-}

Following previous research by DeLuca and colleagues [@delucaTeachersApproachesClassroom2016; @delucaExploringAssessmentCultures2021; @delucaApproachesClassroomAssessment2016] in the K-12 sector, and Massey et al [-@masseyAssessmentLiteracyCollege2020] in the higher education sector, the purpose of this research is to investigate current assessment literacies and practices among higher education instructors and the impacts of those approaches on learners. In order to better respond to actual assessment practices, it is critical to understand the conceptions of HE instructors with respect to assessment [@delucaExploringAssessmentCultures2021; @offerdahlChangesInstructorsAssessment2011]. Similarly, due to the significant influence assessment practices have on learners and learning, understanding the relationship between instructors' assessment conceptions and practices and the experience of learners will be important in order to provide a foundation for moving into the remainder of the 21st century with assessment practices aligned with both pedagogical models and learner contexts.

## Research Questions{-}

Assessment in HE is a tremendously complex enterprise with myriad forces influencing the approaches instructors take in their assessment practice, which, in turn, becomes one of the myriad forces influencing the approaches learners take in their learning activities. Instructors conceptions of assessment are often derived from their past experiences as learners who were the objects of assessment, leading to an over-reliance on summative assessment strategies which may not obtain sufficient levels of reliability or validity. Increasing assessment literacy among HE instructors is a necessary, but not likely sufficient, step in advancing fair and balanced assessment practices in an increasingly technology-mediated HE landscape. These concepts lead to the following research questions:

1. Are there distinct patterns in higher education instructors' approaches to assessment in Canada?
2. Does the prevalence of these patterns differ by:
    - instructors' levels of experience in teaching face-to-face versus online?
    - instructors' levels of experience using technology?
3. What factors influence instructors' approaches to assessment?
4. How do different assessment strategies affect learners' experiences?

## Significance of the Research{-}

